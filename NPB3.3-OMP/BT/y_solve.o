; ModuleID = 'y_solve.f'
target datalayout = "e-p:64:64:64-S128-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f16:16:16-f32:32:32-f64:64:64-f128:128:128-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64"
target triple = "x86_64-unknown-linux-gnu"

module asm "\09.ident\09\22GCC: (GNU) 4.8.2 20140206 (prerelease) LLVM: 3.4\22"

%0 = type { double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, [65 x double], double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double }
%1 = type { [1352000 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [1352000 x double], [1352000 x double] }
%2 = type { double, [3 x i32], i32 }
%3 = type { [65 x double], [65 x double], [325 x double], [325 x double] }
%4 = type { [1625 x double], [1625 x double], [4875 x double], double, double, double }

@constants_ = common unnamed_addr global %0 zeroinitializer, align 32
@fields_ = common global %1 zeroinitializer, align 32
@global_ = common unnamed_addr global %2 zeroinitializer, align 16
@work_1d_ = common unnamed_addr global %3 zeroinitializer, align 32
@work_lhs_ = common global %4 zeroinitializer, align 32
@0 = internal constant i32 7
@1 = internal constant i32 7

; Function Attrs: nounwind uwtable
define void @y_solve_() unnamed_addr #0 {
entry:
  %i = alloca i32
  %j = alloca i32
  %jsize = alloca i32
  %k = alloca i32
  %m = alloca i32
  %n = alloca i32
  %D.2142 = alloca i32
  %D.2182 = alloca i32
  %D.2145 = alloca i32
  %D.2181 = alloca i32
  %D.2148 = alloca i32
  %D.2157 = alloca i32
  %D.2156 = alloca double
  %D.2155 = alloca double
  %D.2154 = alloca double
  %D.2153 = alloca double
  %D.2152 = alloca double
  %D.2151 = alloca double
  %D.2159 = alloca i32
  %D.2162 = alloca i32
  %D.2164 = alloca i32
  %D.2169 = alloca i32
  %j.12 = alloca i32
  %D.2180 = alloca i32
  %D.2179 = alloca i32
  %D.2178 = alloca i32
  %"alloca point" = bitcast i32 0 to i32
  %"ssa point" = bitcast i32 0 to i32
  br label %"2"

"2":                                              ; preds = %entry
  %0 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %1 = trunc i32 %0 to i1
  %2 = icmp ne i1 %1, false
  br i1 %2, label %"3", label %"4"

"3":                                              ; preds = %"2"
  call void bitcast (void (...)* @timer_start_ to void (i32*)*)(i32* @0) #1
  br label %"4"

"4":                                              ; preds = %"3", %"2"
  %3 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 1), align 4
  %4 = add i32 %3, -1
  store i32 %4, i32* %jsize, align 4
  %5 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 2), align 4
  %6 = add i32 %5, -2
  %7 = icmp sle i32 1, %6
  br i1 %7, label %"5", label %"28"

"5":                                              ; preds = %"27", %"4"
  %8 = phi i32 [ %2419, %"27" ], [ 1, %"4" ]
  %9 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 0), align 4
  %10 = add i32 %9, -2
  %11 = icmp sle i32 1, %10
  br i1 %11, label %"6", label %"26"

"6":                                              ; preds = %"25", %"5"
  %12 = phi i32 [ %2416, %"25" ], [ 1, %"5" ]
  %13 = load i32* %jsize, align 4
  %14 = icmp sle i32 0, %13
  br i1 %14, label %"7", label %"9"

"7":                                              ; preds = %"8", %"6"
  %15 = phi i32 [ %776, %"8" ], [ 0, %"6" ]
  %16 = sext i32 %8 to i64
  %17 = mul i64 %16, 4225
  %18 = sext i32 %15 to i64
  %19 = mul i64 %18, 65
  %20 = add i64 %17, %19
  %21 = sext i32 %12 to i64
  %22 = add i64 %20, %21
  %23 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 5, i64 0), i64 %22
  %24 = load double* %23, align 8
  store double %24, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %25 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %26 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %27 = fmul double %25, %26
  store double %27, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %28 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %29 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %30 = fmul double %28, %29
  store double %30, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %31 = sext i32 %15 to i64
  %32 = mul i64 %31, 25
  %33 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %32
  store double 0.000000e+00, double* %33, align 8
  %34 = sext i32 %15 to i64
  %35 = mul i64 %34, 25
  %36 = add i64 %35, 5
  %37 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %36
  store double 0.000000e+00, double* %37, align 8
  %38 = sext i32 %15 to i64
  %39 = mul i64 %38, 25
  %40 = add i64 %39, 10
  %41 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %40
  store double 1.000000e+00, double* %41, align 8
  %42 = sext i32 %15 to i64
  %43 = mul i64 %42, 25
  %44 = add i64 %43, 15
  %45 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %44
  store double 0.000000e+00, double* %45, align 8
  %46 = sext i32 %15 to i64
  %47 = mul i64 %46, 25
  %48 = add i64 %47, 20
  %49 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %48
  store double 0.000000e+00, double* %49, align 8
  %50 = sext i32 %15 to i64
  %51 = mul i64 %50, 25
  %52 = add i64 %51, 1
  %53 = sext i32 %8 to i64
  %54 = mul i64 %53, 21125
  %55 = sext i32 %15 to i64
  %56 = mul i64 %55, 325
  %57 = add i64 %54, %56
  %58 = sext i32 %12 to i64
  %59 = mul i64 %58, 5
  %60 = add i64 %57, %59
  %61 = add i64 %60, 1
  %62 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %61
  %63 = load double* %62, align 8
  %64 = sext i32 %8 to i64
  %65 = mul i64 %64, 21125
  %66 = sext i32 %15 to i64
  %67 = mul i64 %66, 325
  %68 = add i64 %65, %67
  %69 = sext i32 %12 to i64
  %70 = mul i64 %69, 5
  %71 = add i64 %68, %70
  %72 = add i64 %71, 2
  %73 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %72
  %74 = load double* %73, align 8
  %75 = fmul double %63, %74
  %76 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %77 = fmul double %75, %76
  %78 = fsub double -0.000000e+00, %77
  %79 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %52
  store double %78, double* %79, align 8
  %80 = sext i32 %15 to i64
  %81 = mul i64 %80, 25
  %82 = add i64 %81, 6
  %83 = sext i32 %8 to i64
  %84 = mul i64 %83, 21125
  %85 = sext i32 %15 to i64
  %86 = mul i64 %85, 325
  %87 = add i64 %84, %86
  %88 = sext i32 %12 to i64
  %89 = mul i64 %88, 5
  %90 = add i64 %87, %89
  %91 = add i64 %90, 2
  %92 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %91
  %93 = load double* %92, align 8
  %94 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %95 = fmul double %93, %94
  %96 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %82
  store double %95, double* %96, align 8
  %97 = sext i32 %15 to i64
  %98 = mul i64 %97, 25
  %99 = add i64 %98, 11
  %100 = sext i32 %8 to i64
  %101 = mul i64 %100, 21125
  %102 = sext i32 %15 to i64
  %103 = mul i64 %102, 325
  %104 = add i64 %101, %103
  %105 = sext i32 %12 to i64
  %106 = mul i64 %105, 5
  %107 = add i64 %104, %106
  %108 = add i64 %107, 1
  %109 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %108
  %110 = load double* %109, align 8
  %111 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %112 = fmul double %110, %111
  %113 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %99
  store double %112, double* %113, align 8
  %114 = sext i32 %15 to i64
  %115 = mul i64 %114, 25
  %116 = add i64 %115, 16
  %117 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %116
  store double 0.000000e+00, double* %117, align 8
  %118 = sext i32 %15 to i64
  %119 = mul i64 %118, 25
  %120 = add i64 %119, 21
  %121 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %120
  store double 0.000000e+00, double* %121, align 8
  %122 = sext i32 %15 to i64
  %123 = mul i64 %122, 25
  %124 = add i64 %123, 2
  %125 = sext i32 %8 to i64
  %126 = mul i64 %125, 4225
  %127 = sext i32 %15 to i64
  %128 = mul i64 %127, 65
  %129 = add i64 %126, %128
  %130 = sext i32 %12 to i64
  %131 = add i64 %129, %130
  %132 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %131
  %133 = load double* %132, align 8
  %134 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %135 = fmul double %133, %134
  %136 = sext i32 %8 to i64
  %137 = mul i64 %136, 21125
  %138 = sext i32 %15 to i64
  %139 = mul i64 %138, 325
  %140 = add i64 %137, %139
  %141 = sext i32 %12 to i64
  %142 = mul i64 %141, 5
  %143 = add i64 %140, %142
  %144 = add i64 %143, 2
  %145 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %144
  %146 = load double* %145, align 8
  %147 = sext i32 %8 to i64
  %148 = mul i64 %147, 21125
  %149 = sext i32 %15 to i64
  %150 = mul i64 %149, 325
  %151 = add i64 %148, %150
  %152 = sext i32 %12 to i64
  %153 = mul i64 %152, 5
  %154 = add i64 %151, %153
  %155 = add i64 %154, 2
  %156 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %155
  %157 = load double* %156, align 8
  %158 = fmul double %146, %157
  %159 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %160 = fmul double %158, %159
  %161 = fsub double %135, %160
  %162 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %124
  store double %161, double* %162, align 8
  %163 = sext i32 %15 to i64
  %164 = mul i64 %163, 25
  %165 = add i64 %164, 7
  %166 = sext i32 %8 to i64
  %167 = mul i64 %166, 21125
  %168 = sext i32 %15 to i64
  %169 = mul i64 %168, 325
  %170 = add i64 %167, %169
  %171 = sext i32 %12 to i64
  %172 = mul i64 %171, 5
  %173 = add i64 %170, %172
  %174 = add i64 %173, 1
  %175 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %174
  %176 = load double* %175, align 8
  %177 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %178 = fmul double %176, %177
  %179 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %180 = fmul double %178, %179
  %181 = fsub double -0.000000e+00, %180
  %182 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %165
  store double %181, double* %182, align 8
  %183 = sext i32 %15 to i64
  %184 = mul i64 %183, 25
  %185 = add i64 %184, 12
  %186 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %187 = fsub double 2.000000e+00, %186
  %188 = sext i32 %8 to i64
  %189 = mul i64 %188, 21125
  %190 = sext i32 %15 to i64
  %191 = mul i64 %190, 325
  %192 = add i64 %189, %191
  %193 = sext i32 %12 to i64
  %194 = mul i64 %193, 5
  %195 = add i64 %192, %194
  %196 = add i64 %195, 2
  %197 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %196
  %198 = load double* %197, align 8
  %199 = fmul double %187, %198
  %200 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %201 = fmul double %199, %200
  %202 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %185
  store double %201, double* %202, align 8
  %203 = sext i32 %15 to i64
  %204 = mul i64 %203, 25
  %205 = add i64 %204, 17
  %206 = sext i32 %8 to i64
  %207 = mul i64 %206, 21125
  %208 = sext i32 %15 to i64
  %209 = mul i64 %208, 325
  %210 = add i64 %207, %209
  %211 = sext i32 %12 to i64
  %212 = mul i64 %211, 5
  %213 = add i64 %210, %212
  %214 = add i64 %213, 3
  %215 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %214
  %216 = load double* %215, align 8
  %217 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %218 = fmul double %216, %217
  %219 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %220 = fmul double %218, %219
  %221 = fsub double -0.000000e+00, %220
  %222 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %205
  store double %221, double* %222, align 8
  %223 = sext i32 %15 to i64
  %224 = mul i64 %223, 25
  %225 = add i64 %224, 22
  %226 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %227 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %225
  store double %226, double* %227, align 8
  %228 = sext i32 %15 to i64
  %229 = mul i64 %228, 25
  %230 = add i64 %229, 3
  %231 = sext i32 %8 to i64
  %232 = mul i64 %231, 21125
  %233 = sext i32 %15 to i64
  %234 = mul i64 %233, 325
  %235 = add i64 %232, %234
  %236 = sext i32 %12 to i64
  %237 = mul i64 %236, 5
  %238 = add i64 %235, %237
  %239 = add i64 %238, 2
  %240 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %239
  %241 = load double* %240, align 8
  %242 = sext i32 %8 to i64
  %243 = mul i64 %242, 21125
  %244 = sext i32 %15 to i64
  %245 = mul i64 %244, 325
  %246 = add i64 %243, %245
  %247 = sext i32 %12 to i64
  %248 = mul i64 %247, 5
  %249 = add i64 %246, %248
  %250 = add i64 %249, 3
  %251 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %250
  %252 = load double* %251, align 8
  %253 = fmul double %241, %252
  %254 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %255 = fmul double %253, %254
  %256 = fsub double -0.000000e+00, %255
  %257 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %230
  store double %256, double* %257, align 8
  %258 = sext i32 %15 to i64
  %259 = mul i64 %258, 25
  %260 = add i64 %259, 8
  %261 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %260
  store double 0.000000e+00, double* %261, align 8
  %262 = sext i32 %15 to i64
  %263 = mul i64 %262, 25
  %264 = add i64 %263, 13
  %265 = sext i32 %8 to i64
  %266 = mul i64 %265, 21125
  %267 = sext i32 %15 to i64
  %268 = mul i64 %267, 325
  %269 = add i64 %266, %268
  %270 = sext i32 %12 to i64
  %271 = mul i64 %270, 5
  %272 = add i64 %269, %271
  %273 = add i64 %272, 3
  %274 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %273
  %275 = load double* %274, align 8
  %276 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %277 = fmul double %275, %276
  %278 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %264
  store double %277, double* %278, align 8
  %279 = sext i32 %15 to i64
  %280 = mul i64 %279, 25
  %281 = add i64 %280, 18
  %282 = sext i32 %8 to i64
  %283 = mul i64 %282, 21125
  %284 = sext i32 %15 to i64
  %285 = mul i64 %284, 325
  %286 = add i64 %283, %285
  %287 = sext i32 %12 to i64
  %288 = mul i64 %287, 5
  %289 = add i64 %286, %288
  %290 = add i64 %289, 2
  %291 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %290
  %292 = load double* %291, align 8
  %293 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %294 = fmul double %292, %293
  %295 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %281
  store double %294, double* %295, align 8
  %296 = sext i32 %15 to i64
  %297 = mul i64 %296, 25
  %298 = add i64 %297, 23
  %299 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %298
  store double 0.000000e+00, double* %299, align 8
  %300 = sext i32 %15 to i64
  %301 = mul i64 %300, 25
  %302 = add i64 %301, 4
  %303 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %304 = fmul double %303, 2.000000e+00
  %305 = sext i32 %8 to i64
  %306 = mul i64 %305, 4225
  %307 = sext i32 %15 to i64
  %308 = mul i64 %307, 65
  %309 = add i64 %306, %308
  %310 = sext i32 %12 to i64
  %311 = add i64 %309, %310
  %312 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 6, i64 0), i64 %311
  %313 = load double* %312, align 8
  %314 = fmul double %304, %313
  %315 = sext i32 %8 to i64
  %316 = mul i64 %315, 21125
  %317 = sext i32 %15 to i64
  %318 = mul i64 %317, 325
  %319 = add i64 %316, %318
  %320 = sext i32 %12 to i64
  %321 = mul i64 %320, 5
  %322 = add i64 %319, %321
  %323 = add i64 %322, 4
  %324 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %323
  %325 = load double* %324, align 8
  %326 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %327 = fmul double %325, %326
  %328 = fsub double %314, %327
  %329 = sext i32 %8 to i64
  %330 = mul i64 %329, 21125
  %331 = sext i32 %15 to i64
  %332 = mul i64 %331, 325
  %333 = add i64 %330, %332
  %334 = sext i32 %12 to i64
  %335 = mul i64 %334, 5
  %336 = add i64 %333, %335
  %337 = add i64 %336, 2
  %338 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %337
  %339 = load double* %338, align 8
  %340 = fmul double %328, %339
  %341 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %342 = fmul double %340, %341
  %343 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %302
  store double %342, double* %343, align 8
  %344 = sext i32 %15 to i64
  %345 = mul i64 %344, 25
  %346 = add i64 %345, 9
  %347 = sext i32 %8 to i64
  %348 = mul i64 %347, 21125
  %349 = sext i32 %15 to i64
  %350 = mul i64 %349, 325
  %351 = add i64 %348, %350
  %352 = sext i32 %12 to i64
  %353 = mul i64 %352, 5
  %354 = add i64 %351, %353
  %355 = add i64 %354, 1
  %356 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %355
  %357 = load double* %356, align 8
  %358 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %359 = fmul double %357, %358
  %360 = sext i32 %8 to i64
  %361 = mul i64 %360, 21125
  %362 = sext i32 %15 to i64
  %363 = mul i64 %362, 325
  %364 = add i64 %361, %363
  %365 = sext i32 %12 to i64
  %366 = mul i64 %365, 5
  %367 = add i64 %364, %366
  %368 = add i64 %367, 2
  %369 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %368
  %370 = load double* %369, align 8
  %371 = fmul double %359, %370
  %372 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %373 = fmul double %371, %372
  %374 = fsub double -0.000000e+00, %373
  %375 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %346
  store double %374, double* %375, align 8
  %376 = sext i32 %15 to i64
  %377 = mul i64 %376, 25
  %378 = add i64 %377, 14
  %379 = sext i32 %8 to i64
  %380 = mul i64 %379, 21125
  %381 = sext i32 %15 to i64
  %382 = mul i64 %381, 325
  %383 = add i64 %380, %382
  %384 = sext i32 %12 to i64
  %385 = mul i64 %384, 5
  %386 = add i64 %383, %385
  %387 = add i64 %386, 4
  %388 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %387
  %389 = load double* %388, align 8
  %390 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %391 = fmul double %389, %390
  %392 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %393 = fmul double %391, %392
  %394 = sext i32 %8 to i64
  %395 = mul i64 %394, 4225
  %396 = sext i32 %15 to i64
  %397 = mul i64 %396, 65
  %398 = add i64 %395, %397
  %399 = sext i32 %12 to i64
  %400 = add i64 %398, %399
  %401 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %400
  %402 = load double* %401, align 8
  %403 = sext i32 %8 to i64
  %404 = mul i64 %403, 21125
  %405 = sext i32 %15 to i64
  %406 = mul i64 %405, 325
  %407 = add i64 %404, %406
  %408 = sext i32 %12 to i64
  %409 = mul i64 %408, 5
  %410 = add i64 %407, %409
  %411 = add i64 %410, 2
  %412 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %411
  %413 = load double* %412, align 8
  %414 = sext i32 %8 to i64
  %415 = mul i64 %414, 21125
  %416 = sext i32 %15 to i64
  %417 = mul i64 %416, 325
  %418 = add i64 %415, %417
  %419 = sext i32 %12 to i64
  %420 = mul i64 %419, 5
  %421 = add i64 %418, %420
  %422 = add i64 %421, 2
  %423 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %422
  %424 = load double* %423, align 8
  %425 = fmul double %413, %424
  %426 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %427 = fmul double %425, %426
  %428 = fadd double %402, %427
  %429 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %430 = fmul double %428, %429
  %431 = fsub double %393, %430
  %432 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %378
  store double %431, double* %432, align 8
  %433 = sext i32 %15 to i64
  %434 = mul i64 %433, 25
  %435 = add i64 %434, 19
  %436 = sext i32 %8 to i64
  %437 = mul i64 %436, 21125
  %438 = sext i32 %15 to i64
  %439 = mul i64 %438, 325
  %440 = add i64 %437, %439
  %441 = sext i32 %12 to i64
  %442 = mul i64 %441, 5
  %443 = add i64 %440, %442
  %444 = add i64 %443, 2
  %445 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %444
  %446 = load double* %445, align 8
  %447 = sext i32 %8 to i64
  %448 = mul i64 %447, 21125
  %449 = sext i32 %15 to i64
  %450 = mul i64 %449, 325
  %451 = add i64 %448, %450
  %452 = sext i32 %12 to i64
  %453 = mul i64 %452, 5
  %454 = add i64 %451, %453
  %455 = add i64 %454, 3
  %456 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %455
  %457 = load double* %456, align 8
  %458 = fmul double %446, %457
  %459 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %460 = fmul double %458, %459
  %461 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %462 = fmul double %460, %461
  %463 = fsub double -0.000000e+00, %462
  %464 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %435
  store double %463, double* %464, align 8
  %465 = sext i32 %15 to i64
  %466 = mul i64 %465, 25
  %467 = add i64 %466, 24
  %468 = sext i32 %8 to i64
  %469 = mul i64 %468, 21125
  %470 = sext i32 %15 to i64
  %471 = mul i64 %470, 325
  %472 = add i64 %469, %471
  %473 = sext i32 %12 to i64
  %474 = mul i64 %473, 5
  %475 = add i64 %472, %474
  %476 = add i64 %475, 2
  %477 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %476
  %478 = load double* %477, align 8
  %479 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %480 = fmul double %478, %479
  %481 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %482 = fmul double %480, %481
  %483 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %467
  store double %482, double* %483, align 8
  %484 = sext i32 %15 to i64
  %485 = mul i64 %484, 25
  %486 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %485
  store double 0.000000e+00, double* %486, align 8
  %487 = sext i32 %15 to i64
  %488 = mul i64 %487, 25
  %489 = add i64 %488, 5
  %490 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %489
  store double 0.000000e+00, double* %490, align 8
  %491 = sext i32 %15 to i64
  %492 = mul i64 %491, 25
  %493 = add i64 %492, 10
  %494 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %493
  store double 0.000000e+00, double* %494, align 8
  %495 = sext i32 %15 to i64
  %496 = mul i64 %495, 25
  %497 = add i64 %496, 15
  %498 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %497
  store double 0.000000e+00, double* %498, align 8
  %499 = sext i32 %15 to i64
  %500 = mul i64 %499, 25
  %501 = add i64 %500, 20
  %502 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %501
  store double 0.000000e+00, double* %502, align 8
  %503 = sext i32 %15 to i64
  %504 = mul i64 %503, 25
  %505 = add i64 %504, 1
  %506 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %507 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %508 = fmul double %506, %507
  %509 = sext i32 %8 to i64
  %510 = mul i64 %509, 21125
  %511 = sext i32 %15 to i64
  %512 = mul i64 %511, 325
  %513 = add i64 %510, %512
  %514 = sext i32 %12 to i64
  %515 = mul i64 %514, 5
  %516 = add i64 %513, %515
  %517 = add i64 %516, 1
  %518 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %517
  %519 = load double* %518, align 8
  %520 = fmul double %508, %519
  %521 = fsub double -0.000000e+00, %520
  %522 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %505
  store double %521, double* %522, align 8
  %523 = sext i32 %15 to i64
  %524 = mul i64 %523, 25
  %525 = add i64 %524, 6
  %526 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %527 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %528 = fmul double %526, %527
  %529 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %525
  store double %528, double* %529, align 8
  %530 = sext i32 %15 to i64
  %531 = mul i64 %530, 25
  %532 = add i64 %531, 11
  %533 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %532
  store double 0.000000e+00, double* %533, align 8
  %534 = sext i32 %15 to i64
  %535 = mul i64 %534, 25
  %536 = add i64 %535, 16
  %537 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %536
  store double 0.000000e+00, double* %537, align 8
  %538 = sext i32 %15 to i64
  %539 = mul i64 %538, 25
  %540 = add i64 %539, 21
  %541 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %540
  store double 0.000000e+00, double* %541, align 8
  %542 = sext i32 %15 to i64
  %543 = mul i64 %542, 25
  %544 = add i64 %543, 2
  %545 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %546 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %547 = fmul double %545, %546
  %548 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %549 = fmul double %547, %548
  %550 = sext i32 %8 to i64
  %551 = mul i64 %550, 21125
  %552 = sext i32 %15 to i64
  %553 = mul i64 %552, 325
  %554 = add i64 %551, %553
  %555 = sext i32 %12 to i64
  %556 = mul i64 %555, 5
  %557 = add i64 %554, %556
  %558 = add i64 %557, 2
  %559 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %558
  %560 = load double* %559, align 8
  %561 = fmul double %549, %560
  %562 = fsub double -0.000000e+00, %561
  %563 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %544
  store double %562, double* %563, align 8
  %564 = sext i32 %15 to i64
  %565 = mul i64 %564, 25
  %566 = add i64 %565, 7
  %567 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %566
  store double 0.000000e+00, double* %567, align 8
  %568 = sext i32 %15 to i64
  %569 = mul i64 %568, 25
  %570 = add i64 %569, 12
  %571 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %572 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %573 = fmul double %571, %572
  %574 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %575 = fmul double %573, %574
  %576 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %570
  store double %575, double* %576, align 8
  %577 = sext i32 %15 to i64
  %578 = mul i64 %577, 25
  %579 = add i64 %578, 17
  %580 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %579
  store double 0.000000e+00, double* %580, align 8
  %581 = sext i32 %15 to i64
  %582 = mul i64 %581, 25
  %583 = add i64 %582, 22
  %584 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %583
  store double 0.000000e+00, double* %584, align 8
  %585 = sext i32 %15 to i64
  %586 = mul i64 %585, 25
  %587 = add i64 %586, 3
  %588 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %589 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %590 = fmul double %588, %589
  %591 = sext i32 %8 to i64
  %592 = mul i64 %591, 21125
  %593 = sext i32 %15 to i64
  %594 = mul i64 %593, 325
  %595 = add i64 %592, %594
  %596 = sext i32 %12 to i64
  %597 = mul i64 %596, 5
  %598 = add i64 %595, %597
  %599 = add i64 %598, 3
  %600 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %599
  %601 = load double* %600, align 8
  %602 = fmul double %590, %601
  %603 = fsub double -0.000000e+00, %602
  %604 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %587
  store double %603, double* %604, align 8
  %605 = sext i32 %15 to i64
  %606 = mul i64 %605, 25
  %607 = add i64 %606, 8
  %608 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %607
  store double 0.000000e+00, double* %608, align 8
  %609 = sext i32 %15 to i64
  %610 = mul i64 %609, 25
  %611 = add i64 %610, 13
  %612 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %611
  store double 0.000000e+00, double* %612, align 8
  %613 = sext i32 %15 to i64
  %614 = mul i64 %613, 25
  %615 = add i64 %614, 18
  %616 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %617 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %618 = fmul double %616, %617
  %619 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %615
  store double %618, double* %619, align 8
  %620 = sext i32 %15 to i64
  %621 = mul i64 %620, 25
  %622 = add i64 %621, 23
  %623 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %622
  store double 0.000000e+00, double* %623, align 8
  %624 = sext i32 %8 to i64
  %625 = mul i64 %624, 21125
  %626 = sext i32 %15 to i64
  %627 = mul i64 %626, 325
  %628 = add i64 %625, %627
  %629 = sext i32 %12 to i64
  %630 = mul i64 %629, 5
  %631 = add i64 %628, %630
  %632 = add i64 %631, 1
  %633 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %632
  %634 = load double* %633, align 8
  %635 = fmul double %634, %634
  %636 = sext i32 %8 to i64
  %637 = mul i64 %636, 21125
  %638 = sext i32 %15 to i64
  %639 = mul i64 %638, 325
  %640 = add i64 %637, %639
  %641 = sext i32 %12 to i64
  %642 = mul i64 %641, 5
  %643 = add i64 %640, %642
  %644 = add i64 %643, 2
  %645 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %644
  %646 = load double* %645, align 8
  %647 = fmul double %646, %646
  %648 = sext i32 %8 to i64
  %649 = mul i64 %648, 21125
  %650 = sext i32 %15 to i64
  %651 = mul i64 %650, 325
  %652 = add i64 %649, %651
  %653 = sext i32 %12 to i64
  %654 = mul i64 %653, 5
  %655 = add i64 %652, %654
  %656 = add i64 %655, 3
  %657 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %656
  %658 = load double* %657, align 8
  %659 = fmul double %658, %658
  %660 = sext i32 %15 to i64
  %661 = mul i64 %660, 25
  %662 = add i64 %661, 4
  %663 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %664 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %665 = fsub double %663, %664
  %666 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %667 = fmul double %665, %666
  %668 = fmul double %667, %635
  %669 = fsub double -0.000000e+00, %668
  %670 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %671 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %672 = fmul double %670, %671
  %673 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %674 = fsub double %672, %673
  %675 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %676 = fmul double %674, %675
  %677 = fmul double %676, %647
  %678 = fsub double %669, %677
  %679 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %680 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %681 = fsub double %679, %680
  %682 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %683 = fmul double %681, %682
  %684 = fmul double %683, %659
  %685 = fsub double %678, %684
  %686 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %687 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %688 = fmul double %686, %687
  %689 = sext i32 %8 to i64
  %690 = mul i64 %689, 21125
  %691 = sext i32 %15 to i64
  %692 = mul i64 %691, 325
  %693 = add i64 %690, %692
  %694 = sext i32 %12 to i64
  %695 = mul i64 %694, 5
  %696 = add i64 %693, %695
  %697 = add i64 %696, 4
  %698 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %697
  %699 = load double* %698, align 8
  %700 = fmul double %688, %699
  %701 = fsub double %685, %700
  %702 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %662
  store double %701, double* %702, align 8
  %703 = sext i32 %15 to i64
  %704 = mul i64 %703, 25
  %705 = add i64 %704, 9
  %706 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %707 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %708 = fsub double %706, %707
  %709 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %710 = fmul double %708, %709
  %711 = sext i32 %8 to i64
  %712 = mul i64 %711, 21125
  %713 = sext i32 %15 to i64
  %714 = mul i64 %713, 325
  %715 = add i64 %712, %714
  %716 = sext i32 %12 to i64
  %717 = mul i64 %716, 5
  %718 = add i64 %715, %717
  %719 = add i64 %718, 1
  %720 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %719
  %721 = load double* %720, align 8
  %722 = fmul double %710, %721
  %723 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %705
  store double %722, double* %723, align 8
  %724 = sext i32 %15 to i64
  %725 = mul i64 %724, 25
  %726 = add i64 %725, 14
  %727 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %728 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %729 = fmul double %727, %728
  %730 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %731 = fsub double %729, %730
  %732 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %733 = fmul double %731, %732
  %734 = sext i32 %8 to i64
  %735 = mul i64 %734, 21125
  %736 = sext i32 %15 to i64
  %737 = mul i64 %736, 325
  %738 = add i64 %735, %737
  %739 = sext i32 %12 to i64
  %740 = mul i64 %739, 5
  %741 = add i64 %738, %740
  %742 = add i64 %741, 2
  %743 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %742
  %744 = load double* %743, align 8
  %745 = fmul double %733, %744
  %746 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %726
  store double %745, double* %746, align 8
  %747 = sext i32 %15 to i64
  %748 = mul i64 %747, 25
  %749 = add i64 %748, 19
  %750 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %751 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %752 = fsub double %750, %751
  %753 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %754 = fmul double %752, %753
  %755 = sext i32 %8 to i64
  %756 = mul i64 %755, 21125
  %757 = sext i32 %15 to i64
  %758 = mul i64 %757, 325
  %759 = add i64 %756, %758
  %760 = sext i32 %12 to i64
  %761 = mul i64 %760, 5
  %762 = add i64 %759, %761
  %763 = add i64 %762, 3
  %764 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %763
  %765 = load double* %764, align 8
  %766 = fmul double %754, %765
  %767 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %749
  store double %766, double* %767, align 8
  %768 = sext i32 %15 to i64
  %769 = mul i64 %768, 25
  %770 = add i64 %769, 24
  %771 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %772 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %773 = fmul double %771, %772
  %774 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %770
  store double %773, double* %774, align 8
  %775 = icmp eq i32 %15, %13
  %776 = add i32 %15, 1
  %777 = icmp ne i1 %775, false
  br i1 %777, label %"9", label %"8"

"8":                                              ; preds = %"7"
  br label %"7"

"9":                                              ; preds = %"7", %"6"
  call void bitcast (void (...)* @lhsinit_ to void ([4875 x double]*, i32*)*)([4875 x double]* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2), i32* %jsize) #1
  %778 = load i32* %jsize, align 4
  %779 = add i32 %778, -1
  %780 = icmp sle i32 1, %779
  br i1 %780, label %"10", label %"12"

"10":                                             ; preds = %"11", %"9"
  %781 = phi i32 [ %2228, %"11" ], [ 1, %"9" ]
  %782 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %783 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 3), align 8
  %784 = fmul double %782, %783
  store double %784, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %785 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %786 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 4), align 16
  %787 = fmul double %785, %786
  store double %787, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %788 = sext i32 %781 to i64
  %789 = mul i64 %788, 75
  %790 = add i32 %781, -1
  %791 = sext i32 %790 to i64
  %792 = mul i64 %791, 25
  %793 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %792
  %794 = load double* %793, align 8
  %795 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %796 = fmul double %794, %795
  %797 = fsub double -0.000000e+00, %796
  %798 = add i32 %781, -1
  %799 = sext i32 %798 to i64
  %800 = mul i64 %799, 25
  %801 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %800
  %802 = load double* %801, align 8
  %803 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %804 = fmul double %802, %803
  %805 = fsub double %797, %804
  %806 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %807 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 14), align 16
  %808 = fmul double %806, %807
  %809 = fsub double %805, %808
  %810 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %789
  store double %809, double* %810, align 8
  %811 = sext i32 %781 to i64
  %812 = mul i64 %811, 75
  %813 = add i64 %812, 5
  %814 = add i32 %781, -1
  %815 = sext i32 %814 to i64
  %816 = mul i64 %815, 25
  %817 = add i64 %816, 5
  %818 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %817
  %819 = load double* %818, align 8
  %820 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %821 = fmul double %819, %820
  %822 = fsub double -0.000000e+00, %821
  %823 = add i32 %781, -1
  %824 = sext i32 %823 to i64
  %825 = mul i64 %824, 25
  %826 = add i64 %825, 5
  %827 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %826
  %828 = load double* %827, align 8
  %829 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %830 = fmul double %828, %829
  %831 = fsub double %822, %830
  %832 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %813
  store double %831, double* %832, align 8
  %833 = sext i32 %781 to i64
  %834 = mul i64 %833, 75
  %835 = add i64 %834, 10
  %836 = add i32 %781, -1
  %837 = sext i32 %836 to i64
  %838 = mul i64 %837, 25
  %839 = add i64 %838, 10
  %840 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %839
  %841 = load double* %840, align 8
  %842 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %843 = fmul double %841, %842
  %844 = fsub double -0.000000e+00, %843
  %845 = add i32 %781, -1
  %846 = sext i32 %845 to i64
  %847 = mul i64 %846, 25
  %848 = add i64 %847, 10
  %849 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %848
  %850 = load double* %849, align 8
  %851 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %852 = fmul double %850, %851
  %853 = fsub double %844, %852
  %854 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %835
  store double %853, double* %854, align 8
  %855 = sext i32 %781 to i64
  %856 = mul i64 %855, 75
  %857 = add i64 %856, 15
  %858 = add i32 %781, -1
  %859 = sext i32 %858 to i64
  %860 = mul i64 %859, 25
  %861 = add i64 %860, 15
  %862 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %861
  %863 = load double* %862, align 8
  %864 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %865 = fmul double %863, %864
  %866 = fsub double -0.000000e+00, %865
  %867 = add i32 %781, -1
  %868 = sext i32 %867 to i64
  %869 = mul i64 %868, 25
  %870 = add i64 %869, 15
  %871 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %870
  %872 = load double* %871, align 8
  %873 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %874 = fmul double %872, %873
  %875 = fsub double %866, %874
  %876 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %857
  store double %875, double* %876, align 8
  %877 = sext i32 %781 to i64
  %878 = mul i64 %877, 75
  %879 = add i64 %878, 20
  %880 = add i32 %781, -1
  %881 = sext i32 %880 to i64
  %882 = mul i64 %881, 25
  %883 = add i64 %882, 20
  %884 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %883
  %885 = load double* %884, align 8
  %886 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %887 = fmul double %885, %886
  %888 = fsub double -0.000000e+00, %887
  %889 = add i32 %781, -1
  %890 = sext i32 %889 to i64
  %891 = mul i64 %890, 25
  %892 = add i64 %891, 20
  %893 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %892
  %894 = load double* %893, align 8
  %895 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %896 = fmul double %894, %895
  %897 = fsub double %888, %896
  %898 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %879
  store double %897, double* %898, align 8
  %899 = sext i32 %781 to i64
  %900 = mul i64 %899, 75
  %901 = add i64 %900, 1
  %902 = add i32 %781, -1
  %903 = sext i32 %902 to i64
  %904 = mul i64 %903, 25
  %905 = add i64 %904, 1
  %906 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %905
  %907 = load double* %906, align 8
  %908 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %909 = fmul double %907, %908
  %910 = fsub double -0.000000e+00, %909
  %911 = add i32 %781, -1
  %912 = sext i32 %911 to i64
  %913 = mul i64 %912, 25
  %914 = add i64 %913, 1
  %915 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %914
  %916 = load double* %915, align 8
  %917 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %918 = fmul double %916, %917
  %919 = fsub double %910, %918
  %920 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %901
  store double %919, double* %920, align 8
  %921 = sext i32 %781 to i64
  %922 = mul i64 %921, 75
  %923 = add i64 %922, 6
  %924 = add i32 %781, -1
  %925 = sext i32 %924 to i64
  %926 = mul i64 %925, 25
  %927 = add i64 %926, 6
  %928 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %927
  %929 = load double* %928, align 8
  %930 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %931 = fmul double %929, %930
  %932 = fsub double -0.000000e+00, %931
  %933 = add i32 %781, -1
  %934 = sext i32 %933 to i64
  %935 = mul i64 %934, 25
  %936 = add i64 %935, 6
  %937 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %936
  %938 = load double* %937, align 8
  %939 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %940 = fmul double %938, %939
  %941 = fsub double %932, %940
  %942 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %943 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 15), align 8
  %944 = fmul double %942, %943
  %945 = fsub double %941, %944
  %946 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %923
  store double %945, double* %946, align 8
  %947 = sext i32 %781 to i64
  %948 = mul i64 %947, 75
  %949 = add i64 %948, 11
  %950 = add i32 %781, -1
  %951 = sext i32 %950 to i64
  %952 = mul i64 %951, 25
  %953 = add i64 %952, 11
  %954 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %953
  %955 = load double* %954, align 8
  %956 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %957 = fmul double %955, %956
  %958 = fsub double -0.000000e+00, %957
  %959 = add i32 %781, -1
  %960 = sext i32 %959 to i64
  %961 = mul i64 %960, 25
  %962 = add i64 %961, 11
  %963 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %962
  %964 = load double* %963, align 8
  %965 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %966 = fmul double %964, %965
  %967 = fsub double %958, %966
  %968 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %949
  store double %967, double* %968, align 8
  %969 = sext i32 %781 to i64
  %970 = mul i64 %969, 75
  %971 = add i64 %970, 16
  %972 = add i32 %781, -1
  %973 = sext i32 %972 to i64
  %974 = mul i64 %973, 25
  %975 = add i64 %974, 16
  %976 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %975
  %977 = load double* %976, align 8
  %978 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %979 = fmul double %977, %978
  %980 = fsub double -0.000000e+00, %979
  %981 = add i32 %781, -1
  %982 = sext i32 %981 to i64
  %983 = mul i64 %982, 25
  %984 = add i64 %983, 16
  %985 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %984
  %986 = load double* %985, align 8
  %987 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %988 = fmul double %986, %987
  %989 = fsub double %980, %988
  %990 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %971
  store double %989, double* %990, align 8
  %991 = sext i32 %781 to i64
  %992 = mul i64 %991, 75
  %993 = add i64 %992, 21
  %994 = add i32 %781, -1
  %995 = sext i32 %994 to i64
  %996 = mul i64 %995, 25
  %997 = add i64 %996, 21
  %998 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %997
  %999 = load double* %998, align 8
  %1000 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1001 = fmul double %999, %1000
  %1002 = fsub double -0.000000e+00, %1001
  %1003 = add i32 %781, -1
  %1004 = sext i32 %1003 to i64
  %1005 = mul i64 %1004, 25
  %1006 = add i64 %1005, 21
  %1007 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1006
  %1008 = load double* %1007, align 8
  %1009 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1010 = fmul double %1008, %1009
  %1011 = fsub double %1002, %1010
  %1012 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %993
  store double %1011, double* %1012, align 8
  %1013 = sext i32 %781 to i64
  %1014 = mul i64 %1013, 75
  %1015 = add i64 %1014, 2
  %1016 = add i32 %781, -1
  %1017 = sext i32 %1016 to i64
  %1018 = mul i64 %1017, 25
  %1019 = add i64 %1018, 2
  %1020 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1019
  %1021 = load double* %1020, align 8
  %1022 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1023 = fmul double %1021, %1022
  %1024 = fsub double -0.000000e+00, %1023
  %1025 = add i32 %781, -1
  %1026 = sext i32 %1025 to i64
  %1027 = mul i64 %1026, 25
  %1028 = add i64 %1027, 2
  %1029 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1028
  %1030 = load double* %1029, align 8
  %1031 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1032 = fmul double %1030, %1031
  %1033 = fsub double %1024, %1032
  %1034 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1015
  store double %1033, double* %1034, align 8
  %1035 = sext i32 %781 to i64
  %1036 = mul i64 %1035, 75
  %1037 = add i64 %1036, 7
  %1038 = add i32 %781, -1
  %1039 = sext i32 %1038 to i64
  %1040 = mul i64 %1039, 25
  %1041 = add i64 %1040, 7
  %1042 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1041
  %1043 = load double* %1042, align 8
  %1044 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1045 = fmul double %1043, %1044
  %1046 = fsub double -0.000000e+00, %1045
  %1047 = add i32 %781, -1
  %1048 = sext i32 %1047 to i64
  %1049 = mul i64 %1048, 25
  %1050 = add i64 %1049, 7
  %1051 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1050
  %1052 = load double* %1051, align 8
  %1053 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1054 = fmul double %1052, %1053
  %1055 = fsub double %1046, %1054
  %1056 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1037
  store double %1055, double* %1056, align 8
  %1057 = sext i32 %781 to i64
  %1058 = mul i64 %1057, 75
  %1059 = add i64 %1058, 12
  %1060 = add i32 %781, -1
  %1061 = sext i32 %1060 to i64
  %1062 = mul i64 %1061, 25
  %1063 = add i64 %1062, 12
  %1064 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1063
  %1065 = load double* %1064, align 8
  %1066 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1067 = fmul double %1065, %1066
  %1068 = fsub double -0.000000e+00, %1067
  %1069 = add i32 %781, -1
  %1070 = sext i32 %1069 to i64
  %1071 = mul i64 %1070, 25
  %1072 = add i64 %1071, 12
  %1073 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1072
  %1074 = load double* %1073, align 8
  %1075 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1076 = fmul double %1074, %1075
  %1077 = fsub double %1068, %1076
  %1078 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1079 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 16), align 16
  %1080 = fmul double %1078, %1079
  %1081 = fsub double %1077, %1080
  %1082 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1059
  store double %1081, double* %1082, align 8
  %1083 = sext i32 %781 to i64
  %1084 = mul i64 %1083, 75
  %1085 = add i64 %1084, 17
  %1086 = add i32 %781, -1
  %1087 = sext i32 %1086 to i64
  %1088 = mul i64 %1087, 25
  %1089 = add i64 %1088, 17
  %1090 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1089
  %1091 = load double* %1090, align 8
  %1092 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1093 = fmul double %1091, %1092
  %1094 = fsub double -0.000000e+00, %1093
  %1095 = add i32 %781, -1
  %1096 = sext i32 %1095 to i64
  %1097 = mul i64 %1096, 25
  %1098 = add i64 %1097, 17
  %1099 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1098
  %1100 = load double* %1099, align 8
  %1101 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1102 = fmul double %1100, %1101
  %1103 = fsub double %1094, %1102
  %1104 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1085
  store double %1103, double* %1104, align 8
  %1105 = sext i32 %781 to i64
  %1106 = mul i64 %1105, 75
  %1107 = add i64 %1106, 22
  %1108 = add i32 %781, -1
  %1109 = sext i32 %1108 to i64
  %1110 = mul i64 %1109, 25
  %1111 = add i64 %1110, 22
  %1112 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1111
  %1113 = load double* %1112, align 8
  %1114 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1115 = fmul double %1113, %1114
  %1116 = fsub double -0.000000e+00, %1115
  %1117 = add i32 %781, -1
  %1118 = sext i32 %1117 to i64
  %1119 = mul i64 %1118, 25
  %1120 = add i64 %1119, 22
  %1121 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1120
  %1122 = load double* %1121, align 8
  %1123 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1124 = fmul double %1122, %1123
  %1125 = fsub double %1116, %1124
  %1126 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1107
  store double %1125, double* %1126, align 8
  %1127 = sext i32 %781 to i64
  %1128 = mul i64 %1127, 75
  %1129 = add i64 %1128, 3
  %1130 = add i32 %781, -1
  %1131 = sext i32 %1130 to i64
  %1132 = mul i64 %1131, 25
  %1133 = add i64 %1132, 3
  %1134 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1133
  %1135 = load double* %1134, align 8
  %1136 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1137 = fmul double %1135, %1136
  %1138 = fsub double -0.000000e+00, %1137
  %1139 = add i32 %781, -1
  %1140 = sext i32 %1139 to i64
  %1141 = mul i64 %1140, 25
  %1142 = add i64 %1141, 3
  %1143 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1142
  %1144 = load double* %1143, align 8
  %1145 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1146 = fmul double %1144, %1145
  %1147 = fsub double %1138, %1146
  %1148 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1129
  store double %1147, double* %1148, align 8
  %1149 = sext i32 %781 to i64
  %1150 = mul i64 %1149, 75
  %1151 = add i64 %1150, 8
  %1152 = add i32 %781, -1
  %1153 = sext i32 %1152 to i64
  %1154 = mul i64 %1153, 25
  %1155 = add i64 %1154, 8
  %1156 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1155
  %1157 = load double* %1156, align 8
  %1158 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1159 = fmul double %1157, %1158
  %1160 = fsub double -0.000000e+00, %1159
  %1161 = add i32 %781, -1
  %1162 = sext i32 %1161 to i64
  %1163 = mul i64 %1162, 25
  %1164 = add i64 %1163, 8
  %1165 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1164
  %1166 = load double* %1165, align 8
  %1167 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1168 = fmul double %1166, %1167
  %1169 = fsub double %1160, %1168
  %1170 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1151
  store double %1169, double* %1170, align 8
  %1171 = sext i32 %781 to i64
  %1172 = mul i64 %1171, 75
  %1173 = add i64 %1172, 13
  %1174 = add i32 %781, -1
  %1175 = sext i32 %1174 to i64
  %1176 = mul i64 %1175, 25
  %1177 = add i64 %1176, 13
  %1178 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1177
  %1179 = load double* %1178, align 8
  %1180 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1181 = fmul double %1179, %1180
  %1182 = fsub double -0.000000e+00, %1181
  %1183 = add i32 %781, -1
  %1184 = sext i32 %1183 to i64
  %1185 = mul i64 %1184, 25
  %1186 = add i64 %1185, 13
  %1187 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1186
  %1188 = load double* %1187, align 8
  %1189 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1190 = fmul double %1188, %1189
  %1191 = fsub double %1182, %1190
  %1192 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1173
  store double %1191, double* %1192, align 8
  %1193 = sext i32 %781 to i64
  %1194 = mul i64 %1193, 75
  %1195 = add i64 %1194, 18
  %1196 = add i32 %781, -1
  %1197 = sext i32 %1196 to i64
  %1198 = mul i64 %1197, 25
  %1199 = add i64 %1198, 18
  %1200 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1199
  %1201 = load double* %1200, align 8
  %1202 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1203 = fmul double %1201, %1202
  %1204 = fsub double -0.000000e+00, %1203
  %1205 = add i32 %781, -1
  %1206 = sext i32 %1205 to i64
  %1207 = mul i64 %1206, 25
  %1208 = add i64 %1207, 18
  %1209 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1208
  %1210 = load double* %1209, align 8
  %1211 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1212 = fmul double %1210, %1211
  %1213 = fsub double %1204, %1212
  %1214 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1215 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 17), align 8
  %1216 = fmul double %1214, %1215
  %1217 = fsub double %1213, %1216
  %1218 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1195
  store double %1217, double* %1218, align 8
  %1219 = sext i32 %781 to i64
  %1220 = mul i64 %1219, 75
  %1221 = add i64 %1220, 23
  %1222 = add i32 %781, -1
  %1223 = sext i32 %1222 to i64
  %1224 = mul i64 %1223, 25
  %1225 = add i64 %1224, 23
  %1226 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1225
  %1227 = load double* %1226, align 8
  %1228 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1229 = fmul double %1227, %1228
  %1230 = fsub double -0.000000e+00, %1229
  %1231 = add i32 %781, -1
  %1232 = sext i32 %1231 to i64
  %1233 = mul i64 %1232, 25
  %1234 = add i64 %1233, 23
  %1235 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1234
  %1236 = load double* %1235, align 8
  %1237 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1238 = fmul double %1236, %1237
  %1239 = fsub double %1230, %1238
  %1240 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1221
  store double %1239, double* %1240, align 8
  %1241 = sext i32 %781 to i64
  %1242 = mul i64 %1241, 75
  %1243 = add i64 %1242, 4
  %1244 = add i32 %781, -1
  %1245 = sext i32 %1244 to i64
  %1246 = mul i64 %1245, 25
  %1247 = add i64 %1246, 4
  %1248 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1247
  %1249 = load double* %1248, align 8
  %1250 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1251 = fmul double %1249, %1250
  %1252 = fsub double -0.000000e+00, %1251
  %1253 = add i32 %781, -1
  %1254 = sext i32 %1253 to i64
  %1255 = mul i64 %1254, 25
  %1256 = add i64 %1255, 4
  %1257 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1256
  %1258 = load double* %1257, align 8
  %1259 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1260 = fmul double %1258, %1259
  %1261 = fsub double %1252, %1260
  %1262 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1243
  store double %1261, double* %1262, align 8
  %1263 = sext i32 %781 to i64
  %1264 = mul i64 %1263, 75
  %1265 = add i64 %1264, 9
  %1266 = add i32 %781, -1
  %1267 = sext i32 %1266 to i64
  %1268 = mul i64 %1267, 25
  %1269 = add i64 %1268, 9
  %1270 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1269
  %1271 = load double* %1270, align 8
  %1272 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1273 = fmul double %1271, %1272
  %1274 = fsub double -0.000000e+00, %1273
  %1275 = add i32 %781, -1
  %1276 = sext i32 %1275 to i64
  %1277 = mul i64 %1276, 25
  %1278 = add i64 %1277, 9
  %1279 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1278
  %1280 = load double* %1279, align 8
  %1281 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1282 = fmul double %1280, %1281
  %1283 = fsub double %1274, %1282
  %1284 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1265
  store double %1283, double* %1284, align 8
  %1285 = sext i32 %781 to i64
  %1286 = mul i64 %1285, 75
  %1287 = add i64 %1286, 14
  %1288 = add i32 %781, -1
  %1289 = sext i32 %1288 to i64
  %1290 = mul i64 %1289, 25
  %1291 = add i64 %1290, 14
  %1292 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1291
  %1293 = load double* %1292, align 8
  %1294 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1295 = fmul double %1293, %1294
  %1296 = fsub double -0.000000e+00, %1295
  %1297 = add i32 %781, -1
  %1298 = sext i32 %1297 to i64
  %1299 = mul i64 %1298, 25
  %1300 = add i64 %1299, 14
  %1301 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1300
  %1302 = load double* %1301, align 8
  %1303 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1304 = fmul double %1302, %1303
  %1305 = fsub double %1296, %1304
  %1306 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1287
  store double %1305, double* %1306, align 8
  %1307 = sext i32 %781 to i64
  %1308 = mul i64 %1307, 75
  %1309 = add i64 %1308, 19
  %1310 = add i32 %781, -1
  %1311 = sext i32 %1310 to i64
  %1312 = mul i64 %1311, 25
  %1313 = add i64 %1312, 19
  %1314 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1313
  %1315 = load double* %1314, align 8
  %1316 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1317 = fmul double %1315, %1316
  %1318 = fsub double -0.000000e+00, %1317
  %1319 = add i32 %781, -1
  %1320 = sext i32 %1319 to i64
  %1321 = mul i64 %1320, 25
  %1322 = add i64 %1321, 19
  %1323 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1322
  %1324 = load double* %1323, align 8
  %1325 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1326 = fmul double %1324, %1325
  %1327 = fsub double %1318, %1326
  %1328 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1309
  store double %1327, double* %1328, align 8
  %1329 = sext i32 %781 to i64
  %1330 = mul i64 %1329, 75
  %1331 = add i64 %1330, 24
  %1332 = add i32 %781, -1
  %1333 = sext i32 %1332 to i64
  %1334 = mul i64 %1333, 25
  %1335 = add i64 %1334, 24
  %1336 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1335
  %1337 = load double* %1336, align 8
  %1338 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1339 = fmul double %1337, %1338
  %1340 = fsub double -0.000000e+00, %1339
  %1341 = add i32 %781, -1
  %1342 = sext i32 %1341 to i64
  %1343 = mul i64 %1342, 25
  %1344 = add i64 %1343, 24
  %1345 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1344
  %1346 = load double* %1345, align 8
  %1347 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1348 = fmul double %1346, %1347
  %1349 = fsub double %1340, %1348
  %1350 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1351 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 18), align 16
  %1352 = fmul double %1350, %1351
  %1353 = fsub double %1349, %1352
  %1354 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1331
  store double %1353, double* %1354, align 8
  %1355 = sext i32 %781 to i64
  %1356 = mul i64 %1355, 75
  %1357 = add i64 %1356, 25
  %1358 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1359 = fmul double %1358, 2.000000e+00
  %1360 = sext i32 %781 to i64
  %1361 = mul i64 %1360, 25
  %1362 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1361
  %1363 = load double* %1362, align 8
  %1364 = fmul double %1359, %1363
  %1365 = fadd double %1364, 1.000000e+00
  %1366 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1367 = fmul double %1366, 2.000000e+00
  %1368 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 14), align 16
  %1369 = fmul double %1367, %1368
  %1370 = fadd double %1365, %1369
  %1371 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1357
  store double %1370, double* %1371, align 8
  %1372 = sext i32 %781 to i64
  %1373 = mul i64 %1372, 75
  %1374 = add i64 %1373, 30
  %1375 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1376 = fmul double %1375, 2.000000e+00
  %1377 = sext i32 %781 to i64
  %1378 = mul i64 %1377, 25
  %1379 = add i64 %1378, 5
  %1380 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1379
  %1381 = load double* %1380, align 8
  %1382 = fmul double %1376, %1381
  %1383 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1374
  store double %1382, double* %1383, align 8
  %1384 = sext i32 %781 to i64
  %1385 = mul i64 %1384, 75
  %1386 = add i64 %1385, 35
  %1387 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1388 = fmul double %1387, 2.000000e+00
  %1389 = sext i32 %781 to i64
  %1390 = mul i64 %1389, 25
  %1391 = add i64 %1390, 10
  %1392 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1391
  %1393 = load double* %1392, align 8
  %1394 = fmul double %1388, %1393
  %1395 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1386
  store double %1394, double* %1395, align 8
  %1396 = sext i32 %781 to i64
  %1397 = mul i64 %1396, 75
  %1398 = add i64 %1397, 40
  %1399 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1400 = fmul double %1399, 2.000000e+00
  %1401 = sext i32 %781 to i64
  %1402 = mul i64 %1401, 25
  %1403 = add i64 %1402, 15
  %1404 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1403
  %1405 = load double* %1404, align 8
  %1406 = fmul double %1400, %1405
  %1407 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1398
  store double %1406, double* %1407, align 8
  %1408 = sext i32 %781 to i64
  %1409 = mul i64 %1408, 75
  %1410 = add i64 %1409, 45
  %1411 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1412 = fmul double %1411, 2.000000e+00
  %1413 = sext i32 %781 to i64
  %1414 = mul i64 %1413, 25
  %1415 = add i64 %1414, 20
  %1416 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1415
  %1417 = load double* %1416, align 8
  %1418 = fmul double %1412, %1417
  %1419 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1410
  store double %1418, double* %1419, align 8
  %1420 = sext i32 %781 to i64
  %1421 = mul i64 %1420, 75
  %1422 = add i64 %1421, 26
  %1423 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1424 = fmul double %1423, 2.000000e+00
  %1425 = sext i32 %781 to i64
  %1426 = mul i64 %1425, 25
  %1427 = add i64 %1426, 1
  %1428 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1427
  %1429 = load double* %1428, align 8
  %1430 = fmul double %1424, %1429
  %1431 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1422
  store double %1430, double* %1431, align 8
  %1432 = sext i32 %781 to i64
  %1433 = mul i64 %1432, 75
  %1434 = add i64 %1433, 31
  %1435 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1436 = fmul double %1435, 2.000000e+00
  %1437 = sext i32 %781 to i64
  %1438 = mul i64 %1437, 25
  %1439 = add i64 %1438, 6
  %1440 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1439
  %1441 = load double* %1440, align 8
  %1442 = fmul double %1436, %1441
  %1443 = fadd double %1442, 1.000000e+00
  %1444 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1445 = fmul double %1444, 2.000000e+00
  %1446 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 15), align 8
  %1447 = fmul double %1445, %1446
  %1448 = fadd double %1443, %1447
  %1449 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1434
  store double %1448, double* %1449, align 8
  %1450 = sext i32 %781 to i64
  %1451 = mul i64 %1450, 75
  %1452 = add i64 %1451, 36
  %1453 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1454 = fmul double %1453, 2.000000e+00
  %1455 = sext i32 %781 to i64
  %1456 = mul i64 %1455, 25
  %1457 = add i64 %1456, 11
  %1458 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1457
  %1459 = load double* %1458, align 8
  %1460 = fmul double %1454, %1459
  %1461 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1452
  store double %1460, double* %1461, align 8
  %1462 = sext i32 %781 to i64
  %1463 = mul i64 %1462, 75
  %1464 = add i64 %1463, 41
  %1465 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1466 = fmul double %1465, 2.000000e+00
  %1467 = sext i32 %781 to i64
  %1468 = mul i64 %1467, 25
  %1469 = add i64 %1468, 16
  %1470 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1469
  %1471 = load double* %1470, align 8
  %1472 = fmul double %1466, %1471
  %1473 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1464
  store double %1472, double* %1473, align 8
  %1474 = sext i32 %781 to i64
  %1475 = mul i64 %1474, 75
  %1476 = add i64 %1475, 46
  %1477 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1478 = fmul double %1477, 2.000000e+00
  %1479 = sext i32 %781 to i64
  %1480 = mul i64 %1479, 25
  %1481 = add i64 %1480, 21
  %1482 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1481
  %1483 = load double* %1482, align 8
  %1484 = fmul double %1478, %1483
  %1485 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1476
  store double %1484, double* %1485, align 8
  %1486 = sext i32 %781 to i64
  %1487 = mul i64 %1486, 75
  %1488 = add i64 %1487, 27
  %1489 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1490 = fmul double %1489, 2.000000e+00
  %1491 = sext i32 %781 to i64
  %1492 = mul i64 %1491, 25
  %1493 = add i64 %1492, 2
  %1494 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1493
  %1495 = load double* %1494, align 8
  %1496 = fmul double %1490, %1495
  %1497 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1488
  store double %1496, double* %1497, align 8
  %1498 = sext i32 %781 to i64
  %1499 = mul i64 %1498, 75
  %1500 = add i64 %1499, 32
  %1501 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1502 = fmul double %1501, 2.000000e+00
  %1503 = sext i32 %781 to i64
  %1504 = mul i64 %1503, 25
  %1505 = add i64 %1504, 7
  %1506 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1505
  %1507 = load double* %1506, align 8
  %1508 = fmul double %1502, %1507
  %1509 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1500
  store double %1508, double* %1509, align 8
  %1510 = sext i32 %781 to i64
  %1511 = mul i64 %1510, 75
  %1512 = add i64 %1511, 37
  %1513 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1514 = fmul double %1513, 2.000000e+00
  %1515 = sext i32 %781 to i64
  %1516 = mul i64 %1515, 25
  %1517 = add i64 %1516, 12
  %1518 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1517
  %1519 = load double* %1518, align 8
  %1520 = fmul double %1514, %1519
  %1521 = fadd double %1520, 1.000000e+00
  %1522 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1523 = fmul double %1522, 2.000000e+00
  %1524 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 16), align 16
  %1525 = fmul double %1523, %1524
  %1526 = fadd double %1521, %1525
  %1527 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1512
  store double %1526, double* %1527, align 8
  %1528 = sext i32 %781 to i64
  %1529 = mul i64 %1528, 75
  %1530 = add i64 %1529, 42
  %1531 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1532 = fmul double %1531, 2.000000e+00
  %1533 = sext i32 %781 to i64
  %1534 = mul i64 %1533, 25
  %1535 = add i64 %1534, 17
  %1536 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1535
  %1537 = load double* %1536, align 8
  %1538 = fmul double %1532, %1537
  %1539 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1530
  store double %1538, double* %1539, align 8
  %1540 = sext i32 %781 to i64
  %1541 = mul i64 %1540, 75
  %1542 = add i64 %1541, 47
  %1543 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1544 = fmul double %1543, 2.000000e+00
  %1545 = sext i32 %781 to i64
  %1546 = mul i64 %1545, 25
  %1547 = add i64 %1546, 22
  %1548 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1547
  %1549 = load double* %1548, align 8
  %1550 = fmul double %1544, %1549
  %1551 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1542
  store double %1550, double* %1551, align 8
  %1552 = sext i32 %781 to i64
  %1553 = mul i64 %1552, 75
  %1554 = add i64 %1553, 28
  %1555 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1556 = fmul double %1555, 2.000000e+00
  %1557 = sext i32 %781 to i64
  %1558 = mul i64 %1557, 25
  %1559 = add i64 %1558, 3
  %1560 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1559
  %1561 = load double* %1560, align 8
  %1562 = fmul double %1556, %1561
  %1563 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1554
  store double %1562, double* %1563, align 8
  %1564 = sext i32 %781 to i64
  %1565 = mul i64 %1564, 75
  %1566 = add i64 %1565, 33
  %1567 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1568 = fmul double %1567, 2.000000e+00
  %1569 = sext i32 %781 to i64
  %1570 = mul i64 %1569, 25
  %1571 = add i64 %1570, 8
  %1572 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1571
  %1573 = load double* %1572, align 8
  %1574 = fmul double %1568, %1573
  %1575 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1566
  store double %1574, double* %1575, align 8
  %1576 = sext i32 %781 to i64
  %1577 = mul i64 %1576, 75
  %1578 = add i64 %1577, 38
  %1579 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1580 = fmul double %1579, 2.000000e+00
  %1581 = sext i32 %781 to i64
  %1582 = mul i64 %1581, 25
  %1583 = add i64 %1582, 13
  %1584 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1583
  %1585 = load double* %1584, align 8
  %1586 = fmul double %1580, %1585
  %1587 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1578
  store double %1586, double* %1587, align 8
  %1588 = sext i32 %781 to i64
  %1589 = mul i64 %1588, 75
  %1590 = add i64 %1589, 43
  %1591 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1592 = fmul double %1591, 2.000000e+00
  %1593 = sext i32 %781 to i64
  %1594 = mul i64 %1593, 25
  %1595 = add i64 %1594, 18
  %1596 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1595
  %1597 = load double* %1596, align 8
  %1598 = fmul double %1592, %1597
  %1599 = fadd double %1598, 1.000000e+00
  %1600 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1601 = fmul double %1600, 2.000000e+00
  %1602 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 17), align 8
  %1603 = fmul double %1601, %1602
  %1604 = fadd double %1599, %1603
  %1605 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1590
  store double %1604, double* %1605, align 8
  %1606 = sext i32 %781 to i64
  %1607 = mul i64 %1606, 75
  %1608 = add i64 %1607, 48
  %1609 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1610 = fmul double %1609, 2.000000e+00
  %1611 = sext i32 %781 to i64
  %1612 = mul i64 %1611, 25
  %1613 = add i64 %1612, 23
  %1614 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1613
  %1615 = load double* %1614, align 8
  %1616 = fmul double %1610, %1615
  %1617 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1608
  store double %1616, double* %1617, align 8
  %1618 = sext i32 %781 to i64
  %1619 = mul i64 %1618, 75
  %1620 = add i64 %1619, 29
  %1621 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1622 = fmul double %1621, 2.000000e+00
  %1623 = sext i32 %781 to i64
  %1624 = mul i64 %1623, 25
  %1625 = add i64 %1624, 4
  %1626 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1625
  %1627 = load double* %1626, align 8
  %1628 = fmul double %1622, %1627
  %1629 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1620
  store double %1628, double* %1629, align 8
  %1630 = sext i32 %781 to i64
  %1631 = mul i64 %1630, 75
  %1632 = add i64 %1631, 34
  %1633 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1634 = fmul double %1633, 2.000000e+00
  %1635 = sext i32 %781 to i64
  %1636 = mul i64 %1635, 25
  %1637 = add i64 %1636, 9
  %1638 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1637
  %1639 = load double* %1638, align 8
  %1640 = fmul double %1634, %1639
  %1641 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1632
  store double %1640, double* %1641, align 8
  %1642 = sext i32 %781 to i64
  %1643 = mul i64 %1642, 75
  %1644 = add i64 %1643, 39
  %1645 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1646 = fmul double %1645, 2.000000e+00
  %1647 = sext i32 %781 to i64
  %1648 = mul i64 %1647, 25
  %1649 = add i64 %1648, 14
  %1650 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1649
  %1651 = load double* %1650, align 8
  %1652 = fmul double %1646, %1651
  %1653 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1644
  store double %1652, double* %1653, align 8
  %1654 = sext i32 %781 to i64
  %1655 = mul i64 %1654, 75
  %1656 = add i64 %1655, 44
  %1657 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1658 = fmul double %1657, 2.000000e+00
  %1659 = sext i32 %781 to i64
  %1660 = mul i64 %1659, 25
  %1661 = add i64 %1660, 19
  %1662 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1661
  %1663 = load double* %1662, align 8
  %1664 = fmul double %1658, %1663
  %1665 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1656
  store double %1664, double* %1665, align 8
  %1666 = sext i32 %781 to i64
  %1667 = mul i64 %1666, 75
  %1668 = add i64 %1667, 49
  %1669 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1670 = fmul double %1669, 2.000000e+00
  %1671 = sext i32 %781 to i64
  %1672 = mul i64 %1671, 25
  %1673 = add i64 %1672, 24
  %1674 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1673
  %1675 = load double* %1674, align 8
  %1676 = fmul double %1670, %1675
  %1677 = fadd double %1676, 1.000000e+00
  %1678 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1679 = fmul double %1678, 2.000000e+00
  %1680 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 18), align 16
  %1681 = fmul double %1679, %1680
  %1682 = fadd double %1677, %1681
  %1683 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1668
  store double %1682, double* %1683, align 8
  %1684 = sext i32 %781 to i64
  %1685 = mul i64 %1684, 75
  %1686 = add i64 %1685, 50
  %1687 = add i32 %781, 1
  %1688 = sext i32 %1687 to i64
  %1689 = mul i64 %1688, 25
  %1690 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1689
  %1691 = load double* %1690, align 8
  %1692 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1693 = fmul double %1691, %1692
  %1694 = add i32 %781, 1
  %1695 = sext i32 %1694 to i64
  %1696 = mul i64 %1695, 25
  %1697 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1696
  %1698 = load double* %1697, align 8
  %1699 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1700 = fmul double %1698, %1699
  %1701 = fsub double %1693, %1700
  %1702 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1703 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 14), align 16
  %1704 = fmul double %1702, %1703
  %1705 = fsub double %1701, %1704
  %1706 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1686
  store double %1705, double* %1706, align 8
  %1707 = sext i32 %781 to i64
  %1708 = mul i64 %1707, 75
  %1709 = add i64 %1708, 55
  %1710 = add i32 %781, 1
  %1711 = sext i32 %1710 to i64
  %1712 = mul i64 %1711, 25
  %1713 = add i64 %1712, 5
  %1714 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1713
  %1715 = load double* %1714, align 8
  %1716 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1717 = fmul double %1715, %1716
  %1718 = add i32 %781, 1
  %1719 = sext i32 %1718 to i64
  %1720 = mul i64 %1719, 25
  %1721 = add i64 %1720, 5
  %1722 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1721
  %1723 = load double* %1722, align 8
  %1724 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1725 = fmul double %1723, %1724
  %1726 = fsub double %1717, %1725
  %1727 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1709
  store double %1726, double* %1727, align 8
  %1728 = sext i32 %781 to i64
  %1729 = mul i64 %1728, 75
  %1730 = add i64 %1729, 60
  %1731 = add i32 %781, 1
  %1732 = sext i32 %1731 to i64
  %1733 = mul i64 %1732, 25
  %1734 = add i64 %1733, 10
  %1735 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1734
  %1736 = load double* %1735, align 8
  %1737 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1738 = fmul double %1736, %1737
  %1739 = add i32 %781, 1
  %1740 = sext i32 %1739 to i64
  %1741 = mul i64 %1740, 25
  %1742 = add i64 %1741, 10
  %1743 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1742
  %1744 = load double* %1743, align 8
  %1745 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1746 = fmul double %1744, %1745
  %1747 = fsub double %1738, %1746
  %1748 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1730
  store double %1747, double* %1748, align 8
  %1749 = sext i32 %781 to i64
  %1750 = mul i64 %1749, 75
  %1751 = add i64 %1750, 65
  %1752 = add i32 %781, 1
  %1753 = sext i32 %1752 to i64
  %1754 = mul i64 %1753, 25
  %1755 = add i64 %1754, 15
  %1756 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1755
  %1757 = load double* %1756, align 8
  %1758 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1759 = fmul double %1757, %1758
  %1760 = add i32 %781, 1
  %1761 = sext i32 %1760 to i64
  %1762 = mul i64 %1761, 25
  %1763 = add i64 %1762, 15
  %1764 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1763
  %1765 = load double* %1764, align 8
  %1766 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1767 = fmul double %1765, %1766
  %1768 = fsub double %1759, %1767
  %1769 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1751
  store double %1768, double* %1769, align 8
  %1770 = sext i32 %781 to i64
  %1771 = mul i64 %1770, 75
  %1772 = add i64 %1771, 70
  %1773 = add i32 %781, 1
  %1774 = sext i32 %1773 to i64
  %1775 = mul i64 %1774, 25
  %1776 = add i64 %1775, 20
  %1777 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1776
  %1778 = load double* %1777, align 8
  %1779 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1780 = fmul double %1778, %1779
  %1781 = add i32 %781, 1
  %1782 = sext i32 %1781 to i64
  %1783 = mul i64 %1782, 25
  %1784 = add i64 %1783, 20
  %1785 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1784
  %1786 = load double* %1785, align 8
  %1787 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1788 = fmul double %1786, %1787
  %1789 = fsub double %1780, %1788
  %1790 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1772
  store double %1789, double* %1790, align 8
  %1791 = sext i32 %781 to i64
  %1792 = mul i64 %1791, 75
  %1793 = add i64 %1792, 51
  %1794 = add i32 %781, 1
  %1795 = sext i32 %1794 to i64
  %1796 = mul i64 %1795, 25
  %1797 = add i64 %1796, 1
  %1798 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1797
  %1799 = load double* %1798, align 8
  %1800 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1801 = fmul double %1799, %1800
  %1802 = add i32 %781, 1
  %1803 = sext i32 %1802 to i64
  %1804 = mul i64 %1803, 25
  %1805 = add i64 %1804, 1
  %1806 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1805
  %1807 = load double* %1806, align 8
  %1808 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1809 = fmul double %1807, %1808
  %1810 = fsub double %1801, %1809
  %1811 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1793
  store double %1810, double* %1811, align 8
  %1812 = sext i32 %781 to i64
  %1813 = mul i64 %1812, 75
  %1814 = add i64 %1813, 56
  %1815 = add i32 %781, 1
  %1816 = sext i32 %1815 to i64
  %1817 = mul i64 %1816, 25
  %1818 = add i64 %1817, 6
  %1819 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1818
  %1820 = load double* %1819, align 8
  %1821 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1822 = fmul double %1820, %1821
  %1823 = add i32 %781, 1
  %1824 = sext i32 %1823 to i64
  %1825 = mul i64 %1824, 25
  %1826 = add i64 %1825, 6
  %1827 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1826
  %1828 = load double* %1827, align 8
  %1829 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1830 = fmul double %1828, %1829
  %1831 = fsub double %1822, %1830
  %1832 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1833 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 15), align 8
  %1834 = fmul double %1832, %1833
  %1835 = fsub double %1831, %1834
  %1836 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1814
  store double %1835, double* %1836, align 8
  %1837 = sext i32 %781 to i64
  %1838 = mul i64 %1837, 75
  %1839 = add i64 %1838, 61
  %1840 = add i32 %781, 1
  %1841 = sext i32 %1840 to i64
  %1842 = mul i64 %1841, 25
  %1843 = add i64 %1842, 11
  %1844 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1843
  %1845 = load double* %1844, align 8
  %1846 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1847 = fmul double %1845, %1846
  %1848 = add i32 %781, 1
  %1849 = sext i32 %1848 to i64
  %1850 = mul i64 %1849, 25
  %1851 = add i64 %1850, 11
  %1852 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1851
  %1853 = load double* %1852, align 8
  %1854 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1855 = fmul double %1853, %1854
  %1856 = fsub double %1847, %1855
  %1857 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1839
  store double %1856, double* %1857, align 8
  %1858 = sext i32 %781 to i64
  %1859 = mul i64 %1858, 75
  %1860 = add i64 %1859, 66
  %1861 = add i32 %781, 1
  %1862 = sext i32 %1861 to i64
  %1863 = mul i64 %1862, 25
  %1864 = add i64 %1863, 16
  %1865 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1864
  %1866 = load double* %1865, align 8
  %1867 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1868 = fmul double %1866, %1867
  %1869 = add i32 %781, 1
  %1870 = sext i32 %1869 to i64
  %1871 = mul i64 %1870, 25
  %1872 = add i64 %1871, 16
  %1873 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1872
  %1874 = load double* %1873, align 8
  %1875 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1876 = fmul double %1874, %1875
  %1877 = fsub double %1868, %1876
  %1878 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1860
  store double %1877, double* %1878, align 8
  %1879 = sext i32 %781 to i64
  %1880 = mul i64 %1879, 75
  %1881 = add i64 %1880, 71
  %1882 = add i32 %781, 1
  %1883 = sext i32 %1882 to i64
  %1884 = mul i64 %1883, 25
  %1885 = add i64 %1884, 21
  %1886 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1885
  %1887 = load double* %1886, align 8
  %1888 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1889 = fmul double %1887, %1888
  %1890 = add i32 %781, 1
  %1891 = sext i32 %1890 to i64
  %1892 = mul i64 %1891, 25
  %1893 = add i64 %1892, 21
  %1894 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1893
  %1895 = load double* %1894, align 8
  %1896 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1897 = fmul double %1895, %1896
  %1898 = fsub double %1889, %1897
  %1899 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1881
  store double %1898, double* %1899, align 8
  %1900 = sext i32 %781 to i64
  %1901 = mul i64 %1900, 75
  %1902 = add i64 %1901, 52
  %1903 = add i32 %781, 1
  %1904 = sext i32 %1903 to i64
  %1905 = mul i64 %1904, 25
  %1906 = add i64 %1905, 2
  %1907 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1906
  %1908 = load double* %1907, align 8
  %1909 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1910 = fmul double %1908, %1909
  %1911 = add i32 %781, 1
  %1912 = sext i32 %1911 to i64
  %1913 = mul i64 %1912, 25
  %1914 = add i64 %1913, 2
  %1915 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1914
  %1916 = load double* %1915, align 8
  %1917 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1918 = fmul double %1916, %1917
  %1919 = fsub double %1910, %1918
  %1920 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1902
  store double %1919, double* %1920, align 8
  %1921 = sext i32 %781 to i64
  %1922 = mul i64 %1921, 75
  %1923 = add i64 %1922, 57
  %1924 = add i32 %781, 1
  %1925 = sext i32 %1924 to i64
  %1926 = mul i64 %1925, 25
  %1927 = add i64 %1926, 7
  %1928 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1927
  %1929 = load double* %1928, align 8
  %1930 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1931 = fmul double %1929, %1930
  %1932 = add i32 %781, 1
  %1933 = sext i32 %1932 to i64
  %1934 = mul i64 %1933, 25
  %1935 = add i64 %1934, 7
  %1936 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1935
  %1937 = load double* %1936, align 8
  %1938 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1939 = fmul double %1937, %1938
  %1940 = fsub double %1931, %1939
  %1941 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1923
  store double %1940, double* %1941, align 8
  %1942 = sext i32 %781 to i64
  %1943 = mul i64 %1942, 75
  %1944 = add i64 %1943, 62
  %1945 = add i32 %781, 1
  %1946 = sext i32 %1945 to i64
  %1947 = mul i64 %1946, 25
  %1948 = add i64 %1947, 12
  %1949 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1948
  %1950 = load double* %1949, align 8
  %1951 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1952 = fmul double %1950, %1951
  %1953 = add i32 %781, 1
  %1954 = sext i32 %1953 to i64
  %1955 = mul i64 %1954, 25
  %1956 = add i64 %1955, 12
  %1957 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1956
  %1958 = load double* %1957, align 8
  %1959 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1960 = fmul double %1958, %1959
  %1961 = fsub double %1952, %1960
  %1962 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1963 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 16), align 16
  %1964 = fmul double %1962, %1963
  %1965 = fsub double %1961, %1964
  %1966 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1944
  store double %1965, double* %1966, align 8
  %1967 = sext i32 %781 to i64
  %1968 = mul i64 %1967, 75
  %1969 = add i64 %1968, 67
  %1970 = add i32 %781, 1
  %1971 = sext i32 %1970 to i64
  %1972 = mul i64 %1971, 25
  %1973 = add i64 %1972, 17
  %1974 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1973
  %1975 = load double* %1974, align 8
  %1976 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1977 = fmul double %1975, %1976
  %1978 = add i32 %781, 1
  %1979 = sext i32 %1978 to i64
  %1980 = mul i64 %1979, 25
  %1981 = add i64 %1980, 17
  %1982 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1981
  %1983 = load double* %1982, align 8
  %1984 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1985 = fmul double %1983, %1984
  %1986 = fsub double %1977, %1985
  %1987 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1969
  store double %1986, double* %1987, align 8
  %1988 = sext i32 %781 to i64
  %1989 = mul i64 %1988, 75
  %1990 = add i64 %1989, 72
  %1991 = add i32 %781, 1
  %1992 = sext i32 %1991 to i64
  %1993 = mul i64 %1992, 25
  %1994 = add i64 %1993, 22
  %1995 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1994
  %1996 = load double* %1995, align 8
  %1997 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1998 = fmul double %1996, %1997
  %1999 = add i32 %781, 1
  %2000 = sext i32 %1999 to i64
  %2001 = mul i64 %2000, 25
  %2002 = add i64 %2001, 22
  %2003 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2002
  %2004 = load double* %2003, align 8
  %2005 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2006 = fmul double %2004, %2005
  %2007 = fsub double %1998, %2006
  %2008 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1990
  store double %2007, double* %2008, align 8
  %2009 = sext i32 %781 to i64
  %2010 = mul i64 %2009, 75
  %2011 = add i64 %2010, 53
  %2012 = add i32 %781, 1
  %2013 = sext i32 %2012 to i64
  %2014 = mul i64 %2013, 25
  %2015 = add i64 %2014, 3
  %2016 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2015
  %2017 = load double* %2016, align 8
  %2018 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2019 = fmul double %2017, %2018
  %2020 = add i32 %781, 1
  %2021 = sext i32 %2020 to i64
  %2022 = mul i64 %2021, 25
  %2023 = add i64 %2022, 3
  %2024 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2023
  %2025 = load double* %2024, align 8
  %2026 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2027 = fmul double %2025, %2026
  %2028 = fsub double %2019, %2027
  %2029 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2011
  store double %2028, double* %2029, align 8
  %2030 = sext i32 %781 to i64
  %2031 = mul i64 %2030, 75
  %2032 = add i64 %2031, 58
  %2033 = add i32 %781, 1
  %2034 = sext i32 %2033 to i64
  %2035 = mul i64 %2034, 25
  %2036 = add i64 %2035, 8
  %2037 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2036
  %2038 = load double* %2037, align 8
  %2039 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2040 = fmul double %2038, %2039
  %2041 = add i32 %781, 1
  %2042 = sext i32 %2041 to i64
  %2043 = mul i64 %2042, 25
  %2044 = add i64 %2043, 8
  %2045 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2044
  %2046 = load double* %2045, align 8
  %2047 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2048 = fmul double %2046, %2047
  %2049 = fsub double %2040, %2048
  %2050 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2032
  store double %2049, double* %2050, align 8
  %2051 = sext i32 %781 to i64
  %2052 = mul i64 %2051, 75
  %2053 = add i64 %2052, 63
  %2054 = add i32 %781, 1
  %2055 = sext i32 %2054 to i64
  %2056 = mul i64 %2055, 25
  %2057 = add i64 %2056, 13
  %2058 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2057
  %2059 = load double* %2058, align 8
  %2060 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2061 = fmul double %2059, %2060
  %2062 = add i32 %781, 1
  %2063 = sext i32 %2062 to i64
  %2064 = mul i64 %2063, 25
  %2065 = add i64 %2064, 13
  %2066 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2065
  %2067 = load double* %2066, align 8
  %2068 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2069 = fmul double %2067, %2068
  %2070 = fsub double %2061, %2069
  %2071 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2053
  store double %2070, double* %2071, align 8
  %2072 = sext i32 %781 to i64
  %2073 = mul i64 %2072, 75
  %2074 = add i64 %2073, 68
  %2075 = add i32 %781, 1
  %2076 = sext i32 %2075 to i64
  %2077 = mul i64 %2076, 25
  %2078 = add i64 %2077, 18
  %2079 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2078
  %2080 = load double* %2079, align 8
  %2081 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2082 = fmul double %2080, %2081
  %2083 = add i32 %781, 1
  %2084 = sext i32 %2083 to i64
  %2085 = mul i64 %2084, 25
  %2086 = add i64 %2085, 18
  %2087 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2086
  %2088 = load double* %2087, align 8
  %2089 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2090 = fmul double %2088, %2089
  %2091 = fsub double %2082, %2090
  %2092 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2093 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 17), align 8
  %2094 = fmul double %2092, %2093
  %2095 = fsub double %2091, %2094
  %2096 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2074
  store double %2095, double* %2096, align 8
  %2097 = sext i32 %781 to i64
  %2098 = mul i64 %2097, 75
  %2099 = add i64 %2098, 73
  %2100 = add i32 %781, 1
  %2101 = sext i32 %2100 to i64
  %2102 = mul i64 %2101, 25
  %2103 = add i64 %2102, 23
  %2104 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2103
  %2105 = load double* %2104, align 8
  %2106 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2107 = fmul double %2105, %2106
  %2108 = add i32 %781, 1
  %2109 = sext i32 %2108 to i64
  %2110 = mul i64 %2109, 25
  %2111 = add i64 %2110, 23
  %2112 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2111
  %2113 = load double* %2112, align 8
  %2114 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2115 = fmul double %2113, %2114
  %2116 = fsub double %2107, %2115
  %2117 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2099
  store double %2116, double* %2117, align 8
  %2118 = sext i32 %781 to i64
  %2119 = mul i64 %2118, 75
  %2120 = add i64 %2119, 54
  %2121 = add i32 %781, 1
  %2122 = sext i32 %2121 to i64
  %2123 = mul i64 %2122, 25
  %2124 = add i64 %2123, 4
  %2125 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2124
  %2126 = load double* %2125, align 8
  %2127 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2128 = fmul double %2126, %2127
  %2129 = add i32 %781, 1
  %2130 = sext i32 %2129 to i64
  %2131 = mul i64 %2130, 25
  %2132 = add i64 %2131, 4
  %2133 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2132
  %2134 = load double* %2133, align 8
  %2135 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2136 = fmul double %2134, %2135
  %2137 = fsub double %2128, %2136
  %2138 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2120
  store double %2137, double* %2138, align 8
  %2139 = sext i32 %781 to i64
  %2140 = mul i64 %2139, 75
  %2141 = add i64 %2140, 59
  %2142 = add i32 %781, 1
  %2143 = sext i32 %2142 to i64
  %2144 = mul i64 %2143, 25
  %2145 = add i64 %2144, 9
  %2146 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2145
  %2147 = load double* %2146, align 8
  %2148 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2149 = fmul double %2147, %2148
  %2150 = add i32 %781, 1
  %2151 = sext i32 %2150 to i64
  %2152 = mul i64 %2151, 25
  %2153 = add i64 %2152, 9
  %2154 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2153
  %2155 = load double* %2154, align 8
  %2156 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2157 = fmul double %2155, %2156
  %2158 = fsub double %2149, %2157
  %2159 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2141
  store double %2158, double* %2159, align 8
  %2160 = sext i32 %781 to i64
  %2161 = mul i64 %2160, 75
  %2162 = add i64 %2161, 64
  %2163 = add i32 %781, 1
  %2164 = sext i32 %2163 to i64
  %2165 = mul i64 %2164, 25
  %2166 = add i64 %2165, 14
  %2167 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2166
  %2168 = load double* %2167, align 8
  %2169 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2170 = fmul double %2168, %2169
  %2171 = add i32 %781, 1
  %2172 = sext i32 %2171 to i64
  %2173 = mul i64 %2172, 25
  %2174 = add i64 %2173, 14
  %2175 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2174
  %2176 = load double* %2175, align 8
  %2177 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2178 = fmul double %2176, %2177
  %2179 = fsub double %2170, %2178
  %2180 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2162
  store double %2179, double* %2180, align 8
  %2181 = sext i32 %781 to i64
  %2182 = mul i64 %2181, 75
  %2183 = add i64 %2182, 69
  %2184 = add i32 %781, 1
  %2185 = sext i32 %2184 to i64
  %2186 = mul i64 %2185, 25
  %2187 = add i64 %2186, 19
  %2188 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2187
  %2189 = load double* %2188, align 8
  %2190 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2191 = fmul double %2189, %2190
  %2192 = add i32 %781, 1
  %2193 = sext i32 %2192 to i64
  %2194 = mul i64 %2193, 25
  %2195 = add i64 %2194, 19
  %2196 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2195
  %2197 = load double* %2196, align 8
  %2198 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2199 = fmul double %2197, %2198
  %2200 = fsub double %2191, %2199
  %2201 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2183
  store double %2200, double* %2201, align 8
  %2202 = sext i32 %781 to i64
  %2203 = mul i64 %2202, 75
  %2204 = add i64 %2203, 74
  %2205 = add i32 %781, 1
  %2206 = sext i32 %2205 to i64
  %2207 = mul i64 %2206, 25
  %2208 = add i64 %2207, 24
  %2209 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2208
  %2210 = load double* %2209, align 8
  %2211 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2212 = fmul double %2210, %2211
  %2213 = add i32 %781, 1
  %2214 = sext i32 %2213 to i64
  %2215 = mul i64 %2214, 25
  %2216 = add i64 %2215, 24
  %2217 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2216
  %2218 = load double* %2217, align 8
  %2219 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2220 = fmul double %2218, %2219
  %2221 = fsub double %2212, %2220
  %2222 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2223 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 18), align 16
  %2224 = fmul double %2222, %2223
  %2225 = fsub double %2221, %2224
  %2226 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2204
  store double %2225, double* %2226, align 8
  %2227 = icmp eq i32 %781, %779
  %2228 = add i32 %781, 1
  %2229 = icmp ne i1 %2227, false
  br i1 %2229, label %"12", label %"11"

"11":                                             ; preds = %"10"
  br label %"10"

"12":                                             ; preds = %"10", %"9"
  %2230 = sext i32 %8 to i64
  %2231 = mul i64 %2230, 21125
  %2232 = sext i32 %12 to i64
  %2233 = mul i64 %2232, 5
  %2234 = add i64 %2231, %2233
  %2235 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2234
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 25), double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 50), double* %2235) #1
  %2236 = load i32* %jsize, align 4
  %2237 = add i32 %2236, -1
  %2238 = icmp sle i32 1, %2237
  br i1 %2238, label %"13", label %"15"

"13":                                             ; preds = %"14", %"12"
  %2239 = phi i32 [ %2292, %"14" ], [ 1, %"12" ]
  %2240 = sext i32 %8 to i64
  %2241 = mul i64 %2240, 21125
  %2242 = sext i32 %2239 to i64
  %2243 = mul i64 %2242, 325
  %2244 = add i64 %2241, %2243
  %2245 = sext i32 %12 to i64
  %2246 = mul i64 %2245, 5
  %2247 = add i64 %2244, %2246
  %2248 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2247
  %2249 = sext i32 %8 to i64
  %2250 = mul i64 %2249, 21125
  %2251 = add i32 %2239, -1
  %2252 = sext i32 %2251 to i64
  %2253 = mul i64 %2252, 325
  %2254 = add i64 %2250, %2253
  %2255 = sext i32 %12 to i64
  %2256 = mul i64 %2255, 5
  %2257 = add i64 %2254, %2256
  %2258 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2257
  %2259 = sext i32 %2239 to i64
  %2260 = mul i64 %2259, 75
  %2261 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2260
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2261, double* %2258, double* %2248) #1
  %2262 = sext i32 %2239 to i64
  %2263 = mul i64 %2262, 75
  %2264 = add i64 %2263, 25
  %2265 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2264
  %2266 = add i32 %2239, -1
  %2267 = sext i32 %2266 to i64
  %2268 = mul i64 %2267, 75
  %2269 = add i64 %2268, 50
  %2270 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2269
  %2271 = sext i32 %2239 to i64
  %2272 = mul i64 %2271, 75
  %2273 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2272
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2273, double* %2270, double* %2265) #1
  %2274 = sext i32 %8 to i64
  %2275 = mul i64 %2274, 21125
  %2276 = sext i32 %2239 to i64
  %2277 = mul i64 %2276, 325
  %2278 = add i64 %2275, %2277
  %2279 = sext i32 %12 to i64
  %2280 = mul i64 %2279, 5
  %2281 = add i64 %2278, %2280
  %2282 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2281
  %2283 = sext i32 %2239 to i64
  %2284 = mul i64 %2283, 75
  %2285 = add i64 %2284, 50
  %2286 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2285
  %2287 = sext i32 %2239 to i64
  %2288 = mul i64 %2287, 75
  %2289 = add i64 %2288, 25
  %2290 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2289
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* %2290, double* %2286, double* %2282) #1
  %2291 = icmp eq i32 %2239, %2237
  %2292 = add i32 %2239, 1
  %2293 = icmp ne i1 %2291, false
  br i1 %2293, label %"15", label %"14"

"14":                                             ; preds = %"13"
  br label %"13"

"15":                                             ; preds = %"13", %"12"
  %2294 = sext i32 %8 to i64
  %2295 = mul i64 %2294, 21125
  %2296 = load i32* %jsize, align 4
  %2297 = sext i32 %2296 to i64
  %2298 = mul i64 %2297, 325
  %2299 = add i64 %2295, %2298
  %2300 = sext i32 %12 to i64
  %2301 = mul i64 %2300, 5
  %2302 = add i64 %2299, %2301
  %2303 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2302
  %2304 = sext i32 %8 to i64
  %2305 = mul i64 %2304, 21125
  %2306 = load i32* %jsize, align 4
  %2307 = add i32 %2306, -1
  %2308 = sext i32 %2307 to i64
  %2309 = mul i64 %2308, 325
  %2310 = add i64 %2305, %2309
  %2311 = sext i32 %12 to i64
  %2312 = mul i64 %2311, 5
  %2313 = add i64 %2310, %2312
  %2314 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2313
  %2315 = load i32* %jsize, align 4
  %2316 = sext i32 %2315 to i64
  %2317 = mul i64 %2316, 75
  %2318 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2317
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2318, double* %2314, double* %2303) #1
  %2319 = load i32* %jsize, align 4
  %2320 = sext i32 %2319 to i64
  %2321 = mul i64 %2320, 75
  %2322 = add i64 %2321, 25
  %2323 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2322
  %2324 = load i32* %jsize, align 4
  %2325 = add i32 %2324, -1
  %2326 = sext i32 %2325 to i64
  %2327 = mul i64 %2326, 75
  %2328 = add i64 %2327, 50
  %2329 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2328
  %2330 = load i32* %jsize, align 4
  %2331 = sext i32 %2330 to i64
  %2332 = mul i64 %2331, 75
  %2333 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2332
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2333, double* %2329, double* %2323) #1
  %2334 = sext i32 %8 to i64
  %2335 = mul i64 %2334, 21125
  %2336 = load i32* %jsize, align 4
  %2337 = sext i32 %2336 to i64
  %2338 = mul i64 %2337, 325
  %2339 = add i64 %2335, %2338
  %2340 = sext i32 %12 to i64
  %2341 = mul i64 %2340, 5
  %2342 = add i64 %2339, %2341
  %2343 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2342
  %2344 = load i32* %jsize, align 4
  %2345 = sext i32 %2344 to i64
  %2346 = mul i64 %2345, 75
  %2347 = add i64 %2346, 25
  %2348 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2347
  call void bitcast (void (...)* @binvrhs_ to void (double*, double*)*)(double* %2348, double* %2343) #1
  %2349 = load i32* %jsize, align 4
  %2350 = add i32 %2349, -1
  %2351 = icmp sge i32 %2350, 0
  br i1 %2351, label %"16", label %"24"

"16":                                             ; preds = %"23", %"15"
  %2352 = phi i32 [ %2413, %"23" ], [ %2350, %"15" ]
  br i1 true, label %"17", label %"22"

"17":                                             ; preds = %"21", %"16"
  %2353 = phi i32 [ %2410, %"21" ], [ 1, %"16" ]
  br i1 true, label %"18", label %"20"

"18":                                             ; preds = %"19", %"17"
  %2354 = phi i32 [ %2407, %"19" ], [ 1, %"17" ]
  %2355 = sext i32 %8 to i64
  %2356 = mul i64 %2355, 21125
  %2357 = sext i32 %2352 to i64
  %2358 = mul i64 %2357, 325
  %2359 = add i64 %2356, %2358
  %2360 = sext i32 %12 to i64
  %2361 = mul i64 %2360, 5
  %2362 = add i64 %2359, %2361
  %2363 = sext i32 %2353 to i64
  %2364 = add i64 %2362, %2363
  %2365 = add i64 %2364, -1
  %2366 = sext i32 %8 to i64
  %2367 = mul i64 %2366, 21125
  %2368 = sext i32 %2352 to i64
  %2369 = mul i64 %2368, 325
  %2370 = add i64 %2367, %2369
  %2371 = sext i32 %12 to i64
  %2372 = mul i64 %2371, 5
  %2373 = add i64 %2370, %2372
  %2374 = sext i32 %2353 to i64
  %2375 = add i64 %2373, %2374
  %2376 = add i64 %2375, -1
  %2377 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2376
  %2378 = load double* %2377, align 8
  %2379 = sext i32 %2352 to i64
  %2380 = mul i64 %2379, 75
  %2381 = sext i32 %2354 to i64
  %2382 = mul i64 %2381, 5
  %2383 = add i64 %2380, %2382
  %2384 = sext i32 %2353 to i64
  %2385 = add i64 %2383, %2384
  %2386 = add i64 %2385, 44
  %2387 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2386
  %2388 = load double* %2387, align 8
  %2389 = sext i32 %8 to i64
  %2390 = mul i64 %2389, 21125
  %2391 = add i32 %2352, 1
  %2392 = sext i32 %2391 to i64
  %2393 = mul i64 %2392, 325
  %2394 = add i64 %2390, %2393
  %2395 = sext i32 %12 to i64
  %2396 = mul i64 %2395, 5
  %2397 = add i64 %2394, %2396
  %2398 = sext i32 %2354 to i64
  %2399 = add i64 %2397, %2398
  %2400 = add i64 %2399, -1
  %2401 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2400
  %2402 = load double* %2401, align 8
  %2403 = fmul double %2388, %2402
  %2404 = fsub double %2378, %2403
  %2405 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2365
  store double %2404, double* %2405, align 8
  %2406 = icmp eq i32 %2354, 5
  %2407 = add i32 %2354, 1
  %2408 = icmp ne i1 %2406, false
  br i1 %2408, label %"20", label %"19"

"19":                                             ; preds = %"18"
  br label %"18"

"20":                                             ; preds = %"18", %"17"
  %2409 = icmp eq i32 %2353, 5
  %2410 = add i32 %2353, 1
  %2411 = icmp ne i1 %2409, false
  br i1 %2411, label %"22", label %"21"

"21":                                             ; preds = %"20"
  br label %"17"

"22":                                             ; preds = %"20", %"16"
  %2412 = icmp eq i32 %2352, 0
  %2413 = add i32 %2352, -1
  %2414 = icmp ne i1 %2412, false
  br i1 %2414, label %"24", label %"23"

"23":                                             ; preds = %"22"
  br label %"16"

"24":                                             ; preds = %"22", %"15"
  %2415 = icmp eq i32 %12, %10
  %2416 = add i32 %12, 1
  %2417 = icmp ne i1 %2415, false
  br i1 %2417, label %"26", label %"25"

"25":                                             ; preds = %"24"
  br label %"6"

"26":                                             ; preds = %"24", %"5"
  %2418 = icmp eq i32 %8, %6
  %2419 = add i32 %8, 1
  %2420 = icmp ne i1 %2418, false
  br i1 %2420, label %"28", label %"27"

"27":                                             ; preds = %"26"
  br label %"5"

"28":                                             ; preds = %"26", %"4"
  %2421 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %2422 = trunc i32 %2421 to i1
  %2423 = icmp ne i1 %2422, false
  br i1 %2423, label %"29", label %"30"

"29":                                             ; preds = %"28"
  call void bitcast (void (...)* @timer_stop_ to void (i32*)*)(i32* @1) #1
  br label %"30"

"30":                                             ; preds = %"29", %"28"
  br label %"31"

"31":                                             ; preds = %"30"
  %2424 = bitcast i32* %jsize to i8*
  call void @llvm.lifetime.end(i64 4, i8* %2424)
  br label %"32"

"32":                                             ; preds = %"31"
  br label %return

return:                                           ; preds = %"32"
  ret void
}

declare void @timer_start_(...)

declare void @lhsinit_(...)

declare void @binvcrhs_(...)

declare void @matvec_sub_(...)

declare void @matmul_sub_(...)

declare void @binvrhs_(...)

declare void @timer_stop_(...)

; Function Attrs: nounwind
declare void @llvm.lifetime.end(i64, i8* nocapture) #1

attributes #0 = { nounwind uwtable "no-frame-pointer-elim-non-leaf"="true" }
attributes #1 = { nounwind }

!0 = metadata !{i32 0, i32 2}
