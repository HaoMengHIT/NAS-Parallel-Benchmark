; ModuleID = 'z_solve.f'
target datalayout = "e-p:64:64:64-S128-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f16:16:16-f32:32:32-f64:64:64-f128:128:128-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64"
target triple = "x86_64-unknown-linux-gnu"

module asm "\09.ident\09\22GCC: (GNU) 4.8.2 20140206 (prerelease) LLVM: 3.4\22"

%0 = type { double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, [65 x double], double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double }
%1 = type { [1352000 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [1352000 x double], [1352000 x double] }
%2 = type { double, [3 x i32], i32 }
%3 = type { [65 x double], [65 x double], [325 x double], [325 x double] }
%4 = type { [1625 x double], [1625 x double], [4875 x double], double, double, double }

@constants_ = common unnamed_addr global %0 zeroinitializer, align 32
@fields_ = common global %1 zeroinitializer, align 32
@global_ = common unnamed_addr global %2 zeroinitializer, align 16
@work_1d_ = common unnamed_addr global %3 zeroinitializer, align 32
@work_lhs_ = common global %4 zeroinitializer, align 32
@0 = internal constant i32 8
@1 = internal constant i32 8

; Function Attrs: nounwind uwtable
define void @z_solve_() unnamed_addr #0 {
entry:
  %i = alloca i32
  %j = alloca i32
  %k = alloca i32
  %ksize = alloca i32
  %m = alloca i32
  %n = alloca i32
  %D.2142 = alloca i32
  %D.2182 = alloca i32
  %D.2145 = alloca i32
  %D.2181 = alloca i32
  %D.2148 = alloca i32
  %D.2157 = alloca i32
  %D.2156 = alloca double
  %D.2155 = alloca double
  %D.2154 = alloca double
  %D.2153 = alloca double
  %D.2152 = alloca double
  %D.2151 = alloca double
  %D.2159 = alloca i32
  %D.2162 = alloca i32
  %D.2164 = alloca i32
  %D.2169 = alloca i32
  %k.12 = alloca i32
  %D.2180 = alloca i32
  %D.2179 = alloca i32
  %D.2178 = alloca i32
  %"alloca point" = bitcast i32 0 to i32
  %"ssa point" = bitcast i32 0 to i32
  br label %"2"

"2":                                              ; preds = %entry
  %0 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %1 = trunc i32 %0 to i1
  %2 = icmp ne i1 %1, false
  br i1 %2, label %"3", label %"4"

"3":                                              ; preds = %"2"
  call void bitcast (void (...)* @timer_start_ to void (i32*)*)(i32* @0) #1
  br label %"4"

"4":                                              ; preds = %"3", %"2"
  %3 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 2), align 4
  %4 = add i32 %3, -1
  store i32 %4, i32* %ksize, align 4
  %5 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 1), align 4
  %6 = add i32 %5, -2
  %7 = icmp sle i32 1, %6
  br i1 %7, label %"5", label %"28"

"5":                                              ; preds = %"27", %"4"
  %8 = phi i32 [ %2423, %"27" ], [ 1, %"4" ]
  %9 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 0), align 4
  %10 = add i32 %9, -2
  %11 = icmp sle i32 1, %10
  br i1 %11, label %"6", label %"26"

"6":                                              ; preds = %"25", %"5"
  %12 = phi i32 [ %2420, %"25" ], [ 1, %"5" ]
  %13 = load i32* %ksize, align 4
  %14 = icmp sle i32 0, %13
  br i1 %14, label %"7", label %"9"

"7":                                              ; preds = %"8", %"6"
  %15 = phi i32 [ %780, %"8" ], [ 0, %"6" ]
  %16 = sext i32 %15 to i64
  %17 = mul i64 %16, 21125
  %18 = sext i32 %8 to i64
  %19 = mul i64 %18, 325
  %20 = add i64 %17, %19
  %21 = sext i32 %12 to i64
  %22 = mul i64 %21, 5
  %23 = add i64 %20, %22
  %24 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %23
  %25 = load double* %24, align 8
  %26 = fdiv double 1.000000e+00, %25
  store double %26, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %27 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %28 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %29 = fmul double %27, %28
  store double %29, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %30 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %31 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %32 = fmul double %30, %31
  store double %32, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %33 = sext i32 %15 to i64
  %34 = mul i64 %33, 25
  %35 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %34
  store double 0.000000e+00, double* %35, align 8
  %36 = sext i32 %15 to i64
  %37 = mul i64 %36, 25
  %38 = add i64 %37, 5
  %39 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %38
  store double 0.000000e+00, double* %39, align 8
  %40 = sext i32 %15 to i64
  %41 = mul i64 %40, 25
  %42 = add i64 %41, 10
  %43 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %42
  store double 0.000000e+00, double* %43, align 8
  %44 = sext i32 %15 to i64
  %45 = mul i64 %44, 25
  %46 = add i64 %45, 15
  %47 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %46
  store double 1.000000e+00, double* %47, align 8
  %48 = sext i32 %15 to i64
  %49 = mul i64 %48, 25
  %50 = add i64 %49, 20
  %51 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %50
  store double 0.000000e+00, double* %51, align 8
  %52 = sext i32 %15 to i64
  %53 = mul i64 %52, 25
  %54 = add i64 %53, 1
  %55 = sext i32 %15 to i64
  %56 = mul i64 %55, 21125
  %57 = sext i32 %8 to i64
  %58 = mul i64 %57, 325
  %59 = add i64 %56, %58
  %60 = sext i32 %12 to i64
  %61 = mul i64 %60, 5
  %62 = add i64 %59, %61
  %63 = add i64 %62, 1
  %64 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %63
  %65 = load double* %64, align 8
  %66 = sext i32 %15 to i64
  %67 = mul i64 %66, 21125
  %68 = sext i32 %8 to i64
  %69 = mul i64 %68, 325
  %70 = add i64 %67, %69
  %71 = sext i32 %12 to i64
  %72 = mul i64 %71, 5
  %73 = add i64 %70, %72
  %74 = add i64 %73, 3
  %75 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %74
  %76 = load double* %75, align 8
  %77 = fmul double %65, %76
  %78 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %79 = fmul double %77, %78
  %80 = fsub double -0.000000e+00, %79
  %81 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %54
  store double %80, double* %81, align 8
  %82 = sext i32 %15 to i64
  %83 = mul i64 %82, 25
  %84 = add i64 %83, 6
  %85 = sext i32 %15 to i64
  %86 = mul i64 %85, 21125
  %87 = sext i32 %8 to i64
  %88 = mul i64 %87, 325
  %89 = add i64 %86, %88
  %90 = sext i32 %12 to i64
  %91 = mul i64 %90, 5
  %92 = add i64 %89, %91
  %93 = add i64 %92, 3
  %94 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %93
  %95 = load double* %94, align 8
  %96 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %97 = fmul double %95, %96
  %98 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %84
  store double %97, double* %98, align 8
  %99 = sext i32 %15 to i64
  %100 = mul i64 %99, 25
  %101 = add i64 %100, 11
  %102 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %101
  store double 0.000000e+00, double* %102, align 8
  %103 = sext i32 %15 to i64
  %104 = mul i64 %103, 25
  %105 = add i64 %104, 16
  %106 = sext i32 %15 to i64
  %107 = mul i64 %106, 21125
  %108 = sext i32 %8 to i64
  %109 = mul i64 %108, 325
  %110 = add i64 %107, %109
  %111 = sext i32 %12 to i64
  %112 = mul i64 %111, 5
  %113 = add i64 %110, %112
  %114 = add i64 %113, 1
  %115 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %114
  %116 = load double* %115, align 8
  %117 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %118 = fmul double %116, %117
  %119 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %105
  store double %118, double* %119, align 8
  %120 = sext i32 %15 to i64
  %121 = mul i64 %120, 25
  %122 = add i64 %121, 21
  %123 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %122
  store double 0.000000e+00, double* %123, align 8
  %124 = sext i32 %15 to i64
  %125 = mul i64 %124, 25
  %126 = add i64 %125, 2
  %127 = sext i32 %15 to i64
  %128 = mul i64 %127, 21125
  %129 = sext i32 %8 to i64
  %130 = mul i64 %129, 325
  %131 = add i64 %128, %130
  %132 = sext i32 %12 to i64
  %133 = mul i64 %132, 5
  %134 = add i64 %131, %133
  %135 = add i64 %134, 2
  %136 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %135
  %137 = load double* %136, align 8
  %138 = sext i32 %15 to i64
  %139 = mul i64 %138, 21125
  %140 = sext i32 %8 to i64
  %141 = mul i64 %140, 325
  %142 = add i64 %139, %141
  %143 = sext i32 %12 to i64
  %144 = mul i64 %143, 5
  %145 = add i64 %142, %144
  %146 = add i64 %145, 3
  %147 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %146
  %148 = load double* %147, align 8
  %149 = fmul double %137, %148
  %150 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %151 = fmul double %149, %150
  %152 = fsub double -0.000000e+00, %151
  %153 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %126
  store double %152, double* %153, align 8
  %154 = sext i32 %15 to i64
  %155 = mul i64 %154, 25
  %156 = add i64 %155, 7
  %157 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %156
  store double 0.000000e+00, double* %157, align 8
  %158 = sext i32 %15 to i64
  %159 = mul i64 %158, 25
  %160 = add i64 %159, 12
  %161 = sext i32 %15 to i64
  %162 = mul i64 %161, 21125
  %163 = sext i32 %8 to i64
  %164 = mul i64 %163, 325
  %165 = add i64 %162, %164
  %166 = sext i32 %12 to i64
  %167 = mul i64 %166, 5
  %168 = add i64 %165, %167
  %169 = add i64 %168, 3
  %170 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %169
  %171 = load double* %170, align 8
  %172 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %173 = fmul double %171, %172
  %174 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %160
  store double %173, double* %174, align 8
  %175 = sext i32 %15 to i64
  %176 = mul i64 %175, 25
  %177 = add i64 %176, 17
  %178 = sext i32 %15 to i64
  %179 = mul i64 %178, 21125
  %180 = sext i32 %8 to i64
  %181 = mul i64 %180, 325
  %182 = add i64 %179, %181
  %183 = sext i32 %12 to i64
  %184 = mul i64 %183, 5
  %185 = add i64 %182, %184
  %186 = add i64 %185, 2
  %187 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %186
  %188 = load double* %187, align 8
  %189 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %190 = fmul double %188, %189
  %191 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %177
  store double %190, double* %191, align 8
  %192 = sext i32 %15 to i64
  %193 = mul i64 %192, 25
  %194 = add i64 %193, 22
  %195 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %194
  store double 0.000000e+00, double* %195, align 8
  %196 = sext i32 %15 to i64
  %197 = mul i64 %196, 25
  %198 = add i64 %197, 3
  %199 = sext i32 %15 to i64
  %200 = mul i64 %199, 4225
  %201 = sext i32 %8 to i64
  %202 = mul i64 %201, 65
  %203 = add i64 %200, %202
  %204 = sext i32 %12 to i64
  %205 = add i64 %203, %204
  %206 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %205
  %207 = load double* %206, align 8
  %208 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %209 = fmul double %207, %208
  %210 = sext i32 %15 to i64
  %211 = mul i64 %210, 21125
  %212 = sext i32 %8 to i64
  %213 = mul i64 %212, 325
  %214 = add i64 %211, %213
  %215 = sext i32 %12 to i64
  %216 = mul i64 %215, 5
  %217 = add i64 %214, %216
  %218 = add i64 %217, 3
  %219 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %218
  %220 = load double* %219, align 8
  %221 = sext i32 %15 to i64
  %222 = mul i64 %221, 21125
  %223 = sext i32 %8 to i64
  %224 = mul i64 %223, 325
  %225 = add i64 %222, %224
  %226 = sext i32 %12 to i64
  %227 = mul i64 %226, 5
  %228 = add i64 %225, %227
  %229 = add i64 %228, 3
  %230 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %229
  %231 = load double* %230, align 8
  %232 = fmul double %220, %231
  %233 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %234 = fmul double %232, %233
  %235 = fsub double %209, %234
  %236 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %198
  store double %235, double* %236, align 8
  %237 = sext i32 %15 to i64
  %238 = mul i64 %237, 25
  %239 = add i64 %238, 8
  %240 = sext i32 %15 to i64
  %241 = mul i64 %240, 21125
  %242 = sext i32 %8 to i64
  %243 = mul i64 %242, 325
  %244 = add i64 %241, %243
  %245 = sext i32 %12 to i64
  %246 = mul i64 %245, 5
  %247 = add i64 %244, %246
  %248 = add i64 %247, 1
  %249 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %248
  %250 = load double* %249, align 8
  %251 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %252 = fmul double %250, %251
  %253 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %254 = fmul double %252, %253
  %255 = fsub double -0.000000e+00, %254
  %256 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %239
  store double %255, double* %256, align 8
  %257 = sext i32 %15 to i64
  %258 = mul i64 %257, 25
  %259 = add i64 %258, 13
  %260 = sext i32 %15 to i64
  %261 = mul i64 %260, 21125
  %262 = sext i32 %8 to i64
  %263 = mul i64 %262, 325
  %264 = add i64 %261, %263
  %265 = sext i32 %12 to i64
  %266 = mul i64 %265, 5
  %267 = add i64 %264, %266
  %268 = add i64 %267, 2
  %269 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %268
  %270 = load double* %269, align 8
  %271 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %272 = fmul double %270, %271
  %273 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %274 = fmul double %272, %273
  %275 = fsub double -0.000000e+00, %274
  %276 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %259
  store double %275, double* %276, align 8
  %277 = sext i32 %15 to i64
  %278 = mul i64 %277, 25
  %279 = add i64 %278, 18
  %280 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %281 = fsub double 2.000000e+00, %280
  %282 = sext i32 %15 to i64
  %283 = mul i64 %282, 21125
  %284 = sext i32 %8 to i64
  %285 = mul i64 %284, 325
  %286 = add i64 %283, %285
  %287 = sext i32 %12 to i64
  %288 = mul i64 %287, 5
  %289 = add i64 %286, %288
  %290 = add i64 %289, 3
  %291 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %290
  %292 = load double* %291, align 8
  %293 = fmul double %281, %292
  %294 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %295 = fmul double %293, %294
  %296 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %279
  store double %295, double* %296, align 8
  %297 = sext i32 %15 to i64
  %298 = mul i64 %297, 25
  %299 = add i64 %298, 23
  %300 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %301 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %299
  store double %300, double* %301, align 8
  %302 = sext i32 %15 to i64
  %303 = mul i64 %302, 25
  %304 = add i64 %303, 4
  %305 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %306 = fmul double %305, 2.000000e+00
  %307 = sext i32 %15 to i64
  %308 = mul i64 %307, 4225
  %309 = sext i32 %8 to i64
  %310 = mul i64 %309, 65
  %311 = add i64 %308, %310
  %312 = sext i32 %12 to i64
  %313 = add i64 %311, %312
  %314 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 6, i64 0), i64 %313
  %315 = load double* %314, align 8
  %316 = fmul double %306, %315
  %317 = sext i32 %15 to i64
  %318 = mul i64 %317, 21125
  %319 = sext i32 %8 to i64
  %320 = mul i64 %319, 325
  %321 = add i64 %318, %320
  %322 = sext i32 %12 to i64
  %323 = mul i64 %322, 5
  %324 = add i64 %321, %323
  %325 = add i64 %324, 4
  %326 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %325
  %327 = load double* %326, align 8
  %328 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %329 = fmul double %327, %328
  %330 = fsub double %316, %329
  %331 = sext i32 %15 to i64
  %332 = mul i64 %331, 21125
  %333 = sext i32 %8 to i64
  %334 = mul i64 %333, 325
  %335 = add i64 %332, %334
  %336 = sext i32 %12 to i64
  %337 = mul i64 %336, 5
  %338 = add i64 %335, %337
  %339 = add i64 %338, 3
  %340 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %339
  %341 = load double* %340, align 8
  %342 = fmul double %330, %341
  %343 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %344 = fmul double %342, %343
  %345 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %304
  store double %344, double* %345, align 8
  %346 = sext i32 %15 to i64
  %347 = mul i64 %346, 25
  %348 = add i64 %347, 9
  %349 = sext i32 %15 to i64
  %350 = mul i64 %349, 21125
  %351 = sext i32 %8 to i64
  %352 = mul i64 %351, 325
  %353 = add i64 %350, %352
  %354 = sext i32 %12 to i64
  %355 = mul i64 %354, 5
  %356 = add i64 %353, %355
  %357 = add i64 %356, 1
  %358 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %357
  %359 = load double* %358, align 8
  %360 = sext i32 %15 to i64
  %361 = mul i64 %360, 21125
  %362 = sext i32 %8 to i64
  %363 = mul i64 %362, 325
  %364 = add i64 %361, %363
  %365 = sext i32 %12 to i64
  %366 = mul i64 %365, 5
  %367 = add i64 %364, %366
  %368 = add i64 %367, 3
  %369 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %368
  %370 = load double* %369, align 8
  %371 = fmul double %359, %370
  %372 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %373 = fmul double %371, %372
  %374 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %375 = fmul double %373, %374
  %376 = fsub double -0.000000e+00, %375
  %377 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %348
  store double %376, double* %377, align 8
  %378 = sext i32 %15 to i64
  %379 = mul i64 %378, 25
  %380 = add i64 %379, 14
  %381 = sext i32 %15 to i64
  %382 = mul i64 %381, 21125
  %383 = sext i32 %8 to i64
  %384 = mul i64 %383, 325
  %385 = add i64 %382, %384
  %386 = sext i32 %12 to i64
  %387 = mul i64 %386, 5
  %388 = add i64 %385, %387
  %389 = add i64 %388, 2
  %390 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %389
  %391 = load double* %390, align 8
  %392 = sext i32 %15 to i64
  %393 = mul i64 %392, 21125
  %394 = sext i32 %8 to i64
  %395 = mul i64 %394, 325
  %396 = add i64 %393, %395
  %397 = sext i32 %12 to i64
  %398 = mul i64 %397, 5
  %399 = add i64 %396, %398
  %400 = add i64 %399, 3
  %401 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %400
  %402 = load double* %401, align 8
  %403 = fmul double %391, %402
  %404 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %405 = fmul double %403, %404
  %406 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %407 = fmul double %405, %406
  %408 = fsub double -0.000000e+00, %407
  %409 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %380
  store double %408, double* %409, align 8
  %410 = sext i32 %15 to i64
  %411 = mul i64 %410, 25
  %412 = add i64 %411, 19
  %413 = sext i32 %15 to i64
  %414 = mul i64 %413, 21125
  %415 = sext i32 %8 to i64
  %416 = mul i64 %415, 325
  %417 = add i64 %414, %416
  %418 = sext i32 %12 to i64
  %419 = mul i64 %418, 5
  %420 = add i64 %417, %419
  %421 = add i64 %420, 4
  %422 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %421
  %423 = load double* %422, align 8
  %424 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %425 = fmul double %423, %424
  %426 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %427 = fmul double %425, %426
  %428 = sext i32 %15 to i64
  %429 = mul i64 %428, 4225
  %430 = sext i32 %8 to i64
  %431 = mul i64 %430, 65
  %432 = add i64 %429, %431
  %433 = sext i32 %12 to i64
  %434 = add i64 %432, %433
  %435 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %434
  %436 = load double* %435, align 8
  %437 = sext i32 %15 to i64
  %438 = mul i64 %437, 21125
  %439 = sext i32 %8 to i64
  %440 = mul i64 %439, 325
  %441 = add i64 %438, %440
  %442 = sext i32 %12 to i64
  %443 = mul i64 %442, 5
  %444 = add i64 %441, %443
  %445 = add i64 %444, 3
  %446 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %445
  %447 = load double* %446, align 8
  %448 = sext i32 %15 to i64
  %449 = mul i64 %448, 21125
  %450 = sext i32 %8 to i64
  %451 = mul i64 %450, 325
  %452 = add i64 %449, %451
  %453 = sext i32 %12 to i64
  %454 = mul i64 %453, 5
  %455 = add i64 %452, %454
  %456 = add i64 %455, 3
  %457 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %456
  %458 = load double* %457, align 8
  %459 = fmul double %447, %458
  %460 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %461 = fmul double %459, %460
  %462 = fadd double %436, %461
  %463 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %464 = fmul double %462, %463
  %465 = fsub double %427, %464
  %466 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %412
  store double %465, double* %466, align 8
  %467 = sext i32 %15 to i64
  %468 = mul i64 %467, 25
  %469 = add i64 %468, 24
  %470 = sext i32 %15 to i64
  %471 = mul i64 %470, 21125
  %472 = sext i32 %8 to i64
  %473 = mul i64 %472, 325
  %474 = add i64 %471, %473
  %475 = sext i32 %12 to i64
  %476 = mul i64 %475, 5
  %477 = add i64 %474, %476
  %478 = add i64 %477, 3
  %479 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %478
  %480 = load double* %479, align 8
  %481 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %482 = fmul double %480, %481
  %483 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %484 = fmul double %482, %483
  %485 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %469
  store double %484, double* %485, align 8
  %486 = sext i32 %15 to i64
  %487 = mul i64 %486, 25
  %488 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %487
  store double 0.000000e+00, double* %488, align 8
  %489 = sext i32 %15 to i64
  %490 = mul i64 %489, 25
  %491 = add i64 %490, 5
  %492 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %491
  store double 0.000000e+00, double* %492, align 8
  %493 = sext i32 %15 to i64
  %494 = mul i64 %493, 25
  %495 = add i64 %494, 10
  %496 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %495
  store double 0.000000e+00, double* %496, align 8
  %497 = sext i32 %15 to i64
  %498 = mul i64 %497, 25
  %499 = add i64 %498, 15
  %500 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %499
  store double 0.000000e+00, double* %500, align 8
  %501 = sext i32 %15 to i64
  %502 = mul i64 %501, 25
  %503 = add i64 %502, 20
  %504 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %503
  store double 0.000000e+00, double* %504, align 8
  %505 = sext i32 %15 to i64
  %506 = mul i64 %505, 25
  %507 = add i64 %506, 1
  %508 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %509 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %510 = fmul double %508, %509
  %511 = sext i32 %15 to i64
  %512 = mul i64 %511, 21125
  %513 = sext i32 %8 to i64
  %514 = mul i64 %513, 325
  %515 = add i64 %512, %514
  %516 = sext i32 %12 to i64
  %517 = mul i64 %516, 5
  %518 = add i64 %515, %517
  %519 = add i64 %518, 1
  %520 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %519
  %521 = load double* %520, align 8
  %522 = fmul double %510, %521
  %523 = fsub double -0.000000e+00, %522
  %524 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %507
  store double %523, double* %524, align 8
  %525 = sext i32 %15 to i64
  %526 = mul i64 %525, 25
  %527 = add i64 %526, 6
  %528 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %529 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %530 = fmul double %528, %529
  %531 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %527
  store double %530, double* %531, align 8
  %532 = sext i32 %15 to i64
  %533 = mul i64 %532, 25
  %534 = add i64 %533, 11
  %535 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %534
  store double 0.000000e+00, double* %535, align 8
  %536 = sext i32 %15 to i64
  %537 = mul i64 %536, 25
  %538 = add i64 %537, 16
  %539 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %538
  store double 0.000000e+00, double* %539, align 8
  %540 = sext i32 %15 to i64
  %541 = mul i64 %540, 25
  %542 = add i64 %541, 21
  %543 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %542
  store double 0.000000e+00, double* %543, align 8
  %544 = sext i32 %15 to i64
  %545 = mul i64 %544, 25
  %546 = add i64 %545, 2
  %547 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %548 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %549 = fmul double %547, %548
  %550 = sext i32 %15 to i64
  %551 = mul i64 %550, 21125
  %552 = sext i32 %8 to i64
  %553 = mul i64 %552, 325
  %554 = add i64 %551, %553
  %555 = sext i32 %12 to i64
  %556 = mul i64 %555, 5
  %557 = add i64 %554, %556
  %558 = add i64 %557, 2
  %559 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %558
  %560 = load double* %559, align 8
  %561 = fmul double %549, %560
  %562 = fsub double -0.000000e+00, %561
  %563 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %546
  store double %562, double* %563, align 8
  %564 = sext i32 %15 to i64
  %565 = mul i64 %564, 25
  %566 = add i64 %565, 7
  %567 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %566
  store double 0.000000e+00, double* %567, align 8
  %568 = sext i32 %15 to i64
  %569 = mul i64 %568, 25
  %570 = add i64 %569, 12
  %571 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %572 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %573 = fmul double %571, %572
  %574 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %570
  store double %573, double* %574, align 8
  %575 = sext i32 %15 to i64
  %576 = mul i64 %575, 25
  %577 = add i64 %576, 17
  %578 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %577
  store double 0.000000e+00, double* %578, align 8
  %579 = sext i32 %15 to i64
  %580 = mul i64 %579, 25
  %581 = add i64 %580, 22
  %582 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %581
  store double 0.000000e+00, double* %582, align 8
  %583 = sext i32 %15 to i64
  %584 = mul i64 %583, 25
  %585 = add i64 %584, 3
  %586 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %587 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %588 = fmul double %586, %587
  %589 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %590 = fmul double %588, %589
  %591 = sext i32 %15 to i64
  %592 = mul i64 %591, 21125
  %593 = sext i32 %8 to i64
  %594 = mul i64 %593, 325
  %595 = add i64 %592, %594
  %596 = sext i32 %12 to i64
  %597 = mul i64 %596, 5
  %598 = add i64 %595, %597
  %599 = add i64 %598, 3
  %600 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %599
  %601 = load double* %600, align 8
  %602 = fmul double %590, %601
  %603 = fsub double -0.000000e+00, %602
  %604 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %585
  store double %603, double* %604, align 8
  %605 = sext i32 %15 to i64
  %606 = mul i64 %605, 25
  %607 = add i64 %606, 8
  %608 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %607
  store double 0.000000e+00, double* %608, align 8
  %609 = sext i32 %15 to i64
  %610 = mul i64 %609, 25
  %611 = add i64 %610, 13
  %612 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %611
  store double 0.000000e+00, double* %612, align 8
  %613 = sext i32 %15 to i64
  %614 = mul i64 %613, 25
  %615 = add i64 %614, 18
  %616 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %617 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 70), align 16
  %618 = fmul double %616, %617
  %619 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 71), align 8
  %620 = fmul double %618, %619
  %621 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %622 = fmul double %620, %621
  %623 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %615
  store double %622, double* %623, align 8
  %624 = sext i32 %15 to i64
  %625 = mul i64 %624, 25
  %626 = add i64 %625, 23
  %627 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %626
  store double 0.000000e+00, double* %627, align 8
  %628 = sext i32 %15 to i64
  %629 = mul i64 %628, 21125
  %630 = sext i32 %8 to i64
  %631 = mul i64 %630, 325
  %632 = add i64 %629, %631
  %633 = sext i32 %12 to i64
  %634 = mul i64 %633, 5
  %635 = add i64 %632, %634
  %636 = add i64 %635, 1
  %637 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %636
  %638 = load double* %637, align 8
  %639 = fmul double %638, %638
  %640 = sext i32 %15 to i64
  %641 = mul i64 %640, 21125
  %642 = sext i32 %8 to i64
  %643 = mul i64 %642, 325
  %644 = add i64 %641, %643
  %645 = sext i32 %12 to i64
  %646 = mul i64 %645, 5
  %647 = add i64 %644, %646
  %648 = add i64 %647, 2
  %649 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %648
  %650 = load double* %649, align 8
  %651 = fmul double %650, %650
  %652 = sext i32 %15 to i64
  %653 = mul i64 %652, 21125
  %654 = sext i32 %8 to i64
  %655 = mul i64 %654, 325
  %656 = add i64 %653, %655
  %657 = sext i32 %12 to i64
  %658 = mul i64 %657, 5
  %659 = add i64 %656, %658
  %660 = add i64 %659, 3
  %661 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %660
  %662 = load double* %661, align 8
  %663 = fmul double %662, %662
  %664 = sext i32 %15 to i64
  %665 = mul i64 %664, 25
  %666 = add i64 %665, 4
  %667 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %668 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %669 = fsub double %667, %668
  %670 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %671 = fmul double %669, %670
  %672 = fmul double %671, %639
  %673 = fsub double -0.000000e+00, %672
  %674 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %675 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %676 = fsub double %674, %675
  %677 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %678 = fmul double %676, %677
  %679 = fmul double %678, %651
  %680 = fsub double %673, %679
  %681 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %682 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %683 = fmul double %681, %682
  %684 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %685 = fsub double %683, %684
  %686 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %687 = fmul double %685, %686
  %688 = fmul double %687, %663
  %689 = fsub double %680, %688
  %690 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %691 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %692 = fmul double %690, %691
  %693 = sext i32 %15 to i64
  %694 = mul i64 %693, 21125
  %695 = sext i32 %8 to i64
  %696 = mul i64 %695, 325
  %697 = add i64 %694, %696
  %698 = sext i32 %12 to i64
  %699 = mul i64 %698, 5
  %700 = add i64 %697, %699
  %701 = add i64 %700, 4
  %702 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %701
  %703 = load double* %702, align 8
  %704 = fmul double %692, %703
  %705 = fsub double %689, %704
  %706 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %666
  store double %705, double* %706, align 8
  %707 = sext i32 %15 to i64
  %708 = mul i64 %707, 25
  %709 = add i64 %708, 9
  %710 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %711 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %712 = fsub double %710, %711
  %713 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %714 = fmul double %712, %713
  %715 = sext i32 %15 to i64
  %716 = mul i64 %715, 21125
  %717 = sext i32 %8 to i64
  %718 = mul i64 %717, 325
  %719 = add i64 %716, %718
  %720 = sext i32 %12 to i64
  %721 = mul i64 %720, 5
  %722 = add i64 %719, %721
  %723 = add i64 %722, 1
  %724 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %723
  %725 = load double* %724, align 8
  %726 = fmul double %714, %725
  %727 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %709
  store double %726, double* %727, align 8
  %728 = sext i32 %15 to i64
  %729 = mul i64 %728, 25
  %730 = add i64 %729, 14
  %731 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %732 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %733 = fsub double %731, %732
  %734 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %735 = fmul double %733, %734
  %736 = sext i32 %15 to i64
  %737 = mul i64 %736, 21125
  %738 = sext i32 %8 to i64
  %739 = mul i64 %738, 325
  %740 = add i64 %737, %739
  %741 = sext i32 %12 to i64
  %742 = mul i64 %741, 5
  %743 = add i64 %740, %742
  %744 = add i64 %743, 2
  %745 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %744
  %746 = load double* %745, align 8
  %747 = fmul double %735, %746
  %748 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %730
  store double %747, double* %748, align 8
  %749 = sext i32 %15 to i64
  %750 = mul i64 %749, 25
  %751 = add i64 %750, 19
  %752 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %753 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %754 = fmul double %752, %753
  %755 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %756 = fsub double %754, %755
  %757 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %758 = fmul double %756, %757
  %759 = sext i32 %15 to i64
  %760 = mul i64 %759, 21125
  %761 = sext i32 %8 to i64
  %762 = mul i64 %761, 325
  %763 = add i64 %760, %762
  %764 = sext i32 %12 to i64
  %765 = mul i64 %764, 5
  %766 = add i64 %763, %765
  %767 = add i64 %766, 3
  %768 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %767
  %769 = load double* %768, align 8
  %770 = fmul double %758, %769
  %771 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %751
  store double %770, double* %771, align 8
  %772 = sext i32 %15 to i64
  %773 = mul i64 %772, 25
  %774 = add i64 %773, 24
  %775 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %776 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %777 = fmul double %775, %776
  %778 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %774
  store double %777, double* %778, align 8
  %779 = icmp eq i32 %15, %13
  %780 = add i32 %15, 1
  %781 = icmp ne i1 %779, false
  br i1 %781, label %"9", label %"8"

"8":                                              ; preds = %"7"
  br label %"7"

"9":                                              ; preds = %"7", %"6"
  call void bitcast (void (...)* @lhsinit_ to void ([4875 x double]*, i32*)*)([4875 x double]* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2), i32* %ksize) #1
  %782 = load i32* %ksize, align 4
  %783 = add i32 %782, -1
  %784 = icmp sle i32 1, %783
  br i1 %784, label %"10", label %"12"

"10":                                             ; preds = %"11", %"9"
  %785 = phi i32 [ %2232, %"11" ], [ 1, %"9" ]
  %786 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %787 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 6), align 16
  %788 = fmul double %786, %787
  store double %788, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %789 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %790 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 7), align 8
  %791 = fmul double %789, %790
  store double %791, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %792 = sext i32 %785 to i64
  %793 = mul i64 %792, 75
  %794 = add i32 %785, -1
  %795 = sext i32 %794 to i64
  %796 = mul i64 %795, 25
  %797 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %796
  %798 = load double* %797, align 8
  %799 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %800 = fmul double %798, %799
  %801 = fsub double -0.000000e+00, %800
  %802 = add i32 %785, -1
  %803 = sext i32 %802 to i64
  %804 = mul i64 %803, 25
  %805 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %804
  %806 = load double* %805, align 8
  %807 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %808 = fmul double %806, %807
  %809 = fsub double %801, %808
  %810 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %811 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 19), align 8
  %812 = fmul double %810, %811
  %813 = fsub double %809, %812
  %814 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %793
  store double %813, double* %814, align 8
  %815 = sext i32 %785 to i64
  %816 = mul i64 %815, 75
  %817 = add i64 %816, 5
  %818 = add i32 %785, -1
  %819 = sext i32 %818 to i64
  %820 = mul i64 %819, 25
  %821 = add i64 %820, 5
  %822 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %821
  %823 = load double* %822, align 8
  %824 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %825 = fmul double %823, %824
  %826 = fsub double -0.000000e+00, %825
  %827 = add i32 %785, -1
  %828 = sext i32 %827 to i64
  %829 = mul i64 %828, 25
  %830 = add i64 %829, 5
  %831 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %830
  %832 = load double* %831, align 8
  %833 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %834 = fmul double %832, %833
  %835 = fsub double %826, %834
  %836 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %817
  store double %835, double* %836, align 8
  %837 = sext i32 %785 to i64
  %838 = mul i64 %837, 75
  %839 = add i64 %838, 10
  %840 = add i32 %785, -1
  %841 = sext i32 %840 to i64
  %842 = mul i64 %841, 25
  %843 = add i64 %842, 10
  %844 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %843
  %845 = load double* %844, align 8
  %846 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %847 = fmul double %845, %846
  %848 = fsub double -0.000000e+00, %847
  %849 = add i32 %785, -1
  %850 = sext i32 %849 to i64
  %851 = mul i64 %850, 25
  %852 = add i64 %851, 10
  %853 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %852
  %854 = load double* %853, align 8
  %855 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %856 = fmul double %854, %855
  %857 = fsub double %848, %856
  %858 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %839
  store double %857, double* %858, align 8
  %859 = sext i32 %785 to i64
  %860 = mul i64 %859, 75
  %861 = add i64 %860, 15
  %862 = add i32 %785, -1
  %863 = sext i32 %862 to i64
  %864 = mul i64 %863, 25
  %865 = add i64 %864, 15
  %866 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %865
  %867 = load double* %866, align 8
  %868 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %869 = fmul double %867, %868
  %870 = fsub double -0.000000e+00, %869
  %871 = add i32 %785, -1
  %872 = sext i32 %871 to i64
  %873 = mul i64 %872, 25
  %874 = add i64 %873, 15
  %875 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %874
  %876 = load double* %875, align 8
  %877 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %878 = fmul double %876, %877
  %879 = fsub double %870, %878
  %880 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %861
  store double %879, double* %880, align 8
  %881 = sext i32 %785 to i64
  %882 = mul i64 %881, 75
  %883 = add i64 %882, 20
  %884 = add i32 %785, -1
  %885 = sext i32 %884 to i64
  %886 = mul i64 %885, 25
  %887 = add i64 %886, 20
  %888 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %887
  %889 = load double* %888, align 8
  %890 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %891 = fmul double %889, %890
  %892 = fsub double -0.000000e+00, %891
  %893 = add i32 %785, -1
  %894 = sext i32 %893 to i64
  %895 = mul i64 %894, 25
  %896 = add i64 %895, 20
  %897 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %896
  %898 = load double* %897, align 8
  %899 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %900 = fmul double %898, %899
  %901 = fsub double %892, %900
  %902 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %883
  store double %901, double* %902, align 8
  %903 = sext i32 %785 to i64
  %904 = mul i64 %903, 75
  %905 = add i64 %904, 1
  %906 = add i32 %785, -1
  %907 = sext i32 %906 to i64
  %908 = mul i64 %907, 25
  %909 = add i64 %908, 1
  %910 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %909
  %911 = load double* %910, align 8
  %912 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %913 = fmul double %911, %912
  %914 = fsub double -0.000000e+00, %913
  %915 = add i32 %785, -1
  %916 = sext i32 %915 to i64
  %917 = mul i64 %916, 25
  %918 = add i64 %917, 1
  %919 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %918
  %920 = load double* %919, align 8
  %921 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %922 = fmul double %920, %921
  %923 = fsub double %914, %922
  %924 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %905
  store double %923, double* %924, align 8
  %925 = sext i32 %785 to i64
  %926 = mul i64 %925, 75
  %927 = add i64 %926, 6
  %928 = add i32 %785, -1
  %929 = sext i32 %928 to i64
  %930 = mul i64 %929, 25
  %931 = add i64 %930, 6
  %932 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %931
  %933 = load double* %932, align 8
  %934 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %935 = fmul double %933, %934
  %936 = fsub double -0.000000e+00, %935
  %937 = add i32 %785, -1
  %938 = sext i32 %937 to i64
  %939 = mul i64 %938, 25
  %940 = add i64 %939, 6
  %941 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %940
  %942 = load double* %941, align 8
  %943 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %944 = fmul double %942, %943
  %945 = fsub double %936, %944
  %946 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %947 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 20), align 16
  %948 = fmul double %946, %947
  %949 = fsub double %945, %948
  %950 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %927
  store double %949, double* %950, align 8
  %951 = sext i32 %785 to i64
  %952 = mul i64 %951, 75
  %953 = add i64 %952, 11
  %954 = add i32 %785, -1
  %955 = sext i32 %954 to i64
  %956 = mul i64 %955, 25
  %957 = add i64 %956, 11
  %958 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %957
  %959 = load double* %958, align 8
  %960 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %961 = fmul double %959, %960
  %962 = fsub double -0.000000e+00, %961
  %963 = add i32 %785, -1
  %964 = sext i32 %963 to i64
  %965 = mul i64 %964, 25
  %966 = add i64 %965, 11
  %967 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %966
  %968 = load double* %967, align 8
  %969 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %970 = fmul double %968, %969
  %971 = fsub double %962, %970
  %972 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %953
  store double %971, double* %972, align 8
  %973 = sext i32 %785 to i64
  %974 = mul i64 %973, 75
  %975 = add i64 %974, 16
  %976 = add i32 %785, -1
  %977 = sext i32 %976 to i64
  %978 = mul i64 %977, 25
  %979 = add i64 %978, 16
  %980 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %979
  %981 = load double* %980, align 8
  %982 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %983 = fmul double %981, %982
  %984 = fsub double -0.000000e+00, %983
  %985 = add i32 %785, -1
  %986 = sext i32 %985 to i64
  %987 = mul i64 %986, 25
  %988 = add i64 %987, 16
  %989 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %988
  %990 = load double* %989, align 8
  %991 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %992 = fmul double %990, %991
  %993 = fsub double %984, %992
  %994 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %975
  store double %993, double* %994, align 8
  %995 = sext i32 %785 to i64
  %996 = mul i64 %995, 75
  %997 = add i64 %996, 21
  %998 = add i32 %785, -1
  %999 = sext i32 %998 to i64
  %1000 = mul i64 %999, 25
  %1001 = add i64 %1000, 21
  %1002 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1001
  %1003 = load double* %1002, align 8
  %1004 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1005 = fmul double %1003, %1004
  %1006 = fsub double -0.000000e+00, %1005
  %1007 = add i32 %785, -1
  %1008 = sext i32 %1007 to i64
  %1009 = mul i64 %1008, 25
  %1010 = add i64 %1009, 21
  %1011 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1010
  %1012 = load double* %1011, align 8
  %1013 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1014 = fmul double %1012, %1013
  %1015 = fsub double %1006, %1014
  %1016 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %997
  store double %1015, double* %1016, align 8
  %1017 = sext i32 %785 to i64
  %1018 = mul i64 %1017, 75
  %1019 = add i64 %1018, 2
  %1020 = add i32 %785, -1
  %1021 = sext i32 %1020 to i64
  %1022 = mul i64 %1021, 25
  %1023 = add i64 %1022, 2
  %1024 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1023
  %1025 = load double* %1024, align 8
  %1026 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1027 = fmul double %1025, %1026
  %1028 = fsub double -0.000000e+00, %1027
  %1029 = add i32 %785, -1
  %1030 = sext i32 %1029 to i64
  %1031 = mul i64 %1030, 25
  %1032 = add i64 %1031, 2
  %1033 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1032
  %1034 = load double* %1033, align 8
  %1035 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1036 = fmul double %1034, %1035
  %1037 = fsub double %1028, %1036
  %1038 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1019
  store double %1037, double* %1038, align 8
  %1039 = sext i32 %785 to i64
  %1040 = mul i64 %1039, 75
  %1041 = add i64 %1040, 7
  %1042 = add i32 %785, -1
  %1043 = sext i32 %1042 to i64
  %1044 = mul i64 %1043, 25
  %1045 = add i64 %1044, 7
  %1046 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1045
  %1047 = load double* %1046, align 8
  %1048 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1049 = fmul double %1047, %1048
  %1050 = fsub double -0.000000e+00, %1049
  %1051 = add i32 %785, -1
  %1052 = sext i32 %1051 to i64
  %1053 = mul i64 %1052, 25
  %1054 = add i64 %1053, 7
  %1055 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1054
  %1056 = load double* %1055, align 8
  %1057 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1058 = fmul double %1056, %1057
  %1059 = fsub double %1050, %1058
  %1060 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1041
  store double %1059, double* %1060, align 8
  %1061 = sext i32 %785 to i64
  %1062 = mul i64 %1061, 75
  %1063 = add i64 %1062, 12
  %1064 = add i32 %785, -1
  %1065 = sext i32 %1064 to i64
  %1066 = mul i64 %1065, 25
  %1067 = add i64 %1066, 12
  %1068 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1067
  %1069 = load double* %1068, align 8
  %1070 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1071 = fmul double %1069, %1070
  %1072 = fsub double -0.000000e+00, %1071
  %1073 = add i32 %785, -1
  %1074 = sext i32 %1073 to i64
  %1075 = mul i64 %1074, 25
  %1076 = add i64 %1075, 12
  %1077 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1076
  %1078 = load double* %1077, align 8
  %1079 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1080 = fmul double %1078, %1079
  %1081 = fsub double %1072, %1080
  %1082 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1083 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 21), align 8
  %1084 = fmul double %1082, %1083
  %1085 = fsub double %1081, %1084
  %1086 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1063
  store double %1085, double* %1086, align 8
  %1087 = sext i32 %785 to i64
  %1088 = mul i64 %1087, 75
  %1089 = add i64 %1088, 17
  %1090 = add i32 %785, -1
  %1091 = sext i32 %1090 to i64
  %1092 = mul i64 %1091, 25
  %1093 = add i64 %1092, 17
  %1094 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1093
  %1095 = load double* %1094, align 8
  %1096 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1097 = fmul double %1095, %1096
  %1098 = fsub double -0.000000e+00, %1097
  %1099 = add i32 %785, -1
  %1100 = sext i32 %1099 to i64
  %1101 = mul i64 %1100, 25
  %1102 = add i64 %1101, 17
  %1103 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1102
  %1104 = load double* %1103, align 8
  %1105 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1106 = fmul double %1104, %1105
  %1107 = fsub double %1098, %1106
  %1108 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1089
  store double %1107, double* %1108, align 8
  %1109 = sext i32 %785 to i64
  %1110 = mul i64 %1109, 75
  %1111 = add i64 %1110, 22
  %1112 = add i32 %785, -1
  %1113 = sext i32 %1112 to i64
  %1114 = mul i64 %1113, 25
  %1115 = add i64 %1114, 22
  %1116 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1115
  %1117 = load double* %1116, align 8
  %1118 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1119 = fmul double %1117, %1118
  %1120 = fsub double -0.000000e+00, %1119
  %1121 = add i32 %785, -1
  %1122 = sext i32 %1121 to i64
  %1123 = mul i64 %1122, 25
  %1124 = add i64 %1123, 22
  %1125 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1124
  %1126 = load double* %1125, align 8
  %1127 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1128 = fmul double %1126, %1127
  %1129 = fsub double %1120, %1128
  %1130 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1111
  store double %1129, double* %1130, align 8
  %1131 = sext i32 %785 to i64
  %1132 = mul i64 %1131, 75
  %1133 = add i64 %1132, 3
  %1134 = add i32 %785, -1
  %1135 = sext i32 %1134 to i64
  %1136 = mul i64 %1135, 25
  %1137 = add i64 %1136, 3
  %1138 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1137
  %1139 = load double* %1138, align 8
  %1140 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1141 = fmul double %1139, %1140
  %1142 = fsub double -0.000000e+00, %1141
  %1143 = add i32 %785, -1
  %1144 = sext i32 %1143 to i64
  %1145 = mul i64 %1144, 25
  %1146 = add i64 %1145, 3
  %1147 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1146
  %1148 = load double* %1147, align 8
  %1149 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1150 = fmul double %1148, %1149
  %1151 = fsub double %1142, %1150
  %1152 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1133
  store double %1151, double* %1152, align 8
  %1153 = sext i32 %785 to i64
  %1154 = mul i64 %1153, 75
  %1155 = add i64 %1154, 8
  %1156 = add i32 %785, -1
  %1157 = sext i32 %1156 to i64
  %1158 = mul i64 %1157, 25
  %1159 = add i64 %1158, 8
  %1160 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1159
  %1161 = load double* %1160, align 8
  %1162 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1163 = fmul double %1161, %1162
  %1164 = fsub double -0.000000e+00, %1163
  %1165 = add i32 %785, -1
  %1166 = sext i32 %1165 to i64
  %1167 = mul i64 %1166, 25
  %1168 = add i64 %1167, 8
  %1169 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1168
  %1170 = load double* %1169, align 8
  %1171 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1172 = fmul double %1170, %1171
  %1173 = fsub double %1164, %1172
  %1174 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1155
  store double %1173, double* %1174, align 8
  %1175 = sext i32 %785 to i64
  %1176 = mul i64 %1175, 75
  %1177 = add i64 %1176, 13
  %1178 = add i32 %785, -1
  %1179 = sext i32 %1178 to i64
  %1180 = mul i64 %1179, 25
  %1181 = add i64 %1180, 13
  %1182 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1181
  %1183 = load double* %1182, align 8
  %1184 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1185 = fmul double %1183, %1184
  %1186 = fsub double -0.000000e+00, %1185
  %1187 = add i32 %785, -1
  %1188 = sext i32 %1187 to i64
  %1189 = mul i64 %1188, 25
  %1190 = add i64 %1189, 13
  %1191 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1190
  %1192 = load double* %1191, align 8
  %1193 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1194 = fmul double %1192, %1193
  %1195 = fsub double %1186, %1194
  %1196 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1177
  store double %1195, double* %1196, align 8
  %1197 = sext i32 %785 to i64
  %1198 = mul i64 %1197, 75
  %1199 = add i64 %1198, 18
  %1200 = add i32 %785, -1
  %1201 = sext i32 %1200 to i64
  %1202 = mul i64 %1201, 25
  %1203 = add i64 %1202, 18
  %1204 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1203
  %1205 = load double* %1204, align 8
  %1206 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1207 = fmul double %1205, %1206
  %1208 = fsub double -0.000000e+00, %1207
  %1209 = add i32 %785, -1
  %1210 = sext i32 %1209 to i64
  %1211 = mul i64 %1210, 25
  %1212 = add i64 %1211, 18
  %1213 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1212
  %1214 = load double* %1213, align 8
  %1215 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1216 = fmul double %1214, %1215
  %1217 = fsub double %1208, %1216
  %1218 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1219 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 22), align 16
  %1220 = fmul double %1218, %1219
  %1221 = fsub double %1217, %1220
  %1222 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1199
  store double %1221, double* %1222, align 8
  %1223 = sext i32 %785 to i64
  %1224 = mul i64 %1223, 75
  %1225 = add i64 %1224, 23
  %1226 = add i32 %785, -1
  %1227 = sext i32 %1226 to i64
  %1228 = mul i64 %1227, 25
  %1229 = add i64 %1228, 23
  %1230 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1229
  %1231 = load double* %1230, align 8
  %1232 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1233 = fmul double %1231, %1232
  %1234 = fsub double -0.000000e+00, %1233
  %1235 = add i32 %785, -1
  %1236 = sext i32 %1235 to i64
  %1237 = mul i64 %1236, 25
  %1238 = add i64 %1237, 23
  %1239 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1238
  %1240 = load double* %1239, align 8
  %1241 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1242 = fmul double %1240, %1241
  %1243 = fsub double %1234, %1242
  %1244 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1225
  store double %1243, double* %1244, align 8
  %1245 = sext i32 %785 to i64
  %1246 = mul i64 %1245, 75
  %1247 = add i64 %1246, 4
  %1248 = add i32 %785, -1
  %1249 = sext i32 %1248 to i64
  %1250 = mul i64 %1249, 25
  %1251 = add i64 %1250, 4
  %1252 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1251
  %1253 = load double* %1252, align 8
  %1254 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1255 = fmul double %1253, %1254
  %1256 = fsub double -0.000000e+00, %1255
  %1257 = add i32 %785, -1
  %1258 = sext i32 %1257 to i64
  %1259 = mul i64 %1258, 25
  %1260 = add i64 %1259, 4
  %1261 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1260
  %1262 = load double* %1261, align 8
  %1263 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1264 = fmul double %1262, %1263
  %1265 = fsub double %1256, %1264
  %1266 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1247
  store double %1265, double* %1266, align 8
  %1267 = sext i32 %785 to i64
  %1268 = mul i64 %1267, 75
  %1269 = add i64 %1268, 9
  %1270 = add i32 %785, -1
  %1271 = sext i32 %1270 to i64
  %1272 = mul i64 %1271, 25
  %1273 = add i64 %1272, 9
  %1274 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1273
  %1275 = load double* %1274, align 8
  %1276 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1277 = fmul double %1275, %1276
  %1278 = fsub double -0.000000e+00, %1277
  %1279 = add i32 %785, -1
  %1280 = sext i32 %1279 to i64
  %1281 = mul i64 %1280, 25
  %1282 = add i64 %1281, 9
  %1283 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1282
  %1284 = load double* %1283, align 8
  %1285 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1286 = fmul double %1284, %1285
  %1287 = fsub double %1278, %1286
  %1288 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1269
  store double %1287, double* %1288, align 8
  %1289 = sext i32 %785 to i64
  %1290 = mul i64 %1289, 75
  %1291 = add i64 %1290, 14
  %1292 = add i32 %785, -1
  %1293 = sext i32 %1292 to i64
  %1294 = mul i64 %1293, 25
  %1295 = add i64 %1294, 14
  %1296 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1295
  %1297 = load double* %1296, align 8
  %1298 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1299 = fmul double %1297, %1298
  %1300 = fsub double -0.000000e+00, %1299
  %1301 = add i32 %785, -1
  %1302 = sext i32 %1301 to i64
  %1303 = mul i64 %1302, 25
  %1304 = add i64 %1303, 14
  %1305 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1304
  %1306 = load double* %1305, align 8
  %1307 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1308 = fmul double %1306, %1307
  %1309 = fsub double %1300, %1308
  %1310 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1291
  store double %1309, double* %1310, align 8
  %1311 = sext i32 %785 to i64
  %1312 = mul i64 %1311, 75
  %1313 = add i64 %1312, 19
  %1314 = add i32 %785, -1
  %1315 = sext i32 %1314 to i64
  %1316 = mul i64 %1315, 25
  %1317 = add i64 %1316, 19
  %1318 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1317
  %1319 = load double* %1318, align 8
  %1320 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1321 = fmul double %1319, %1320
  %1322 = fsub double -0.000000e+00, %1321
  %1323 = add i32 %785, -1
  %1324 = sext i32 %1323 to i64
  %1325 = mul i64 %1324, 25
  %1326 = add i64 %1325, 19
  %1327 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1326
  %1328 = load double* %1327, align 8
  %1329 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1330 = fmul double %1328, %1329
  %1331 = fsub double %1322, %1330
  %1332 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1313
  store double %1331, double* %1332, align 8
  %1333 = sext i32 %785 to i64
  %1334 = mul i64 %1333, 75
  %1335 = add i64 %1334, 24
  %1336 = add i32 %785, -1
  %1337 = sext i32 %1336 to i64
  %1338 = mul i64 %1337, 25
  %1339 = add i64 %1338, 24
  %1340 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1339
  %1341 = load double* %1340, align 8
  %1342 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1343 = fmul double %1341, %1342
  %1344 = fsub double -0.000000e+00, %1343
  %1345 = add i32 %785, -1
  %1346 = sext i32 %1345 to i64
  %1347 = mul i64 %1346, 25
  %1348 = add i64 %1347, 24
  %1349 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1348
  %1350 = load double* %1349, align 8
  %1351 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1352 = fmul double %1350, %1351
  %1353 = fsub double %1344, %1352
  %1354 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1355 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 23), align 8
  %1356 = fmul double %1354, %1355
  %1357 = fsub double %1353, %1356
  %1358 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1335
  store double %1357, double* %1358, align 8
  %1359 = sext i32 %785 to i64
  %1360 = mul i64 %1359, 75
  %1361 = add i64 %1360, 25
  %1362 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1363 = fmul double %1362, 2.000000e+00
  %1364 = sext i32 %785 to i64
  %1365 = mul i64 %1364, 25
  %1366 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1365
  %1367 = load double* %1366, align 8
  %1368 = fmul double %1363, %1367
  %1369 = fadd double %1368, 1.000000e+00
  %1370 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1371 = fmul double %1370, 2.000000e+00
  %1372 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 19), align 8
  %1373 = fmul double %1371, %1372
  %1374 = fadd double %1369, %1373
  %1375 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1361
  store double %1374, double* %1375, align 8
  %1376 = sext i32 %785 to i64
  %1377 = mul i64 %1376, 75
  %1378 = add i64 %1377, 30
  %1379 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1380 = fmul double %1379, 2.000000e+00
  %1381 = sext i32 %785 to i64
  %1382 = mul i64 %1381, 25
  %1383 = add i64 %1382, 5
  %1384 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1383
  %1385 = load double* %1384, align 8
  %1386 = fmul double %1380, %1385
  %1387 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1378
  store double %1386, double* %1387, align 8
  %1388 = sext i32 %785 to i64
  %1389 = mul i64 %1388, 75
  %1390 = add i64 %1389, 35
  %1391 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1392 = fmul double %1391, 2.000000e+00
  %1393 = sext i32 %785 to i64
  %1394 = mul i64 %1393, 25
  %1395 = add i64 %1394, 10
  %1396 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1395
  %1397 = load double* %1396, align 8
  %1398 = fmul double %1392, %1397
  %1399 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1390
  store double %1398, double* %1399, align 8
  %1400 = sext i32 %785 to i64
  %1401 = mul i64 %1400, 75
  %1402 = add i64 %1401, 40
  %1403 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1404 = fmul double %1403, 2.000000e+00
  %1405 = sext i32 %785 to i64
  %1406 = mul i64 %1405, 25
  %1407 = add i64 %1406, 15
  %1408 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1407
  %1409 = load double* %1408, align 8
  %1410 = fmul double %1404, %1409
  %1411 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1402
  store double %1410, double* %1411, align 8
  %1412 = sext i32 %785 to i64
  %1413 = mul i64 %1412, 75
  %1414 = add i64 %1413, 45
  %1415 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1416 = fmul double %1415, 2.000000e+00
  %1417 = sext i32 %785 to i64
  %1418 = mul i64 %1417, 25
  %1419 = add i64 %1418, 20
  %1420 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1419
  %1421 = load double* %1420, align 8
  %1422 = fmul double %1416, %1421
  %1423 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1414
  store double %1422, double* %1423, align 8
  %1424 = sext i32 %785 to i64
  %1425 = mul i64 %1424, 75
  %1426 = add i64 %1425, 26
  %1427 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1428 = fmul double %1427, 2.000000e+00
  %1429 = sext i32 %785 to i64
  %1430 = mul i64 %1429, 25
  %1431 = add i64 %1430, 1
  %1432 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1431
  %1433 = load double* %1432, align 8
  %1434 = fmul double %1428, %1433
  %1435 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1426
  store double %1434, double* %1435, align 8
  %1436 = sext i32 %785 to i64
  %1437 = mul i64 %1436, 75
  %1438 = add i64 %1437, 31
  %1439 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1440 = fmul double %1439, 2.000000e+00
  %1441 = sext i32 %785 to i64
  %1442 = mul i64 %1441, 25
  %1443 = add i64 %1442, 6
  %1444 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1443
  %1445 = load double* %1444, align 8
  %1446 = fmul double %1440, %1445
  %1447 = fadd double %1446, 1.000000e+00
  %1448 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1449 = fmul double %1448, 2.000000e+00
  %1450 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 20), align 16
  %1451 = fmul double %1449, %1450
  %1452 = fadd double %1447, %1451
  %1453 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1438
  store double %1452, double* %1453, align 8
  %1454 = sext i32 %785 to i64
  %1455 = mul i64 %1454, 75
  %1456 = add i64 %1455, 36
  %1457 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1458 = fmul double %1457, 2.000000e+00
  %1459 = sext i32 %785 to i64
  %1460 = mul i64 %1459, 25
  %1461 = add i64 %1460, 11
  %1462 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1461
  %1463 = load double* %1462, align 8
  %1464 = fmul double %1458, %1463
  %1465 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1456
  store double %1464, double* %1465, align 8
  %1466 = sext i32 %785 to i64
  %1467 = mul i64 %1466, 75
  %1468 = add i64 %1467, 41
  %1469 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1470 = fmul double %1469, 2.000000e+00
  %1471 = sext i32 %785 to i64
  %1472 = mul i64 %1471, 25
  %1473 = add i64 %1472, 16
  %1474 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1473
  %1475 = load double* %1474, align 8
  %1476 = fmul double %1470, %1475
  %1477 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1468
  store double %1476, double* %1477, align 8
  %1478 = sext i32 %785 to i64
  %1479 = mul i64 %1478, 75
  %1480 = add i64 %1479, 46
  %1481 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1482 = fmul double %1481, 2.000000e+00
  %1483 = sext i32 %785 to i64
  %1484 = mul i64 %1483, 25
  %1485 = add i64 %1484, 21
  %1486 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1485
  %1487 = load double* %1486, align 8
  %1488 = fmul double %1482, %1487
  %1489 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1480
  store double %1488, double* %1489, align 8
  %1490 = sext i32 %785 to i64
  %1491 = mul i64 %1490, 75
  %1492 = add i64 %1491, 27
  %1493 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1494 = fmul double %1493, 2.000000e+00
  %1495 = sext i32 %785 to i64
  %1496 = mul i64 %1495, 25
  %1497 = add i64 %1496, 2
  %1498 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1497
  %1499 = load double* %1498, align 8
  %1500 = fmul double %1494, %1499
  %1501 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1492
  store double %1500, double* %1501, align 8
  %1502 = sext i32 %785 to i64
  %1503 = mul i64 %1502, 75
  %1504 = add i64 %1503, 32
  %1505 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1506 = fmul double %1505, 2.000000e+00
  %1507 = sext i32 %785 to i64
  %1508 = mul i64 %1507, 25
  %1509 = add i64 %1508, 7
  %1510 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1509
  %1511 = load double* %1510, align 8
  %1512 = fmul double %1506, %1511
  %1513 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1504
  store double %1512, double* %1513, align 8
  %1514 = sext i32 %785 to i64
  %1515 = mul i64 %1514, 75
  %1516 = add i64 %1515, 37
  %1517 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1518 = fmul double %1517, 2.000000e+00
  %1519 = sext i32 %785 to i64
  %1520 = mul i64 %1519, 25
  %1521 = add i64 %1520, 12
  %1522 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1521
  %1523 = load double* %1522, align 8
  %1524 = fmul double %1518, %1523
  %1525 = fadd double %1524, 1.000000e+00
  %1526 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1527 = fmul double %1526, 2.000000e+00
  %1528 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 21), align 8
  %1529 = fmul double %1527, %1528
  %1530 = fadd double %1525, %1529
  %1531 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1516
  store double %1530, double* %1531, align 8
  %1532 = sext i32 %785 to i64
  %1533 = mul i64 %1532, 75
  %1534 = add i64 %1533, 42
  %1535 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1536 = fmul double %1535, 2.000000e+00
  %1537 = sext i32 %785 to i64
  %1538 = mul i64 %1537, 25
  %1539 = add i64 %1538, 17
  %1540 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1539
  %1541 = load double* %1540, align 8
  %1542 = fmul double %1536, %1541
  %1543 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1534
  store double %1542, double* %1543, align 8
  %1544 = sext i32 %785 to i64
  %1545 = mul i64 %1544, 75
  %1546 = add i64 %1545, 47
  %1547 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1548 = fmul double %1547, 2.000000e+00
  %1549 = sext i32 %785 to i64
  %1550 = mul i64 %1549, 25
  %1551 = add i64 %1550, 22
  %1552 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1551
  %1553 = load double* %1552, align 8
  %1554 = fmul double %1548, %1553
  %1555 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1546
  store double %1554, double* %1555, align 8
  %1556 = sext i32 %785 to i64
  %1557 = mul i64 %1556, 75
  %1558 = add i64 %1557, 28
  %1559 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1560 = fmul double %1559, 2.000000e+00
  %1561 = sext i32 %785 to i64
  %1562 = mul i64 %1561, 25
  %1563 = add i64 %1562, 3
  %1564 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1563
  %1565 = load double* %1564, align 8
  %1566 = fmul double %1560, %1565
  %1567 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1558
  store double %1566, double* %1567, align 8
  %1568 = sext i32 %785 to i64
  %1569 = mul i64 %1568, 75
  %1570 = add i64 %1569, 33
  %1571 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1572 = fmul double %1571, 2.000000e+00
  %1573 = sext i32 %785 to i64
  %1574 = mul i64 %1573, 25
  %1575 = add i64 %1574, 8
  %1576 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1575
  %1577 = load double* %1576, align 8
  %1578 = fmul double %1572, %1577
  %1579 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1570
  store double %1578, double* %1579, align 8
  %1580 = sext i32 %785 to i64
  %1581 = mul i64 %1580, 75
  %1582 = add i64 %1581, 38
  %1583 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1584 = fmul double %1583, 2.000000e+00
  %1585 = sext i32 %785 to i64
  %1586 = mul i64 %1585, 25
  %1587 = add i64 %1586, 13
  %1588 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1587
  %1589 = load double* %1588, align 8
  %1590 = fmul double %1584, %1589
  %1591 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1582
  store double %1590, double* %1591, align 8
  %1592 = sext i32 %785 to i64
  %1593 = mul i64 %1592, 75
  %1594 = add i64 %1593, 43
  %1595 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1596 = fmul double %1595, 2.000000e+00
  %1597 = sext i32 %785 to i64
  %1598 = mul i64 %1597, 25
  %1599 = add i64 %1598, 18
  %1600 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1599
  %1601 = load double* %1600, align 8
  %1602 = fmul double %1596, %1601
  %1603 = fadd double %1602, 1.000000e+00
  %1604 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1605 = fmul double %1604, 2.000000e+00
  %1606 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 22), align 16
  %1607 = fmul double %1605, %1606
  %1608 = fadd double %1603, %1607
  %1609 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1594
  store double %1608, double* %1609, align 8
  %1610 = sext i32 %785 to i64
  %1611 = mul i64 %1610, 75
  %1612 = add i64 %1611, 48
  %1613 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1614 = fmul double %1613, 2.000000e+00
  %1615 = sext i32 %785 to i64
  %1616 = mul i64 %1615, 25
  %1617 = add i64 %1616, 23
  %1618 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1617
  %1619 = load double* %1618, align 8
  %1620 = fmul double %1614, %1619
  %1621 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1612
  store double %1620, double* %1621, align 8
  %1622 = sext i32 %785 to i64
  %1623 = mul i64 %1622, 75
  %1624 = add i64 %1623, 29
  %1625 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1626 = fmul double %1625, 2.000000e+00
  %1627 = sext i32 %785 to i64
  %1628 = mul i64 %1627, 25
  %1629 = add i64 %1628, 4
  %1630 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1629
  %1631 = load double* %1630, align 8
  %1632 = fmul double %1626, %1631
  %1633 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1624
  store double %1632, double* %1633, align 8
  %1634 = sext i32 %785 to i64
  %1635 = mul i64 %1634, 75
  %1636 = add i64 %1635, 34
  %1637 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1638 = fmul double %1637, 2.000000e+00
  %1639 = sext i32 %785 to i64
  %1640 = mul i64 %1639, 25
  %1641 = add i64 %1640, 9
  %1642 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1641
  %1643 = load double* %1642, align 8
  %1644 = fmul double %1638, %1643
  %1645 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1636
  store double %1644, double* %1645, align 8
  %1646 = sext i32 %785 to i64
  %1647 = mul i64 %1646, 75
  %1648 = add i64 %1647, 39
  %1649 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1650 = fmul double %1649, 2.000000e+00
  %1651 = sext i32 %785 to i64
  %1652 = mul i64 %1651, 25
  %1653 = add i64 %1652, 14
  %1654 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1653
  %1655 = load double* %1654, align 8
  %1656 = fmul double %1650, %1655
  %1657 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1648
  store double %1656, double* %1657, align 8
  %1658 = sext i32 %785 to i64
  %1659 = mul i64 %1658, 75
  %1660 = add i64 %1659, 44
  %1661 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1662 = fmul double %1661, 2.000000e+00
  %1663 = sext i32 %785 to i64
  %1664 = mul i64 %1663, 25
  %1665 = add i64 %1664, 19
  %1666 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1665
  %1667 = load double* %1666, align 8
  %1668 = fmul double %1662, %1667
  %1669 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1660
  store double %1668, double* %1669, align 8
  %1670 = sext i32 %785 to i64
  %1671 = mul i64 %1670, 75
  %1672 = add i64 %1671, 49
  %1673 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1674 = fmul double %1673, 2.000000e+00
  %1675 = sext i32 %785 to i64
  %1676 = mul i64 %1675, 25
  %1677 = add i64 %1676, 24
  %1678 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1677
  %1679 = load double* %1678, align 8
  %1680 = fmul double %1674, %1679
  %1681 = fadd double %1680, 1.000000e+00
  %1682 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1683 = fmul double %1682, 2.000000e+00
  %1684 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 23), align 8
  %1685 = fmul double %1683, %1684
  %1686 = fadd double %1681, %1685
  %1687 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1672
  store double %1686, double* %1687, align 8
  %1688 = sext i32 %785 to i64
  %1689 = mul i64 %1688, 75
  %1690 = add i64 %1689, 50
  %1691 = add i32 %785, 1
  %1692 = sext i32 %1691 to i64
  %1693 = mul i64 %1692, 25
  %1694 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1693
  %1695 = load double* %1694, align 8
  %1696 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1697 = fmul double %1695, %1696
  %1698 = add i32 %785, 1
  %1699 = sext i32 %1698 to i64
  %1700 = mul i64 %1699, 25
  %1701 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1700
  %1702 = load double* %1701, align 8
  %1703 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1704 = fmul double %1702, %1703
  %1705 = fsub double %1697, %1704
  %1706 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1707 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 19), align 8
  %1708 = fmul double %1706, %1707
  %1709 = fsub double %1705, %1708
  %1710 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1690
  store double %1709, double* %1710, align 8
  %1711 = sext i32 %785 to i64
  %1712 = mul i64 %1711, 75
  %1713 = add i64 %1712, 55
  %1714 = add i32 %785, 1
  %1715 = sext i32 %1714 to i64
  %1716 = mul i64 %1715, 25
  %1717 = add i64 %1716, 5
  %1718 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1717
  %1719 = load double* %1718, align 8
  %1720 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1721 = fmul double %1719, %1720
  %1722 = add i32 %785, 1
  %1723 = sext i32 %1722 to i64
  %1724 = mul i64 %1723, 25
  %1725 = add i64 %1724, 5
  %1726 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1725
  %1727 = load double* %1726, align 8
  %1728 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1729 = fmul double %1727, %1728
  %1730 = fsub double %1721, %1729
  %1731 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1713
  store double %1730, double* %1731, align 8
  %1732 = sext i32 %785 to i64
  %1733 = mul i64 %1732, 75
  %1734 = add i64 %1733, 60
  %1735 = add i32 %785, 1
  %1736 = sext i32 %1735 to i64
  %1737 = mul i64 %1736, 25
  %1738 = add i64 %1737, 10
  %1739 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1738
  %1740 = load double* %1739, align 8
  %1741 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1742 = fmul double %1740, %1741
  %1743 = add i32 %785, 1
  %1744 = sext i32 %1743 to i64
  %1745 = mul i64 %1744, 25
  %1746 = add i64 %1745, 10
  %1747 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1746
  %1748 = load double* %1747, align 8
  %1749 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1750 = fmul double %1748, %1749
  %1751 = fsub double %1742, %1750
  %1752 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1734
  store double %1751, double* %1752, align 8
  %1753 = sext i32 %785 to i64
  %1754 = mul i64 %1753, 75
  %1755 = add i64 %1754, 65
  %1756 = add i32 %785, 1
  %1757 = sext i32 %1756 to i64
  %1758 = mul i64 %1757, 25
  %1759 = add i64 %1758, 15
  %1760 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1759
  %1761 = load double* %1760, align 8
  %1762 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1763 = fmul double %1761, %1762
  %1764 = add i32 %785, 1
  %1765 = sext i32 %1764 to i64
  %1766 = mul i64 %1765, 25
  %1767 = add i64 %1766, 15
  %1768 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1767
  %1769 = load double* %1768, align 8
  %1770 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1771 = fmul double %1769, %1770
  %1772 = fsub double %1763, %1771
  %1773 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1755
  store double %1772, double* %1773, align 8
  %1774 = sext i32 %785 to i64
  %1775 = mul i64 %1774, 75
  %1776 = add i64 %1775, 70
  %1777 = add i32 %785, 1
  %1778 = sext i32 %1777 to i64
  %1779 = mul i64 %1778, 25
  %1780 = add i64 %1779, 20
  %1781 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1780
  %1782 = load double* %1781, align 8
  %1783 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1784 = fmul double %1782, %1783
  %1785 = add i32 %785, 1
  %1786 = sext i32 %1785 to i64
  %1787 = mul i64 %1786, 25
  %1788 = add i64 %1787, 20
  %1789 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1788
  %1790 = load double* %1789, align 8
  %1791 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1792 = fmul double %1790, %1791
  %1793 = fsub double %1784, %1792
  %1794 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1776
  store double %1793, double* %1794, align 8
  %1795 = sext i32 %785 to i64
  %1796 = mul i64 %1795, 75
  %1797 = add i64 %1796, 51
  %1798 = add i32 %785, 1
  %1799 = sext i32 %1798 to i64
  %1800 = mul i64 %1799, 25
  %1801 = add i64 %1800, 1
  %1802 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1801
  %1803 = load double* %1802, align 8
  %1804 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1805 = fmul double %1803, %1804
  %1806 = add i32 %785, 1
  %1807 = sext i32 %1806 to i64
  %1808 = mul i64 %1807, 25
  %1809 = add i64 %1808, 1
  %1810 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1809
  %1811 = load double* %1810, align 8
  %1812 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1813 = fmul double %1811, %1812
  %1814 = fsub double %1805, %1813
  %1815 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1797
  store double %1814, double* %1815, align 8
  %1816 = sext i32 %785 to i64
  %1817 = mul i64 %1816, 75
  %1818 = add i64 %1817, 56
  %1819 = add i32 %785, 1
  %1820 = sext i32 %1819 to i64
  %1821 = mul i64 %1820, 25
  %1822 = add i64 %1821, 6
  %1823 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1822
  %1824 = load double* %1823, align 8
  %1825 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1826 = fmul double %1824, %1825
  %1827 = add i32 %785, 1
  %1828 = sext i32 %1827 to i64
  %1829 = mul i64 %1828, 25
  %1830 = add i64 %1829, 6
  %1831 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1830
  %1832 = load double* %1831, align 8
  %1833 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1834 = fmul double %1832, %1833
  %1835 = fsub double %1826, %1834
  %1836 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1837 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 20), align 16
  %1838 = fmul double %1836, %1837
  %1839 = fsub double %1835, %1838
  %1840 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1818
  store double %1839, double* %1840, align 8
  %1841 = sext i32 %785 to i64
  %1842 = mul i64 %1841, 75
  %1843 = add i64 %1842, 61
  %1844 = add i32 %785, 1
  %1845 = sext i32 %1844 to i64
  %1846 = mul i64 %1845, 25
  %1847 = add i64 %1846, 11
  %1848 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1847
  %1849 = load double* %1848, align 8
  %1850 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1851 = fmul double %1849, %1850
  %1852 = add i32 %785, 1
  %1853 = sext i32 %1852 to i64
  %1854 = mul i64 %1853, 25
  %1855 = add i64 %1854, 11
  %1856 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1855
  %1857 = load double* %1856, align 8
  %1858 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1859 = fmul double %1857, %1858
  %1860 = fsub double %1851, %1859
  %1861 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1843
  store double %1860, double* %1861, align 8
  %1862 = sext i32 %785 to i64
  %1863 = mul i64 %1862, 75
  %1864 = add i64 %1863, 66
  %1865 = add i32 %785, 1
  %1866 = sext i32 %1865 to i64
  %1867 = mul i64 %1866, 25
  %1868 = add i64 %1867, 16
  %1869 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1868
  %1870 = load double* %1869, align 8
  %1871 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1872 = fmul double %1870, %1871
  %1873 = add i32 %785, 1
  %1874 = sext i32 %1873 to i64
  %1875 = mul i64 %1874, 25
  %1876 = add i64 %1875, 16
  %1877 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1876
  %1878 = load double* %1877, align 8
  %1879 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1880 = fmul double %1878, %1879
  %1881 = fsub double %1872, %1880
  %1882 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1864
  store double %1881, double* %1882, align 8
  %1883 = sext i32 %785 to i64
  %1884 = mul i64 %1883, 75
  %1885 = add i64 %1884, 71
  %1886 = add i32 %785, 1
  %1887 = sext i32 %1886 to i64
  %1888 = mul i64 %1887, 25
  %1889 = add i64 %1888, 21
  %1890 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1889
  %1891 = load double* %1890, align 8
  %1892 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1893 = fmul double %1891, %1892
  %1894 = add i32 %785, 1
  %1895 = sext i32 %1894 to i64
  %1896 = mul i64 %1895, 25
  %1897 = add i64 %1896, 21
  %1898 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1897
  %1899 = load double* %1898, align 8
  %1900 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1901 = fmul double %1899, %1900
  %1902 = fsub double %1893, %1901
  %1903 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1885
  store double %1902, double* %1903, align 8
  %1904 = sext i32 %785 to i64
  %1905 = mul i64 %1904, 75
  %1906 = add i64 %1905, 52
  %1907 = add i32 %785, 1
  %1908 = sext i32 %1907 to i64
  %1909 = mul i64 %1908, 25
  %1910 = add i64 %1909, 2
  %1911 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1910
  %1912 = load double* %1911, align 8
  %1913 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1914 = fmul double %1912, %1913
  %1915 = add i32 %785, 1
  %1916 = sext i32 %1915 to i64
  %1917 = mul i64 %1916, 25
  %1918 = add i64 %1917, 2
  %1919 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1918
  %1920 = load double* %1919, align 8
  %1921 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1922 = fmul double %1920, %1921
  %1923 = fsub double %1914, %1922
  %1924 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1906
  store double %1923, double* %1924, align 8
  %1925 = sext i32 %785 to i64
  %1926 = mul i64 %1925, 75
  %1927 = add i64 %1926, 57
  %1928 = add i32 %785, 1
  %1929 = sext i32 %1928 to i64
  %1930 = mul i64 %1929, 25
  %1931 = add i64 %1930, 7
  %1932 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1931
  %1933 = load double* %1932, align 8
  %1934 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1935 = fmul double %1933, %1934
  %1936 = add i32 %785, 1
  %1937 = sext i32 %1936 to i64
  %1938 = mul i64 %1937, 25
  %1939 = add i64 %1938, 7
  %1940 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1939
  %1941 = load double* %1940, align 8
  %1942 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1943 = fmul double %1941, %1942
  %1944 = fsub double %1935, %1943
  %1945 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1927
  store double %1944, double* %1945, align 8
  %1946 = sext i32 %785 to i64
  %1947 = mul i64 %1946, 75
  %1948 = add i64 %1947, 62
  %1949 = add i32 %785, 1
  %1950 = sext i32 %1949 to i64
  %1951 = mul i64 %1950, 25
  %1952 = add i64 %1951, 12
  %1953 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1952
  %1954 = load double* %1953, align 8
  %1955 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1956 = fmul double %1954, %1955
  %1957 = add i32 %785, 1
  %1958 = sext i32 %1957 to i64
  %1959 = mul i64 %1958, 25
  %1960 = add i64 %1959, 12
  %1961 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1960
  %1962 = load double* %1961, align 8
  %1963 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1964 = fmul double %1962, %1963
  %1965 = fsub double %1956, %1964
  %1966 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1967 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 21), align 8
  %1968 = fmul double %1966, %1967
  %1969 = fsub double %1965, %1968
  %1970 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1948
  store double %1969, double* %1970, align 8
  %1971 = sext i32 %785 to i64
  %1972 = mul i64 %1971, 75
  %1973 = add i64 %1972, 67
  %1974 = add i32 %785, 1
  %1975 = sext i32 %1974 to i64
  %1976 = mul i64 %1975, 25
  %1977 = add i64 %1976, 17
  %1978 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1977
  %1979 = load double* %1978, align 8
  %1980 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1981 = fmul double %1979, %1980
  %1982 = add i32 %785, 1
  %1983 = sext i32 %1982 to i64
  %1984 = mul i64 %1983, 25
  %1985 = add i64 %1984, 17
  %1986 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1985
  %1987 = load double* %1986, align 8
  %1988 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1989 = fmul double %1987, %1988
  %1990 = fsub double %1981, %1989
  %1991 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1973
  store double %1990, double* %1991, align 8
  %1992 = sext i32 %785 to i64
  %1993 = mul i64 %1992, 75
  %1994 = add i64 %1993, 72
  %1995 = add i32 %785, 1
  %1996 = sext i32 %1995 to i64
  %1997 = mul i64 %1996, 25
  %1998 = add i64 %1997, 22
  %1999 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1998
  %2000 = load double* %1999, align 8
  %2001 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2002 = fmul double %2000, %2001
  %2003 = add i32 %785, 1
  %2004 = sext i32 %2003 to i64
  %2005 = mul i64 %2004, 25
  %2006 = add i64 %2005, 22
  %2007 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2006
  %2008 = load double* %2007, align 8
  %2009 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2010 = fmul double %2008, %2009
  %2011 = fsub double %2002, %2010
  %2012 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1994
  store double %2011, double* %2012, align 8
  %2013 = sext i32 %785 to i64
  %2014 = mul i64 %2013, 75
  %2015 = add i64 %2014, 53
  %2016 = add i32 %785, 1
  %2017 = sext i32 %2016 to i64
  %2018 = mul i64 %2017, 25
  %2019 = add i64 %2018, 3
  %2020 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2019
  %2021 = load double* %2020, align 8
  %2022 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2023 = fmul double %2021, %2022
  %2024 = add i32 %785, 1
  %2025 = sext i32 %2024 to i64
  %2026 = mul i64 %2025, 25
  %2027 = add i64 %2026, 3
  %2028 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2027
  %2029 = load double* %2028, align 8
  %2030 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2031 = fmul double %2029, %2030
  %2032 = fsub double %2023, %2031
  %2033 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2015
  store double %2032, double* %2033, align 8
  %2034 = sext i32 %785 to i64
  %2035 = mul i64 %2034, 75
  %2036 = add i64 %2035, 58
  %2037 = add i32 %785, 1
  %2038 = sext i32 %2037 to i64
  %2039 = mul i64 %2038, 25
  %2040 = add i64 %2039, 8
  %2041 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2040
  %2042 = load double* %2041, align 8
  %2043 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2044 = fmul double %2042, %2043
  %2045 = add i32 %785, 1
  %2046 = sext i32 %2045 to i64
  %2047 = mul i64 %2046, 25
  %2048 = add i64 %2047, 8
  %2049 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2048
  %2050 = load double* %2049, align 8
  %2051 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2052 = fmul double %2050, %2051
  %2053 = fsub double %2044, %2052
  %2054 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2036
  store double %2053, double* %2054, align 8
  %2055 = sext i32 %785 to i64
  %2056 = mul i64 %2055, 75
  %2057 = add i64 %2056, 63
  %2058 = add i32 %785, 1
  %2059 = sext i32 %2058 to i64
  %2060 = mul i64 %2059, 25
  %2061 = add i64 %2060, 13
  %2062 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2061
  %2063 = load double* %2062, align 8
  %2064 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2065 = fmul double %2063, %2064
  %2066 = add i32 %785, 1
  %2067 = sext i32 %2066 to i64
  %2068 = mul i64 %2067, 25
  %2069 = add i64 %2068, 13
  %2070 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2069
  %2071 = load double* %2070, align 8
  %2072 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2073 = fmul double %2071, %2072
  %2074 = fsub double %2065, %2073
  %2075 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2057
  store double %2074, double* %2075, align 8
  %2076 = sext i32 %785 to i64
  %2077 = mul i64 %2076, 75
  %2078 = add i64 %2077, 68
  %2079 = add i32 %785, 1
  %2080 = sext i32 %2079 to i64
  %2081 = mul i64 %2080, 25
  %2082 = add i64 %2081, 18
  %2083 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2082
  %2084 = load double* %2083, align 8
  %2085 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2086 = fmul double %2084, %2085
  %2087 = add i32 %785, 1
  %2088 = sext i32 %2087 to i64
  %2089 = mul i64 %2088, 25
  %2090 = add i64 %2089, 18
  %2091 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2090
  %2092 = load double* %2091, align 8
  %2093 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2094 = fmul double %2092, %2093
  %2095 = fsub double %2086, %2094
  %2096 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2097 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 22), align 16
  %2098 = fmul double %2096, %2097
  %2099 = fsub double %2095, %2098
  %2100 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2078
  store double %2099, double* %2100, align 8
  %2101 = sext i32 %785 to i64
  %2102 = mul i64 %2101, 75
  %2103 = add i64 %2102, 73
  %2104 = add i32 %785, 1
  %2105 = sext i32 %2104 to i64
  %2106 = mul i64 %2105, 25
  %2107 = add i64 %2106, 23
  %2108 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2107
  %2109 = load double* %2108, align 8
  %2110 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2111 = fmul double %2109, %2110
  %2112 = add i32 %785, 1
  %2113 = sext i32 %2112 to i64
  %2114 = mul i64 %2113, 25
  %2115 = add i64 %2114, 23
  %2116 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2115
  %2117 = load double* %2116, align 8
  %2118 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2119 = fmul double %2117, %2118
  %2120 = fsub double %2111, %2119
  %2121 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2103
  store double %2120, double* %2121, align 8
  %2122 = sext i32 %785 to i64
  %2123 = mul i64 %2122, 75
  %2124 = add i64 %2123, 54
  %2125 = add i32 %785, 1
  %2126 = sext i32 %2125 to i64
  %2127 = mul i64 %2126, 25
  %2128 = add i64 %2127, 4
  %2129 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2128
  %2130 = load double* %2129, align 8
  %2131 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2132 = fmul double %2130, %2131
  %2133 = add i32 %785, 1
  %2134 = sext i32 %2133 to i64
  %2135 = mul i64 %2134, 25
  %2136 = add i64 %2135, 4
  %2137 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2136
  %2138 = load double* %2137, align 8
  %2139 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2140 = fmul double %2138, %2139
  %2141 = fsub double %2132, %2140
  %2142 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2124
  store double %2141, double* %2142, align 8
  %2143 = sext i32 %785 to i64
  %2144 = mul i64 %2143, 75
  %2145 = add i64 %2144, 59
  %2146 = add i32 %785, 1
  %2147 = sext i32 %2146 to i64
  %2148 = mul i64 %2147, 25
  %2149 = add i64 %2148, 9
  %2150 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2149
  %2151 = load double* %2150, align 8
  %2152 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2153 = fmul double %2151, %2152
  %2154 = add i32 %785, 1
  %2155 = sext i32 %2154 to i64
  %2156 = mul i64 %2155, 25
  %2157 = add i64 %2156, 9
  %2158 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2157
  %2159 = load double* %2158, align 8
  %2160 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2161 = fmul double %2159, %2160
  %2162 = fsub double %2153, %2161
  %2163 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2145
  store double %2162, double* %2163, align 8
  %2164 = sext i32 %785 to i64
  %2165 = mul i64 %2164, 75
  %2166 = add i64 %2165, 64
  %2167 = add i32 %785, 1
  %2168 = sext i32 %2167 to i64
  %2169 = mul i64 %2168, 25
  %2170 = add i64 %2169, 14
  %2171 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2170
  %2172 = load double* %2171, align 8
  %2173 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2174 = fmul double %2172, %2173
  %2175 = add i32 %785, 1
  %2176 = sext i32 %2175 to i64
  %2177 = mul i64 %2176, 25
  %2178 = add i64 %2177, 14
  %2179 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2178
  %2180 = load double* %2179, align 8
  %2181 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2182 = fmul double %2180, %2181
  %2183 = fsub double %2174, %2182
  %2184 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2166
  store double %2183, double* %2184, align 8
  %2185 = sext i32 %785 to i64
  %2186 = mul i64 %2185, 75
  %2187 = add i64 %2186, 69
  %2188 = add i32 %785, 1
  %2189 = sext i32 %2188 to i64
  %2190 = mul i64 %2189, 25
  %2191 = add i64 %2190, 19
  %2192 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2191
  %2193 = load double* %2192, align 8
  %2194 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2195 = fmul double %2193, %2194
  %2196 = add i32 %785, 1
  %2197 = sext i32 %2196 to i64
  %2198 = mul i64 %2197, 25
  %2199 = add i64 %2198, 19
  %2200 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2199
  %2201 = load double* %2200, align 8
  %2202 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2203 = fmul double %2201, %2202
  %2204 = fsub double %2195, %2203
  %2205 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2187
  store double %2204, double* %2205, align 8
  %2206 = sext i32 %785 to i64
  %2207 = mul i64 %2206, 75
  %2208 = add i64 %2207, 74
  %2209 = add i32 %785, 1
  %2210 = sext i32 %2209 to i64
  %2211 = mul i64 %2210, 25
  %2212 = add i64 %2211, 24
  %2213 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2212
  %2214 = load double* %2213, align 8
  %2215 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2216 = fmul double %2214, %2215
  %2217 = add i32 %785, 1
  %2218 = sext i32 %2217 to i64
  %2219 = mul i64 %2218, 25
  %2220 = add i64 %2219, 24
  %2221 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2220
  %2222 = load double* %2221, align 8
  %2223 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2224 = fmul double %2222, %2223
  %2225 = fsub double %2216, %2224
  %2226 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2227 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 23), align 8
  %2228 = fmul double %2226, %2227
  %2229 = fsub double %2225, %2228
  %2230 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2208
  store double %2229, double* %2230, align 8
  %2231 = icmp eq i32 %785, %783
  %2232 = add i32 %785, 1
  %2233 = icmp ne i1 %2231, false
  br i1 %2233, label %"12", label %"11"

"11":                                             ; preds = %"10"
  br label %"10"

"12":                                             ; preds = %"10", %"9"
  %2234 = sext i32 %8 to i64
  %2235 = mul i64 %2234, 325
  %2236 = sext i32 %12 to i64
  %2237 = mul i64 %2236, 5
  %2238 = add i64 %2235, %2237
  %2239 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2238
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 25), double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 50), double* %2239) #1
  %2240 = load i32* %ksize, align 4
  %2241 = add i32 %2240, -1
  %2242 = icmp sle i32 1, %2241
  br i1 %2242, label %"13", label %"15"

"13":                                             ; preds = %"14", %"12"
  %2243 = phi i32 [ %2296, %"14" ], [ 1, %"12" ]
  %2244 = sext i32 %2243 to i64
  %2245 = mul i64 %2244, 21125
  %2246 = sext i32 %8 to i64
  %2247 = mul i64 %2246, 325
  %2248 = add i64 %2245, %2247
  %2249 = sext i32 %12 to i64
  %2250 = mul i64 %2249, 5
  %2251 = add i64 %2248, %2250
  %2252 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2251
  %2253 = add i32 %2243, -1
  %2254 = sext i32 %2253 to i64
  %2255 = mul i64 %2254, 21125
  %2256 = sext i32 %8 to i64
  %2257 = mul i64 %2256, 325
  %2258 = add i64 %2255, %2257
  %2259 = sext i32 %12 to i64
  %2260 = mul i64 %2259, 5
  %2261 = add i64 %2258, %2260
  %2262 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2261
  %2263 = sext i32 %2243 to i64
  %2264 = mul i64 %2263, 75
  %2265 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2264
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2265, double* %2262, double* %2252) #1
  %2266 = sext i32 %2243 to i64
  %2267 = mul i64 %2266, 75
  %2268 = add i64 %2267, 25
  %2269 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2268
  %2270 = add i32 %2243, -1
  %2271 = sext i32 %2270 to i64
  %2272 = mul i64 %2271, 75
  %2273 = add i64 %2272, 50
  %2274 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2273
  %2275 = sext i32 %2243 to i64
  %2276 = mul i64 %2275, 75
  %2277 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2276
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2277, double* %2274, double* %2269) #1
  %2278 = sext i32 %2243 to i64
  %2279 = mul i64 %2278, 21125
  %2280 = sext i32 %8 to i64
  %2281 = mul i64 %2280, 325
  %2282 = add i64 %2279, %2281
  %2283 = sext i32 %12 to i64
  %2284 = mul i64 %2283, 5
  %2285 = add i64 %2282, %2284
  %2286 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2285
  %2287 = sext i32 %2243 to i64
  %2288 = mul i64 %2287, 75
  %2289 = add i64 %2288, 50
  %2290 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2289
  %2291 = sext i32 %2243 to i64
  %2292 = mul i64 %2291, 75
  %2293 = add i64 %2292, 25
  %2294 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2293
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* %2294, double* %2290, double* %2286) #1
  %2295 = icmp eq i32 %2243, %2241
  %2296 = add i32 %2243, 1
  %2297 = icmp ne i1 %2295, false
  br i1 %2297, label %"15", label %"14"

"14":                                             ; preds = %"13"
  br label %"13"

"15":                                             ; preds = %"13", %"12"
  %2298 = load i32* %ksize, align 4
  %2299 = sext i32 %2298 to i64
  %2300 = mul i64 %2299, 21125
  %2301 = sext i32 %8 to i64
  %2302 = mul i64 %2301, 325
  %2303 = add i64 %2300, %2302
  %2304 = sext i32 %12 to i64
  %2305 = mul i64 %2304, 5
  %2306 = add i64 %2303, %2305
  %2307 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2306
  %2308 = load i32* %ksize, align 4
  %2309 = add i32 %2308, -1
  %2310 = sext i32 %2309 to i64
  %2311 = mul i64 %2310, 21125
  %2312 = sext i32 %8 to i64
  %2313 = mul i64 %2312, 325
  %2314 = add i64 %2311, %2313
  %2315 = sext i32 %12 to i64
  %2316 = mul i64 %2315, 5
  %2317 = add i64 %2314, %2316
  %2318 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2317
  %2319 = load i32* %ksize, align 4
  %2320 = sext i32 %2319 to i64
  %2321 = mul i64 %2320, 75
  %2322 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2321
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2322, double* %2318, double* %2307) #1
  %2323 = load i32* %ksize, align 4
  %2324 = sext i32 %2323 to i64
  %2325 = mul i64 %2324, 75
  %2326 = add i64 %2325, 25
  %2327 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2326
  %2328 = load i32* %ksize, align 4
  %2329 = add i32 %2328, -1
  %2330 = sext i32 %2329 to i64
  %2331 = mul i64 %2330, 75
  %2332 = add i64 %2331, 50
  %2333 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2332
  %2334 = load i32* %ksize, align 4
  %2335 = sext i32 %2334 to i64
  %2336 = mul i64 %2335, 75
  %2337 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2336
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2337, double* %2333, double* %2327) #1
  %2338 = load i32* %ksize, align 4
  %2339 = sext i32 %2338 to i64
  %2340 = mul i64 %2339, 21125
  %2341 = sext i32 %8 to i64
  %2342 = mul i64 %2341, 325
  %2343 = add i64 %2340, %2342
  %2344 = sext i32 %12 to i64
  %2345 = mul i64 %2344, 5
  %2346 = add i64 %2343, %2345
  %2347 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2346
  %2348 = load i32* %ksize, align 4
  %2349 = sext i32 %2348 to i64
  %2350 = mul i64 %2349, 75
  %2351 = add i64 %2350, 25
  %2352 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2351
  call void bitcast (void (...)* @binvrhs_ to void (double*, double*)*)(double* %2352, double* %2347) #1
  %2353 = load i32* %ksize, align 4
  %2354 = add i32 %2353, -1
  %2355 = icmp sge i32 %2354, 0
  br i1 %2355, label %"16", label %"24"

"16":                                             ; preds = %"23", %"15"
  %2356 = phi i32 [ %2417, %"23" ], [ %2354, %"15" ]
  br i1 true, label %"17", label %"22"

"17":                                             ; preds = %"21", %"16"
  %2357 = phi i32 [ %2414, %"21" ], [ 1, %"16" ]
  br i1 true, label %"18", label %"20"

"18":                                             ; preds = %"19", %"17"
  %2358 = phi i32 [ %2411, %"19" ], [ 1, %"17" ]
  %2359 = sext i32 %2356 to i64
  %2360 = mul i64 %2359, 21125
  %2361 = sext i32 %8 to i64
  %2362 = mul i64 %2361, 325
  %2363 = add i64 %2360, %2362
  %2364 = sext i32 %12 to i64
  %2365 = mul i64 %2364, 5
  %2366 = add i64 %2363, %2365
  %2367 = sext i32 %2357 to i64
  %2368 = add i64 %2366, %2367
  %2369 = add i64 %2368, -1
  %2370 = sext i32 %2356 to i64
  %2371 = mul i64 %2370, 21125
  %2372 = sext i32 %8 to i64
  %2373 = mul i64 %2372, 325
  %2374 = add i64 %2371, %2373
  %2375 = sext i32 %12 to i64
  %2376 = mul i64 %2375, 5
  %2377 = add i64 %2374, %2376
  %2378 = sext i32 %2357 to i64
  %2379 = add i64 %2377, %2378
  %2380 = add i64 %2379, -1
  %2381 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2380
  %2382 = load double* %2381, align 8
  %2383 = sext i32 %2356 to i64
  %2384 = mul i64 %2383, 75
  %2385 = sext i32 %2358 to i64
  %2386 = mul i64 %2385, 5
  %2387 = add i64 %2384, %2386
  %2388 = sext i32 %2357 to i64
  %2389 = add i64 %2387, %2388
  %2390 = add i64 %2389, 44
  %2391 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2390
  %2392 = load double* %2391, align 8
  %2393 = add i32 %2356, 1
  %2394 = sext i32 %2393 to i64
  %2395 = mul i64 %2394, 21125
  %2396 = sext i32 %8 to i64
  %2397 = mul i64 %2396, 325
  %2398 = add i64 %2395, %2397
  %2399 = sext i32 %12 to i64
  %2400 = mul i64 %2399, 5
  %2401 = add i64 %2398, %2400
  %2402 = sext i32 %2358 to i64
  %2403 = add i64 %2401, %2402
  %2404 = add i64 %2403, -1
  %2405 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2404
  %2406 = load double* %2405, align 8
  %2407 = fmul double %2392, %2406
  %2408 = fsub double %2382, %2407
  %2409 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2369
  store double %2408, double* %2409, align 8
  %2410 = icmp eq i32 %2358, 5
  %2411 = add i32 %2358, 1
  %2412 = icmp ne i1 %2410, false
  br i1 %2412, label %"20", label %"19"

"19":                                             ; preds = %"18"
  br label %"18"

"20":                                             ; preds = %"18", %"17"
  %2413 = icmp eq i32 %2357, 5
  %2414 = add i32 %2357, 1
  %2415 = icmp ne i1 %2413, false
  br i1 %2415, label %"22", label %"21"

"21":                                             ; preds = %"20"
  br label %"17"

"22":                                             ; preds = %"20", %"16"
  %2416 = icmp eq i32 %2356, 0
  %2417 = add i32 %2356, -1
  %2418 = icmp ne i1 %2416, false
  br i1 %2418, label %"24", label %"23"

"23":                                             ; preds = %"22"
  br label %"16"

"24":                                             ; preds = %"22", %"15"
  %2419 = icmp eq i32 %12, %10
  %2420 = add i32 %12, 1
  %2421 = icmp ne i1 %2419, false
  br i1 %2421, label %"26", label %"25"

"25":                                             ; preds = %"24"
  br label %"6"

"26":                                             ; preds = %"24", %"5"
  %2422 = icmp eq i32 %8, %6
  %2423 = add i32 %8, 1
  %2424 = icmp ne i1 %2422, false
  br i1 %2424, label %"28", label %"27"

"27":                                             ; preds = %"26"
  br label %"5"

"28":                                             ; preds = %"26", %"4"
  %2425 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %2426 = trunc i32 %2425 to i1
  %2427 = icmp ne i1 %2426, false
  br i1 %2427, label %"29", label %"30"

"29":                                             ; preds = %"28"
  call void bitcast (void (...)* @timer_stop_ to void (i32*)*)(i32* @1) #1
  br label %"30"

"30":                                             ; preds = %"29", %"28"
  br label %"31"

"31":                                             ; preds = %"30"
  %2428 = bitcast i32* %ksize to i8*
  call void @llvm.lifetime.end(i64 4, i8* %2428)
  br label %"32"

"32":                                             ; preds = %"31"
  br label %return

return:                                           ; preds = %"32"
  ret void
}

declare void @timer_start_(...)

declare void @lhsinit_(...)

declare void @binvcrhs_(...)

declare void @matvec_sub_(...)

declare void @matmul_sub_(...)

declare void @binvrhs_(...)

declare void @timer_stop_(...)

; Function Attrs: nounwind
declare void @llvm.lifetime.end(i64, i8* nocapture) #1

attributes #0 = { nounwind uwtable "no-frame-pointer-elim-non-leaf"="true" }
attributes #1 = { nounwind }

!0 = metadata !{i32 0, i32 2}
