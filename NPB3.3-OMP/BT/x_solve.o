; ModuleID = 'x_solve.f'
target datalayout = "e-p:64:64:64-S128-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f16:16:16-f32:32:32-f64:64:64-f128:128:128-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64"
target triple = "x86_64-unknown-linux-gnu"

module asm "\09.ident\09\22GCC: (GNU) 4.8.2 20140206 (prerelease) LLVM: 3.4\22"

%0 = type { double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, [65 x double], double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double, double }
%1 = type { [1352000 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [270400 x double], [1352000 x double], [1352000 x double] }
%2 = type { double, [3 x i32], i32 }
%3 = type { [65 x double], [65 x double], [325 x double], [325 x double] }
%4 = type { [1625 x double], [1625 x double], [4875 x double], double, double, double }

@constants_ = common unnamed_addr global %0 zeroinitializer, align 32
@fields_ = common global %1 zeroinitializer, align 32
@global_ = common unnamed_addr global %2 zeroinitializer, align 16
@work_1d_ = common unnamed_addr global %3 zeroinitializer, align 32
@work_lhs_ = common global %4 zeroinitializer, align 32
@0 = internal constant i32 6
@1 = internal constant i32 6

; Function Attrs: nounwind uwtable
define void @x_solve_() unnamed_addr #0 {
entry:
  %i = alloca i32
  %isize = alloca i32
  %j = alloca i32
  %k = alloca i32
  %m = alloca i32
  %n = alloca i32
  %D.2142 = alloca i32
  %D.2182 = alloca i32
  %D.2145 = alloca i32
  %D.2181 = alloca i32
  %D.2148 = alloca i32
  %D.2157 = alloca i32
  %D.2156 = alloca double
  %D.2155 = alloca double
  %D.2154 = alloca double
  %D.2153 = alloca double
  %D.2152 = alloca double
  %D.2151 = alloca double
  %D.2159 = alloca i32
  %D.2162 = alloca i32
  %D.2164 = alloca i32
  %D.2169 = alloca i32
  %i.12 = alloca i32
  %D.2180 = alloca i32
  %D.2179 = alloca i32
  %D.2178 = alloca i32
  %"alloca point" = bitcast i32 0 to i32
  %"ssa point" = bitcast i32 0 to i32
  br label %"2"

"2":                                              ; preds = %entry
  %0 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %1 = trunc i32 %0 to i1
  %2 = icmp ne i1 %1, false
  br i1 %2, label %"3", label %"4"

"3":                                              ; preds = %"2"
  call void bitcast (void (...)* @timer_start_ to void (i32*)*)(i32* @0) #1
  br label %"4"

"4":                                              ; preds = %"3", %"2"
  %3 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 0), align 4
  %4 = add i32 %3, -1
  store i32 %4, i32* %isize, align 4
  %5 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 2), align 4
  %6 = add i32 %5, -2
  %7 = icmp sle i32 1, %6
  br i1 %7, label %"5", label %"28"

"5":                                              ; preds = %"27", %"4"
  %8 = phi i32 [ %2428, %"27" ], [ 1, %"4" ]
  %9 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 1, i64 1), align 4
  %10 = add i32 %9, -2
  %11 = icmp sle i32 1, %10
  br i1 %11, label %"6", label %"26"

"6":                                              ; preds = %"25", %"5"
  %12 = phi i32 [ %2425, %"25" ], [ 1, %"5" ]
  %13 = load i32* %isize, align 4
  %14 = icmp sle i32 0, %13
  br i1 %14, label %"7", label %"9"

"7":                                              ; preds = %"8", %"6"
  %15 = phi i32 [ %785, %"8" ], [ 0, %"6" ]
  %16 = sext i32 %8 to i64
  %17 = mul i64 %16, 4225
  %18 = sext i32 %12 to i64
  %19 = mul i64 %18, 65
  %20 = add i64 %17, %19
  %21 = sext i32 %15 to i64
  %22 = add i64 %20, %21
  %23 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 5, i64 0), i64 %22
  %24 = load double* %23, align 8
  store double %24, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %25 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %26 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %27 = fmul double %25, %26
  store double %27, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %28 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %29 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %30 = fmul double %28, %29
  store double %30, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %31 = sext i32 %15 to i64
  %32 = mul i64 %31, 25
  %33 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %32
  store double 0.000000e+00, double* %33, align 8
  %34 = sext i32 %15 to i64
  %35 = mul i64 %34, 25
  %36 = add i64 %35, 5
  %37 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %36
  store double 1.000000e+00, double* %37, align 8
  %38 = sext i32 %15 to i64
  %39 = mul i64 %38, 25
  %40 = add i64 %39, 10
  %41 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %40
  store double 0.000000e+00, double* %41, align 8
  %42 = sext i32 %15 to i64
  %43 = mul i64 %42, 25
  %44 = add i64 %43, 15
  %45 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %44
  store double 0.000000e+00, double* %45, align 8
  %46 = sext i32 %15 to i64
  %47 = mul i64 %46, 25
  %48 = add i64 %47, 20
  %49 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %48
  store double 0.000000e+00, double* %49, align 8
  %50 = sext i32 %15 to i64
  %51 = mul i64 %50, 25
  %52 = add i64 %51, 1
  %53 = sext i32 %8 to i64
  %54 = mul i64 %53, 4225
  %55 = sext i32 %12 to i64
  %56 = mul i64 %55, 65
  %57 = add i64 %54, %56
  %58 = sext i32 %15 to i64
  %59 = add i64 %57, %58
  %60 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %59
  %61 = load double* %60, align 8
  %62 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %63 = fmul double %61, %62
  %64 = sext i32 %8 to i64
  %65 = mul i64 %64, 21125
  %66 = sext i32 %12 to i64
  %67 = mul i64 %66, 325
  %68 = add i64 %65, %67
  %69 = sext i32 %15 to i64
  %70 = mul i64 %69, 5
  %71 = add i64 %68, %70
  %72 = add i64 %71, 1
  %73 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %72
  %74 = load double* %73, align 8
  %75 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %76 = fmul double %74, %75
  %77 = sext i32 %8 to i64
  %78 = mul i64 %77, 21125
  %79 = sext i32 %12 to i64
  %80 = mul i64 %79, 325
  %81 = add i64 %78, %80
  %82 = sext i32 %15 to i64
  %83 = mul i64 %82, 5
  %84 = add i64 %81, %83
  %85 = add i64 %84, 1
  %86 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %85
  %87 = load double* %86, align 8
  %88 = fmul double %76, %87
  %89 = fsub double %63, %88
  %90 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %52
  store double %89, double* %90, align 8
  %91 = sext i32 %15 to i64
  %92 = mul i64 %91, 25
  %93 = add i64 %92, 6
  %94 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %95 = fsub double 2.000000e+00, %94
  %96 = sext i32 %8 to i64
  %97 = mul i64 %96, 21125
  %98 = sext i32 %12 to i64
  %99 = mul i64 %98, 325
  %100 = add i64 %97, %99
  %101 = sext i32 %15 to i64
  %102 = mul i64 %101, 5
  %103 = add i64 %100, %102
  %104 = add i64 %103, 1
  %105 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %104
  %106 = load double* %105, align 8
  %107 = sext i32 %8 to i64
  %108 = mul i64 %107, 21125
  %109 = sext i32 %12 to i64
  %110 = mul i64 %109, 325
  %111 = add i64 %108, %110
  %112 = sext i32 %15 to i64
  %113 = mul i64 %112, 5
  %114 = add i64 %111, %113
  %115 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %114
  %116 = load double* %115, align 8
  %117 = fdiv double %106, %116
  %118 = fmul double %95, %117
  %119 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %93
  store double %118, double* %119, align 8
  %120 = sext i32 %15 to i64
  %121 = mul i64 %120, 25
  %122 = add i64 %121, 11
  %123 = sext i32 %8 to i64
  %124 = mul i64 %123, 21125
  %125 = sext i32 %12 to i64
  %126 = mul i64 %125, 325
  %127 = add i64 %124, %126
  %128 = sext i32 %15 to i64
  %129 = mul i64 %128, 5
  %130 = add i64 %127, %129
  %131 = add i64 %130, 2
  %132 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %131
  %133 = load double* %132, align 8
  %134 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %135 = fmul double %133, %134
  %136 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %137 = fmul double %135, %136
  %138 = fsub double -0.000000e+00, %137
  %139 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %122
  store double %138, double* %139, align 8
  %140 = sext i32 %15 to i64
  %141 = mul i64 %140, 25
  %142 = add i64 %141, 16
  %143 = sext i32 %8 to i64
  %144 = mul i64 %143, 21125
  %145 = sext i32 %12 to i64
  %146 = mul i64 %145, 325
  %147 = add i64 %144, %146
  %148 = sext i32 %15 to i64
  %149 = mul i64 %148, 5
  %150 = add i64 %147, %149
  %151 = add i64 %150, 3
  %152 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %151
  %153 = load double* %152, align 8
  %154 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %155 = fmul double %153, %154
  %156 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %157 = fmul double %155, %156
  %158 = fsub double -0.000000e+00, %157
  %159 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %142
  store double %158, double* %159, align 8
  %160 = sext i32 %15 to i64
  %161 = mul i64 %160, 25
  %162 = add i64 %161, 21
  %163 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %164 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %162
  store double %163, double* %164, align 8
  %165 = sext i32 %15 to i64
  %166 = mul i64 %165, 25
  %167 = add i64 %166, 2
  %168 = sext i32 %8 to i64
  %169 = mul i64 %168, 21125
  %170 = sext i32 %12 to i64
  %171 = mul i64 %170, 325
  %172 = add i64 %169, %171
  %173 = sext i32 %15 to i64
  %174 = mul i64 %173, 5
  %175 = add i64 %172, %174
  %176 = add i64 %175, 1
  %177 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %176
  %178 = load double* %177, align 8
  %179 = sext i32 %8 to i64
  %180 = mul i64 %179, 21125
  %181 = sext i32 %12 to i64
  %182 = mul i64 %181, 325
  %183 = add i64 %180, %182
  %184 = sext i32 %15 to i64
  %185 = mul i64 %184, 5
  %186 = add i64 %183, %185
  %187 = add i64 %186, 2
  %188 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %187
  %189 = load double* %188, align 8
  %190 = fmul double %178, %189
  %191 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %192 = fmul double %190, %191
  %193 = fsub double -0.000000e+00, %192
  %194 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %167
  store double %193, double* %194, align 8
  %195 = sext i32 %15 to i64
  %196 = mul i64 %195, 25
  %197 = add i64 %196, 7
  %198 = sext i32 %8 to i64
  %199 = mul i64 %198, 21125
  %200 = sext i32 %12 to i64
  %201 = mul i64 %200, 325
  %202 = add i64 %199, %201
  %203 = sext i32 %15 to i64
  %204 = mul i64 %203, 5
  %205 = add i64 %202, %204
  %206 = add i64 %205, 2
  %207 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %206
  %208 = load double* %207, align 8
  %209 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %210 = fmul double %208, %209
  %211 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %197
  store double %210, double* %211, align 8
  %212 = sext i32 %15 to i64
  %213 = mul i64 %212, 25
  %214 = add i64 %213, 12
  %215 = sext i32 %8 to i64
  %216 = mul i64 %215, 21125
  %217 = sext i32 %12 to i64
  %218 = mul i64 %217, 325
  %219 = add i64 %216, %218
  %220 = sext i32 %15 to i64
  %221 = mul i64 %220, 5
  %222 = add i64 %219, %221
  %223 = add i64 %222, 1
  %224 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %223
  %225 = load double* %224, align 8
  %226 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %227 = fmul double %225, %226
  %228 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %214
  store double %227, double* %228, align 8
  %229 = sext i32 %15 to i64
  %230 = mul i64 %229, 25
  %231 = add i64 %230, 17
  %232 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %231
  store double 0.000000e+00, double* %232, align 8
  %233 = sext i32 %15 to i64
  %234 = mul i64 %233, 25
  %235 = add i64 %234, 22
  %236 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %235
  store double 0.000000e+00, double* %236, align 8
  %237 = sext i32 %15 to i64
  %238 = mul i64 %237, 25
  %239 = add i64 %238, 3
  %240 = sext i32 %8 to i64
  %241 = mul i64 %240, 21125
  %242 = sext i32 %12 to i64
  %243 = mul i64 %242, 325
  %244 = add i64 %241, %243
  %245 = sext i32 %15 to i64
  %246 = mul i64 %245, 5
  %247 = add i64 %244, %246
  %248 = add i64 %247, 1
  %249 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %248
  %250 = load double* %249, align 8
  %251 = sext i32 %8 to i64
  %252 = mul i64 %251, 21125
  %253 = sext i32 %12 to i64
  %254 = mul i64 %253, 325
  %255 = add i64 %252, %254
  %256 = sext i32 %15 to i64
  %257 = mul i64 %256, 5
  %258 = add i64 %255, %257
  %259 = add i64 %258, 3
  %260 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %259
  %261 = load double* %260, align 8
  %262 = fmul double %250, %261
  %263 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %264 = fmul double %262, %263
  %265 = fsub double -0.000000e+00, %264
  %266 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %239
  store double %265, double* %266, align 8
  %267 = sext i32 %15 to i64
  %268 = mul i64 %267, 25
  %269 = add i64 %268, 8
  %270 = sext i32 %8 to i64
  %271 = mul i64 %270, 21125
  %272 = sext i32 %12 to i64
  %273 = mul i64 %272, 325
  %274 = add i64 %271, %273
  %275 = sext i32 %15 to i64
  %276 = mul i64 %275, 5
  %277 = add i64 %274, %276
  %278 = add i64 %277, 3
  %279 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %278
  %280 = load double* %279, align 8
  %281 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %282 = fmul double %280, %281
  %283 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %269
  store double %282, double* %283, align 8
  %284 = sext i32 %15 to i64
  %285 = mul i64 %284, 25
  %286 = add i64 %285, 13
  %287 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %286
  store double 0.000000e+00, double* %287, align 8
  %288 = sext i32 %15 to i64
  %289 = mul i64 %288, 25
  %290 = add i64 %289, 18
  %291 = sext i32 %8 to i64
  %292 = mul i64 %291, 21125
  %293 = sext i32 %12 to i64
  %294 = mul i64 %293, 325
  %295 = add i64 %292, %294
  %296 = sext i32 %15 to i64
  %297 = mul i64 %296, 5
  %298 = add i64 %295, %297
  %299 = add i64 %298, 1
  %300 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %299
  %301 = load double* %300, align 8
  %302 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %303 = fmul double %301, %302
  %304 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %290
  store double %303, double* %304, align 8
  %305 = sext i32 %15 to i64
  %306 = mul i64 %305, 25
  %307 = add i64 %306, 23
  %308 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %307
  store double 0.000000e+00, double* %308, align 8
  %309 = sext i32 %15 to i64
  %310 = mul i64 %309, 25
  %311 = add i64 %310, 4
  %312 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %313 = fmul double %312, 2.000000e+00
  %314 = sext i32 %8 to i64
  %315 = mul i64 %314, 4225
  %316 = sext i32 %12 to i64
  %317 = mul i64 %316, 65
  %318 = add i64 %315, %317
  %319 = sext i32 %15 to i64
  %320 = add i64 %318, %319
  %321 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 6, i64 0), i64 %320
  %322 = load double* %321, align 8
  %323 = fmul double %313, %322
  %324 = sext i32 %8 to i64
  %325 = mul i64 %324, 21125
  %326 = sext i32 %12 to i64
  %327 = mul i64 %326, 325
  %328 = add i64 %325, %327
  %329 = sext i32 %15 to i64
  %330 = mul i64 %329, 5
  %331 = add i64 %328, %330
  %332 = add i64 %331, 4
  %333 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %332
  %334 = load double* %333, align 8
  %335 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %336 = fmul double %334, %335
  %337 = fsub double %323, %336
  %338 = sext i32 %8 to i64
  %339 = mul i64 %338, 21125
  %340 = sext i32 %12 to i64
  %341 = mul i64 %340, 325
  %342 = add i64 %339, %341
  %343 = sext i32 %15 to i64
  %344 = mul i64 %343, 5
  %345 = add i64 %342, %344
  %346 = add i64 %345, 1
  %347 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %346
  %348 = load double* %347, align 8
  %349 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %350 = fmul double %348, %349
  %351 = fmul double %337, %350
  %352 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %311
  store double %351, double* %352, align 8
  %353 = sext i32 %15 to i64
  %354 = mul i64 %353, 25
  %355 = add i64 %354, 9
  %356 = sext i32 %8 to i64
  %357 = mul i64 %356, 21125
  %358 = sext i32 %12 to i64
  %359 = mul i64 %358, 325
  %360 = add i64 %357, %359
  %361 = sext i32 %15 to i64
  %362 = mul i64 %361, 5
  %363 = add i64 %360, %362
  %364 = add i64 %363, 4
  %365 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %364
  %366 = load double* %365, align 8
  %367 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %368 = fmul double %366, %367
  %369 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %370 = fmul double %368, %369
  %371 = sext i32 %8 to i64
  %372 = mul i64 %371, 21125
  %373 = sext i32 %12 to i64
  %374 = mul i64 %373, 325
  %375 = add i64 %372, %374
  %376 = sext i32 %15 to i64
  %377 = mul i64 %376, 5
  %378 = add i64 %375, %377
  %379 = add i64 %378, 1
  %380 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %379
  %381 = load double* %380, align 8
  %382 = sext i32 %8 to i64
  %383 = mul i64 %382, 21125
  %384 = sext i32 %12 to i64
  %385 = mul i64 %384, 325
  %386 = add i64 %383, %385
  %387 = sext i32 %15 to i64
  %388 = mul i64 %387, 5
  %389 = add i64 %386, %388
  %390 = add i64 %389, 1
  %391 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %390
  %392 = load double* %391, align 8
  %393 = fmul double %381, %392
  %394 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %395 = fmul double %393, %394
  %396 = sext i32 %8 to i64
  %397 = mul i64 %396, 4225
  %398 = sext i32 %12 to i64
  %399 = mul i64 %398, 65
  %400 = add i64 %397, %399
  %401 = sext i32 %15 to i64
  %402 = add i64 %400, %401
  %403 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 4, i64 0), i64 %402
  %404 = load double* %403, align 8
  %405 = fadd double %395, %404
  %406 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %407 = fmul double %405, %406
  %408 = fsub double %370, %407
  %409 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %355
  store double %408, double* %409, align 8
  %410 = sext i32 %15 to i64
  %411 = mul i64 %410, 25
  %412 = add i64 %411, 14
  %413 = sext i32 %8 to i64
  %414 = mul i64 %413, 21125
  %415 = sext i32 %12 to i64
  %416 = mul i64 %415, 325
  %417 = add i64 %414, %416
  %418 = sext i32 %15 to i64
  %419 = mul i64 %418, 5
  %420 = add i64 %417, %419
  %421 = add i64 %420, 2
  %422 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %421
  %423 = load double* %422, align 8
  %424 = sext i32 %8 to i64
  %425 = mul i64 %424, 21125
  %426 = sext i32 %12 to i64
  %427 = mul i64 %426, 325
  %428 = add i64 %425, %427
  %429 = sext i32 %15 to i64
  %430 = mul i64 %429, 5
  %431 = add i64 %428, %430
  %432 = add i64 %431, 1
  %433 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %432
  %434 = load double* %433, align 8
  %435 = fmul double %423, %434
  %436 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %437 = fmul double %435, %436
  %438 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %439 = fmul double %437, %438
  %440 = fsub double -0.000000e+00, %439
  %441 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %412
  store double %440, double* %441, align 8
  %442 = sext i32 %15 to i64
  %443 = mul i64 %442, 25
  %444 = add i64 %443, 19
  %445 = sext i32 %8 to i64
  %446 = mul i64 %445, 21125
  %447 = sext i32 %12 to i64
  %448 = mul i64 %447, 325
  %449 = add i64 %446, %448
  %450 = sext i32 %15 to i64
  %451 = mul i64 %450, 5
  %452 = add i64 %449, %451
  %453 = add i64 %452, 3
  %454 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %453
  %455 = load double* %454, align 8
  %456 = sext i32 %8 to i64
  %457 = mul i64 %456, 21125
  %458 = sext i32 %12 to i64
  %459 = mul i64 %458, 325
  %460 = add i64 %457, %459
  %461 = sext i32 %15 to i64
  %462 = mul i64 %461, 5
  %463 = add i64 %460, %462
  %464 = add i64 %463, 1
  %465 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %464
  %466 = load double* %465, align 8
  %467 = fmul double %455, %466
  %468 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 69), align 8
  %469 = fmul double %467, %468
  %470 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %471 = fmul double %469, %470
  %472 = fsub double -0.000000e+00, %471
  %473 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %444
  store double %472, double* %473, align 8
  %474 = sext i32 %15 to i64
  %475 = mul i64 %474, 25
  %476 = add i64 %475, 24
  %477 = sext i32 %8 to i64
  %478 = mul i64 %477, 21125
  %479 = sext i32 %12 to i64
  %480 = mul i64 %479, 325
  %481 = add i64 %478, %480
  %482 = sext i32 %15 to i64
  %483 = mul i64 %482, 5
  %484 = add i64 %481, %483
  %485 = add i64 %484, 1
  %486 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %485
  %487 = load double* %486, align 8
  %488 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %489 = fmul double %487, %488
  %490 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 68), align 16
  %491 = fmul double %489, %490
  %492 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %476
  store double %491, double* %492, align 8
  %493 = sext i32 %15 to i64
  %494 = mul i64 %493, 25
  %495 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %494
  store double 0.000000e+00, double* %495, align 8
  %496 = sext i32 %15 to i64
  %497 = mul i64 %496, 25
  %498 = add i64 %497, 5
  %499 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %498
  store double 0.000000e+00, double* %499, align 8
  %500 = sext i32 %15 to i64
  %501 = mul i64 %500, 25
  %502 = add i64 %501, 10
  %503 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %502
  store double 0.000000e+00, double* %503, align 8
  %504 = sext i32 %15 to i64
  %505 = mul i64 %504, 25
  %506 = add i64 %505, 15
  %507 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %506
  store double 0.000000e+00, double* %507, align 8
  %508 = sext i32 %15 to i64
  %509 = mul i64 %508, 25
  %510 = add i64 %509, 20
  %511 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %510
  store double 0.000000e+00, double* %511, align 8
  %512 = sext i32 %15 to i64
  %513 = mul i64 %512, 25
  %514 = add i64 %513, 1
  %515 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %516 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %517 = fmul double %515, %516
  %518 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %519 = fmul double %517, %518
  %520 = sext i32 %8 to i64
  %521 = mul i64 %520, 21125
  %522 = sext i32 %12 to i64
  %523 = mul i64 %522, 325
  %524 = add i64 %521, %523
  %525 = sext i32 %15 to i64
  %526 = mul i64 %525, 5
  %527 = add i64 %524, %526
  %528 = add i64 %527, 1
  %529 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %528
  %530 = load double* %529, align 8
  %531 = fmul double %519, %530
  %532 = fsub double -0.000000e+00, %531
  %533 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %514
  store double %532, double* %533, align 8
  %534 = sext i32 %15 to i64
  %535 = mul i64 %534, 25
  %536 = add i64 %535, 6
  %537 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %538 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %539 = fmul double %537, %538
  %540 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %541 = fmul double %539, %540
  %542 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %536
  store double %541, double* %542, align 8
  %543 = sext i32 %15 to i64
  %544 = mul i64 %543, 25
  %545 = add i64 %544, 11
  %546 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %545
  store double 0.000000e+00, double* %546, align 8
  %547 = sext i32 %15 to i64
  %548 = mul i64 %547, 25
  %549 = add i64 %548, 16
  %550 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %549
  store double 0.000000e+00, double* %550, align 8
  %551 = sext i32 %15 to i64
  %552 = mul i64 %551, 25
  %553 = add i64 %552, 21
  %554 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %553
  store double 0.000000e+00, double* %554, align 8
  %555 = sext i32 %15 to i64
  %556 = mul i64 %555, 25
  %557 = add i64 %556, 2
  %558 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %559 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %560 = fmul double %558, %559
  %561 = sext i32 %8 to i64
  %562 = mul i64 %561, 21125
  %563 = sext i32 %12 to i64
  %564 = mul i64 %563, 325
  %565 = add i64 %562, %564
  %566 = sext i32 %15 to i64
  %567 = mul i64 %566, 5
  %568 = add i64 %565, %567
  %569 = add i64 %568, 2
  %570 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %569
  %571 = load double* %570, align 8
  %572 = fmul double %560, %571
  %573 = fsub double -0.000000e+00, %572
  %574 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %557
  store double %573, double* %574, align 8
  %575 = sext i32 %15 to i64
  %576 = mul i64 %575, 25
  %577 = add i64 %576, 7
  %578 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %577
  store double 0.000000e+00, double* %578, align 8
  %579 = sext i32 %15 to i64
  %580 = mul i64 %579, 25
  %581 = add i64 %580, 12
  %582 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %583 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %584 = fmul double %582, %583
  %585 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %581
  store double %584, double* %585, align 8
  %586 = sext i32 %15 to i64
  %587 = mul i64 %586, 25
  %588 = add i64 %587, 17
  %589 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %588
  store double 0.000000e+00, double* %589, align 8
  %590 = sext i32 %15 to i64
  %591 = mul i64 %590, 25
  %592 = add i64 %591, 22
  %593 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %592
  store double 0.000000e+00, double* %593, align 8
  %594 = sext i32 %15 to i64
  %595 = mul i64 %594, 25
  %596 = add i64 %595, 3
  %597 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %598 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %599 = fmul double %597, %598
  %600 = sext i32 %8 to i64
  %601 = mul i64 %600, 21125
  %602 = sext i32 %12 to i64
  %603 = mul i64 %602, 325
  %604 = add i64 %601, %603
  %605 = sext i32 %15 to i64
  %606 = mul i64 %605, 5
  %607 = add i64 %604, %606
  %608 = add i64 %607, 3
  %609 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %608
  %610 = load double* %609, align 8
  %611 = fmul double %599, %610
  %612 = fsub double -0.000000e+00, %611
  %613 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %596
  store double %612, double* %613, align 8
  %614 = sext i32 %15 to i64
  %615 = mul i64 %614, 25
  %616 = add i64 %615, 8
  %617 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %616
  store double 0.000000e+00, double* %617, align 8
  %618 = sext i32 %15 to i64
  %619 = mul i64 %618, 25
  %620 = add i64 %619, 13
  %621 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %620
  store double 0.000000e+00, double* %621, align 8
  %622 = sext i32 %15 to i64
  %623 = mul i64 %622, 25
  %624 = add i64 %623, 18
  %625 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %626 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %627 = fmul double %625, %626
  %628 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %624
  store double %627, double* %628, align 8
  %629 = sext i32 %15 to i64
  %630 = mul i64 %629, 25
  %631 = add i64 %630, 23
  %632 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %631
  store double 0.000000e+00, double* %632, align 8
  %633 = sext i32 %8 to i64
  %634 = mul i64 %633, 21125
  %635 = sext i32 %12 to i64
  %636 = mul i64 %635, 325
  %637 = add i64 %634, %636
  %638 = sext i32 %15 to i64
  %639 = mul i64 %638, 5
  %640 = add i64 %637, %639
  %641 = add i64 %640, 1
  %642 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %641
  %643 = load double* %642, align 8
  %644 = fmul double %643, %643
  %645 = sext i32 %8 to i64
  %646 = mul i64 %645, 21125
  %647 = sext i32 %12 to i64
  %648 = mul i64 %647, 325
  %649 = add i64 %646, %648
  %650 = sext i32 %15 to i64
  %651 = mul i64 %650, 5
  %652 = add i64 %649, %651
  %653 = add i64 %652, 2
  %654 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %653
  %655 = load double* %654, align 8
  %656 = fmul double %655, %655
  %657 = sext i32 %8 to i64
  %658 = mul i64 %657, 21125
  %659 = sext i32 %12 to i64
  %660 = mul i64 %659, 325
  %661 = add i64 %658, %660
  %662 = sext i32 %15 to i64
  %663 = mul i64 %662, 5
  %664 = add i64 %661, %663
  %665 = add i64 %664, 3
  %666 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %665
  %667 = load double* %666, align 8
  %668 = fmul double %667, %667
  %669 = sext i32 %15 to i64
  %670 = mul i64 %669, 25
  %671 = add i64 %670, 4
  %672 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %673 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %674 = fmul double %672, %673
  %675 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %676 = fsub double %674, %675
  %677 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %678 = fmul double %676, %677
  %679 = fmul double %678, %644
  %680 = fsub double -0.000000e+00, %679
  %681 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %682 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %683 = fsub double %681, %682
  %684 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %685 = fmul double %683, %684
  %686 = fmul double %685, %656
  %687 = fsub double %680, %686
  %688 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %689 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %690 = fsub double %688, %689
  %691 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 5), align 8
  %692 = fmul double %690, %691
  %693 = fmul double %692, %668
  %694 = fsub double %687, %693
  %695 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %696 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %697 = fmul double %695, %696
  %698 = sext i32 %8 to i64
  %699 = mul i64 %698, 21125
  %700 = sext i32 %12 to i64
  %701 = mul i64 %700, 325
  %702 = add i64 %699, %701
  %703 = sext i32 %15 to i64
  %704 = mul i64 %703, 5
  %705 = add i64 %702, %704
  %706 = add i64 %705, 4
  %707 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %706
  %708 = load double* %707, align 8
  %709 = fmul double %697, %708
  %710 = fsub double %694, %709
  %711 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %671
  store double %710, double* %711, align 8
  %712 = sext i32 %15 to i64
  %713 = mul i64 %712, 25
  %714 = add i64 %713, 9
  %715 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 93), align 8
  %716 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %717 = fmul double %715, %716
  %718 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %719 = fsub double %717, %718
  %720 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %721 = fmul double %719, %720
  %722 = sext i32 %8 to i64
  %723 = mul i64 %722, 21125
  %724 = sext i32 %12 to i64
  %725 = mul i64 %724, 325
  %726 = add i64 %723, %725
  %727 = sext i32 %15 to i64
  %728 = mul i64 %727, 5
  %729 = add i64 %726, %728
  %730 = add i64 %729, 1
  %731 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %730
  %732 = load double* %731, align 8
  %733 = fmul double %721, %732
  %734 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %714
  store double %733, double* %734, align 8
  %735 = sext i32 %15 to i64
  %736 = mul i64 %735, 25
  %737 = add i64 %736, 14
  %738 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %739 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %740 = fsub double %738, %739
  %741 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %742 = fmul double %740, %741
  %743 = sext i32 %8 to i64
  %744 = mul i64 %743, 21125
  %745 = sext i32 %12 to i64
  %746 = mul i64 %745, 325
  %747 = add i64 %744, %746
  %748 = sext i32 %15 to i64
  %749 = mul i64 %748, 5
  %750 = add i64 %747, %749
  %751 = add i64 %750, 2
  %752 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %751
  %753 = load double* %752, align 8
  %754 = fmul double %742, %753
  %755 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %737
  store double %754, double* %755, align 8
  %756 = sext i32 %15 to i64
  %757 = mul i64 %756, 25
  %758 = add i64 %757, 19
  %759 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 65), align 8
  %760 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %761 = fsub double %759, %760
  %762 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %763 = fmul double %761, %762
  %764 = sext i32 %8 to i64
  %765 = mul i64 %764, 21125
  %766 = sext i32 %12 to i64
  %767 = mul i64 %766, 325
  %768 = add i64 %765, %767
  %769 = sext i32 %15 to i64
  %770 = mul i64 %769, 5
  %771 = add i64 %768, %770
  %772 = add i64 %771, 3
  %773 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 0, i64 0), i64 %772
  %774 = load double* %773, align 8
  %775 = fmul double %763, %774
  %776 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %758
  store double %775, double* %776, align 8
  %777 = sext i32 %15 to i64
  %778 = mul i64 %777, 25
  %779 = add i64 %778, 24
  %780 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 66), align 16
  %781 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %782 = fmul double %780, %781
  %783 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %779
  store double %782, double* %783, align 8
  %784 = icmp eq i32 %15, %13
  %785 = add i32 %15, 1
  %786 = icmp ne i1 %784, false
  br i1 %786, label %"9", label %"8"

"8":                                              ; preds = %"7"
  br label %"7"

"9":                                              ; preds = %"7", %"6"
  call void bitcast (void (...)* @lhsinit_ to void ([4875 x double]*, i32*)*)([4875 x double]* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2), i32* %isize) #1
  %787 = load i32* %isize, align 4
  %788 = add i32 %787, -1
  %789 = icmp sle i32 1, %788
  br i1 %789, label %"10", label %"12"

"10":                                             ; preds = %"11", %"9"
  %790 = phi i32 [ %2237, %"11" ], [ 1, %"9" ]
  %791 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %792 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 0), align 16
  %793 = fmul double %791, %792
  store double %793, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %794 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 25), align 8
  %795 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 1), align 8
  %796 = fmul double %794, %795
  store double %796, double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %797 = sext i32 %790 to i64
  %798 = mul i64 %797, 75
  %799 = add i32 %790, -1
  %800 = sext i32 %799 to i64
  %801 = mul i64 %800, 25
  %802 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %801
  %803 = load double* %802, align 8
  %804 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %805 = fmul double %803, %804
  %806 = fsub double -0.000000e+00, %805
  %807 = add i32 %790, -1
  %808 = sext i32 %807 to i64
  %809 = mul i64 %808, 25
  %810 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %809
  %811 = load double* %810, align 8
  %812 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %813 = fmul double %811, %812
  %814 = fsub double %806, %813
  %815 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %816 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 9), align 8
  %817 = fmul double %815, %816
  %818 = fsub double %814, %817
  %819 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %798
  store double %818, double* %819, align 8
  %820 = sext i32 %790 to i64
  %821 = mul i64 %820, 75
  %822 = add i64 %821, 5
  %823 = add i32 %790, -1
  %824 = sext i32 %823 to i64
  %825 = mul i64 %824, 25
  %826 = add i64 %825, 5
  %827 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %826
  %828 = load double* %827, align 8
  %829 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %830 = fmul double %828, %829
  %831 = fsub double -0.000000e+00, %830
  %832 = add i32 %790, -1
  %833 = sext i32 %832 to i64
  %834 = mul i64 %833, 25
  %835 = add i64 %834, 5
  %836 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %835
  %837 = load double* %836, align 8
  %838 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %839 = fmul double %837, %838
  %840 = fsub double %831, %839
  %841 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %822
  store double %840, double* %841, align 8
  %842 = sext i32 %790 to i64
  %843 = mul i64 %842, 75
  %844 = add i64 %843, 10
  %845 = add i32 %790, -1
  %846 = sext i32 %845 to i64
  %847 = mul i64 %846, 25
  %848 = add i64 %847, 10
  %849 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %848
  %850 = load double* %849, align 8
  %851 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %852 = fmul double %850, %851
  %853 = fsub double -0.000000e+00, %852
  %854 = add i32 %790, -1
  %855 = sext i32 %854 to i64
  %856 = mul i64 %855, 25
  %857 = add i64 %856, 10
  %858 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %857
  %859 = load double* %858, align 8
  %860 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %861 = fmul double %859, %860
  %862 = fsub double %853, %861
  %863 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %844
  store double %862, double* %863, align 8
  %864 = sext i32 %790 to i64
  %865 = mul i64 %864, 75
  %866 = add i64 %865, 15
  %867 = add i32 %790, -1
  %868 = sext i32 %867 to i64
  %869 = mul i64 %868, 25
  %870 = add i64 %869, 15
  %871 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %870
  %872 = load double* %871, align 8
  %873 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %874 = fmul double %872, %873
  %875 = fsub double -0.000000e+00, %874
  %876 = add i32 %790, -1
  %877 = sext i32 %876 to i64
  %878 = mul i64 %877, 25
  %879 = add i64 %878, 15
  %880 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %879
  %881 = load double* %880, align 8
  %882 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %883 = fmul double %881, %882
  %884 = fsub double %875, %883
  %885 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %866
  store double %884, double* %885, align 8
  %886 = sext i32 %790 to i64
  %887 = mul i64 %886, 75
  %888 = add i64 %887, 20
  %889 = add i32 %790, -1
  %890 = sext i32 %889 to i64
  %891 = mul i64 %890, 25
  %892 = add i64 %891, 20
  %893 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %892
  %894 = load double* %893, align 8
  %895 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %896 = fmul double %894, %895
  %897 = fsub double -0.000000e+00, %896
  %898 = add i32 %790, -1
  %899 = sext i32 %898 to i64
  %900 = mul i64 %899, 25
  %901 = add i64 %900, 20
  %902 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %901
  %903 = load double* %902, align 8
  %904 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %905 = fmul double %903, %904
  %906 = fsub double %897, %905
  %907 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %888
  store double %906, double* %907, align 8
  %908 = sext i32 %790 to i64
  %909 = mul i64 %908, 75
  %910 = add i64 %909, 1
  %911 = add i32 %790, -1
  %912 = sext i32 %911 to i64
  %913 = mul i64 %912, 25
  %914 = add i64 %913, 1
  %915 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %914
  %916 = load double* %915, align 8
  %917 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %918 = fmul double %916, %917
  %919 = fsub double -0.000000e+00, %918
  %920 = add i32 %790, -1
  %921 = sext i32 %920 to i64
  %922 = mul i64 %921, 25
  %923 = add i64 %922, 1
  %924 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %923
  %925 = load double* %924, align 8
  %926 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %927 = fmul double %925, %926
  %928 = fsub double %919, %927
  %929 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %910
  store double %928, double* %929, align 8
  %930 = sext i32 %790 to i64
  %931 = mul i64 %930, 75
  %932 = add i64 %931, 6
  %933 = add i32 %790, -1
  %934 = sext i32 %933 to i64
  %935 = mul i64 %934, 25
  %936 = add i64 %935, 6
  %937 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %936
  %938 = load double* %937, align 8
  %939 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %940 = fmul double %938, %939
  %941 = fsub double -0.000000e+00, %940
  %942 = add i32 %790, -1
  %943 = sext i32 %942 to i64
  %944 = mul i64 %943, 25
  %945 = add i64 %944, 6
  %946 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %945
  %947 = load double* %946, align 8
  %948 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %949 = fmul double %947, %948
  %950 = fsub double %941, %949
  %951 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %952 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 10), align 16
  %953 = fmul double %951, %952
  %954 = fsub double %950, %953
  %955 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %932
  store double %954, double* %955, align 8
  %956 = sext i32 %790 to i64
  %957 = mul i64 %956, 75
  %958 = add i64 %957, 11
  %959 = add i32 %790, -1
  %960 = sext i32 %959 to i64
  %961 = mul i64 %960, 25
  %962 = add i64 %961, 11
  %963 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %962
  %964 = load double* %963, align 8
  %965 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %966 = fmul double %964, %965
  %967 = fsub double -0.000000e+00, %966
  %968 = add i32 %790, -1
  %969 = sext i32 %968 to i64
  %970 = mul i64 %969, 25
  %971 = add i64 %970, 11
  %972 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %971
  %973 = load double* %972, align 8
  %974 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %975 = fmul double %973, %974
  %976 = fsub double %967, %975
  %977 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %958
  store double %976, double* %977, align 8
  %978 = sext i32 %790 to i64
  %979 = mul i64 %978, 75
  %980 = add i64 %979, 16
  %981 = add i32 %790, -1
  %982 = sext i32 %981 to i64
  %983 = mul i64 %982, 25
  %984 = add i64 %983, 16
  %985 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %984
  %986 = load double* %985, align 8
  %987 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %988 = fmul double %986, %987
  %989 = fsub double -0.000000e+00, %988
  %990 = add i32 %790, -1
  %991 = sext i32 %990 to i64
  %992 = mul i64 %991, 25
  %993 = add i64 %992, 16
  %994 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %993
  %995 = load double* %994, align 8
  %996 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %997 = fmul double %995, %996
  %998 = fsub double %989, %997
  %999 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %980
  store double %998, double* %999, align 8
  %1000 = sext i32 %790 to i64
  %1001 = mul i64 %1000, 75
  %1002 = add i64 %1001, 21
  %1003 = add i32 %790, -1
  %1004 = sext i32 %1003 to i64
  %1005 = mul i64 %1004, 25
  %1006 = add i64 %1005, 21
  %1007 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1006
  %1008 = load double* %1007, align 8
  %1009 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1010 = fmul double %1008, %1009
  %1011 = fsub double -0.000000e+00, %1010
  %1012 = add i32 %790, -1
  %1013 = sext i32 %1012 to i64
  %1014 = mul i64 %1013, 25
  %1015 = add i64 %1014, 21
  %1016 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1015
  %1017 = load double* %1016, align 8
  %1018 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1019 = fmul double %1017, %1018
  %1020 = fsub double %1011, %1019
  %1021 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1002
  store double %1020, double* %1021, align 8
  %1022 = sext i32 %790 to i64
  %1023 = mul i64 %1022, 75
  %1024 = add i64 %1023, 2
  %1025 = add i32 %790, -1
  %1026 = sext i32 %1025 to i64
  %1027 = mul i64 %1026, 25
  %1028 = add i64 %1027, 2
  %1029 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1028
  %1030 = load double* %1029, align 8
  %1031 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1032 = fmul double %1030, %1031
  %1033 = fsub double -0.000000e+00, %1032
  %1034 = add i32 %790, -1
  %1035 = sext i32 %1034 to i64
  %1036 = mul i64 %1035, 25
  %1037 = add i64 %1036, 2
  %1038 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1037
  %1039 = load double* %1038, align 8
  %1040 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1041 = fmul double %1039, %1040
  %1042 = fsub double %1033, %1041
  %1043 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1024
  store double %1042, double* %1043, align 8
  %1044 = sext i32 %790 to i64
  %1045 = mul i64 %1044, 75
  %1046 = add i64 %1045, 7
  %1047 = add i32 %790, -1
  %1048 = sext i32 %1047 to i64
  %1049 = mul i64 %1048, 25
  %1050 = add i64 %1049, 7
  %1051 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1050
  %1052 = load double* %1051, align 8
  %1053 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1054 = fmul double %1052, %1053
  %1055 = fsub double -0.000000e+00, %1054
  %1056 = add i32 %790, -1
  %1057 = sext i32 %1056 to i64
  %1058 = mul i64 %1057, 25
  %1059 = add i64 %1058, 7
  %1060 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1059
  %1061 = load double* %1060, align 8
  %1062 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1063 = fmul double %1061, %1062
  %1064 = fsub double %1055, %1063
  %1065 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1046
  store double %1064, double* %1065, align 8
  %1066 = sext i32 %790 to i64
  %1067 = mul i64 %1066, 75
  %1068 = add i64 %1067, 12
  %1069 = add i32 %790, -1
  %1070 = sext i32 %1069 to i64
  %1071 = mul i64 %1070, 25
  %1072 = add i64 %1071, 12
  %1073 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1072
  %1074 = load double* %1073, align 8
  %1075 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1076 = fmul double %1074, %1075
  %1077 = fsub double -0.000000e+00, %1076
  %1078 = add i32 %790, -1
  %1079 = sext i32 %1078 to i64
  %1080 = mul i64 %1079, 25
  %1081 = add i64 %1080, 12
  %1082 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1081
  %1083 = load double* %1082, align 8
  %1084 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1085 = fmul double %1083, %1084
  %1086 = fsub double %1077, %1085
  %1087 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1088 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 11), align 8
  %1089 = fmul double %1087, %1088
  %1090 = fsub double %1086, %1089
  %1091 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1068
  store double %1090, double* %1091, align 8
  %1092 = sext i32 %790 to i64
  %1093 = mul i64 %1092, 75
  %1094 = add i64 %1093, 17
  %1095 = add i32 %790, -1
  %1096 = sext i32 %1095 to i64
  %1097 = mul i64 %1096, 25
  %1098 = add i64 %1097, 17
  %1099 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1098
  %1100 = load double* %1099, align 8
  %1101 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1102 = fmul double %1100, %1101
  %1103 = fsub double -0.000000e+00, %1102
  %1104 = add i32 %790, -1
  %1105 = sext i32 %1104 to i64
  %1106 = mul i64 %1105, 25
  %1107 = add i64 %1106, 17
  %1108 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1107
  %1109 = load double* %1108, align 8
  %1110 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1111 = fmul double %1109, %1110
  %1112 = fsub double %1103, %1111
  %1113 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1094
  store double %1112, double* %1113, align 8
  %1114 = sext i32 %790 to i64
  %1115 = mul i64 %1114, 75
  %1116 = add i64 %1115, 22
  %1117 = add i32 %790, -1
  %1118 = sext i32 %1117 to i64
  %1119 = mul i64 %1118, 25
  %1120 = add i64 %1119, 22
  %1121 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1120
  %1122 = load double* %1121, align 8
  %1123 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1124 = fmul double %1122, %1123
  %1125 = fsub double -0.000000e+00, %1124
  %1126 = add i32 %790, -1
  %1127 = sext i32 %1126 to i64
  %1128 = mul i64 %1127, 25
  %1129 = add i64 %1128, 22
  %1130 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1129
  %1131 = load double* %1130, align 8
  %1132 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1133 = fmul double %1131, %1132
  %1134 = fsub double %1125, %1133
  %1135 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1116
  store double %1134, double* %1135, align 8
  %1136 = sext i32 %790 to i64
  %1137 = mul i64 %1136, 75
  %1138 = add i64 %1137, 3
  %1139 = add i32 %790, -1
  %1140 = sext i32 %1139 to i64
  %1141 = mul i64 %1140, 25
  %1142 = add i64 %1141, 3
  %1143 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1142
  %1144 = load double* %1143, align 8
  %1145 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1146 = fmul double %1144, %1145
  %1147 = fsub double -0.000000e+00, %1146
  %1148 = add i32 %790, -1
  %1149 = sext i32 %1148 to i64
  %1150 = mul i64 %1149, 25
  %1151 = add i64 %1150, 3
  %1152 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1151
  %1153 = load double* %1152, align 8
  %1154 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1155 = fmul double %1153, %1154
  %1156 = fsub double %1147, %1155
  %1157 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1138
  store double %1156, double* %1157, align 8
  %1158 = sext i32 %790 to i64
  %1159 = mul i64 %1158, 75
  %1160 = add i64 %1159, 8
  %1161 = add i32 %790, -1
  %1162 = sext i32 %1161 to i64
  %1163 = mul i64 %1162, 25
  %1164 = add i64 %1163, 8
  %1165 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1164
  %1166 = load double* %1165, align 8
  %1167 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1168 = fmul double %1166, %1167
  %1169 = fsub double -0.000000e+00, %1168
  %1170 = add i32 %790, -1
  %1171 = sext i32 %1170 to i64
  %1172 = mul i64 %1171, 25
  %1173 = add i64 %1172, 8
  %1174 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1173
  %1175 = load double* %1174, align 8
  %1176 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1177 = fmul double %1175, %1176
  %1178 = fsub double %1169, %1177
  %1179 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1160
  store double %1178, double* %1179, align 8
  %1180 = sext i32 %790 to i64
  %1181 = mul i64 %1180, 75
  %1182 = add i64 %1181, 13
  %1183 = add i32 %790, -1
  %1184 = sext i32 %1183 to i64
  %1185 = mul i64 %1184, 25
  %1186 = add i64 %1185, 13
  %1187 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1186
  %1188 = load double* %1187, align 8
  %1189 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1190 = fmul double %1188, %1189
  %1191 = fsub double -0.000000e+00, %1190
  %1192 = add i32 %790, -1
  %1193 = sext i32 %1192 to i64
  %1194 = mul i64 %1193, 25
  %1195 = add i64 %1194, 13
  %1196 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1195
  %1197 = load double* %1196, align 8
  %1198 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1199 = fmul double %1197, %1198
  %1200 = fsub double %1191, %1199
  %1201 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1182
  store double %1200, double* %1201, align 8
  %1202 = sext i32 %790 to i64
  %1203 = mul i64 %1202, 75
  %1204 = add i64 %1203, 18
  %1205 = add i32 %790, -1
  %1206 = sext i32 %1205 to i64
  %1207 = mul i64 %1206, 25
  %1208 = add i64 %1207, 18
  %1209 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1208
  %1210 = load double* %1209, align 8
  %1211 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1212 = fmul double %1210, %1211
  %1213 = fsub double -0.000000e+00, %1212
  %1214 = add i32 %790, -1
  %1215 = sext i32 %1214 to i64
  %1216 = mul i64 %1215, 25
  %1217 = add i64 %1216, 18
  %1218 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1217
  %1219 = load double* %1218, align 8
  %1220 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1221 = fmul double %1219, %1220
  %1222 = fsub double %1213, %1221
  %1223 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1224 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 12), align 16
  %1225 = fmul double %1223, %1224
  %1226 = fsub double %1222, %1225
  %1227 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1204
  store double %1226, double* %1227, align 8
  %1228 = sext i32 %790 to i64
  %1229 = mul i64 %1228, 75
  %1230 = add i64 %1229, 23
  %1231 = add i32 %790, -1
  %1232 = sext i32 %1231 to i64
  %1233 = mul i64 %1232, 25
  %1234 = add i64 %1233, 23
  %1235 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1234
  %1236 = load double* %1235, align 8
  %1237 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1238 = fmul double %1236, %1237
  %1239 = fsub double -0.000000e+00, %1238
  %1240 = add i32 %790, -1
  %1241 = sext i32 %1240 to i64
  %1242 = mul i64 %1241, 25
  %1243 = add i64 %1242, 23
  %1244 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1243
  %1245 = load double* %1244, align 8
  %1246 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1247 = fmul double %1245, %1246
  %1248 = fsub double %1239, %1247
  %1249 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1230
  store double %1248, double* %1249, align 8
  %1250 = sext i32 %790 to i64
  %1251 = mul i64 %1250, 75
  %1252 = add i64 %1251, 4
  %1253 = add i32 %790, -1
  %1254 = sext i32 %1253 to i64
  %1255 = mul i64 %1254, 25
  %1256 = add i64 %1255, 4
  %1257 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1256
  %1258 = load double* %1257, align 8
  %1259 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1260 = fmul double %1258, %1259
  %1261 = fsub double -0.000000e+00, %1260
  %1262 = add i32 %790, -1
  %1263 = sext i32 %1262 to i64
  %1264 = mul i64 %1263, 25
  %1265 = add i64 %1264, 4
  %1266 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1265
  %1267 = load double* %1266, align 8
  %1268 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1269 = fmul double %1267, %1268
  %1270 = fsub double %1261, %1269
  %1271 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1252
  store double %1270, double* %1271, align 8
  %1272 = sext i32 %790 to i64
  %1273 = mul i64 %1272, 75
  %1274 = add i64 %1273, 9
  %1275 = add i32 %790, -1
  %1276 = sext i32 %1275 to i64
  %1277 = mul i64 %1276, 25
  %1278 = add i64 %1277, 9
  %1279 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1278
  %1280 = load double* %1279, align 8
  %1281 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1282 = fmul double %1280, %1281
  %1283 = fsub double -0.000000e+00, %1282
  %1284 = add i32 %790, -1
  %1285 = sext i32 %1284 to i64
  %1286 = mul i64 %1285, 25
  %1287 = add i64 %1286, 9
  %1288 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1287
  %1289 = load double* %1288, align 8
  %1290 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1291 = fmul double %1289, %1290
  %1292 = fsub double %1283, %1291
  %1293 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1274
  store double %1292, double* %1293, align 8
  %1294 = sext i32 %790 to i64
  %1295 = mul i64 %1294, 75
  %1296 = add i64 %1295, 14
  %1297 = add i32 %790, -1
  %1298 = sext i32 %1297 to i64
  %1299 = mul i64 %1298, 25
  %1300 = add i64 %1299, 14
  %1301 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1300
  %1302 = load double* %1301, align 8
  %1303 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1304 = fmul double %1302, %1303
  %1305 = fsub double -0.000000e+00, %1304
  %1306 = add i32 %790, -1
  %1307 = sext i32 %1306 to i64
  %1308 = mul i64 %1307, 25
  %1309 = add i64 %1308, 14
  %1310 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1309
  %1311 = load double* %1310, align 8
  %1312 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1313 = fmul double %1311, %1312
  %1314 = fsub double %1305, %1313
  %1315 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1296
  store double %1314, double* %1315, align 8
  %1316 = sext i32 %790 to i64
  %1317 = mul i64 %1316, 75
  %1318 = add i64 %1317, 19
  %1319 = add i32 %790, -1
  %1320 = sext i32 %1319 to i64
  %1321 = mul i64 %1320, 25
  %1322 = add i64 %1321, 19
  %1323 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1322
  %1324 = load double* %1323, align 8
  %1325 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1326 = fmul double %1324, %1325
  %1327 = fsub double -0.000000e+00, %1326
  %1328 = add i32 %790, -1
  %1329 = sext i32 %1328 to i64
  %1330 = mul i64 %1329, 25
  %1331 = add i64 %1330, 19
  %1332 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1331
  %1333 = load double* %1332, align 8
  %1334 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1335 = fmul double %1333, %1334
  %1336 = fsub double %1327, %1335
  %1337 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1318
  store double %1336, double* %1337, align 8
  %1338 = sext i32 %790 to i64
  %1339 = mul i64 %1338, 75
  %1340 = add i64 %1339, 24
  %1341 = add i32 %790, -1
  %1342 = sext i32 %1341 to i64
  %1343 = mul i64 %1342, 25
  %1344 = add i64 %1343, 24
  %1345 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1344
  %1346 = load double* %1345, align 8
  %1347 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1348 = fmul double %1346, %1347
  %1349 = fsub double -0.000000e+00, %1348
  %1350 = add i32 %790, -1
  %1351 = sext i32 %1350 to i64
  %1352 = mul i64 %1351, 25
  %1353 = add i64 %1352, 24
  %1354 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1353
  %1355 = load double* %1354, align 8
  %1356 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1357 = fmul double %1355, %1356
  %1358 = fsub double %1349, %1357
  %1359 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1360 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 13), align 8
  %1361 = fmul double %1359, %1360
  %1362 = fsub double %1358, %1361
  %1363 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1340
  store double %1362, double* %1363, align 8
  %1364 = sext i32 %790 to i64
  %1365 = mul i64 %1364, 75
  %1366 = add i64 %1365, 25
  %1367 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1368 = fmul double %1367, 2.000000e+00
  %1369 = sext i32 %790 to i64
  %1370 = mul i64 %1369, 25
  %1371 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1370
  %1372 = load double* %1371, align 8
  %1373 = fmul double %1368, %1372
  %1374 = fadd double %1373, 1.000000e+00
  %1375 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1376 = fmul double %1375, 2.000000e+00
  %1377 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 9), align 8
  %1378 = fmul double %1376, %1377
  %1379 = fadd double %1374, %1378
  %1380 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1366
  store double %1379, double* %1380, align 8
  %1381 = sext i32 %790 to i64
  %1382 = mul i64 %1381, 75
  %1383 = add i64 %1382, 30
  %1384 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1385 = fmul double %1384, 2.000000e+00
  %1386 = sext i32 %790 to i64
  %1387 = mul i64 %1386, 25
  %1388 = add i64 %1387, 5
  %1389 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1388
  %1390 = load double* %1389, align 8
  %1391 = fmul double %1385, %1390
  %1392 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1383
  store double %1391, double* %1392, align 8
  %1393 = sext i32 %790 to i64
  %1394 = mul i64 %1393, 75
  %1395 = add i64 %1394, 35
  %1396 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1397 = fmul double %1396, 2.000000e+00
  %1398 = sext i32 %790 to i64
  %1399 = mul i64 %1398, 25
  %1400 = add i64 %1399, 10
  %1401 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1400
  %1402 = load double* %1401, align 8
  %1403 = fmul double %1397, %1402
  %1404 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1395
  store double %1403, double* %1404, align 8
  %1405 = sext i32 %790 to i64
  %1406 = mul i64 %1405, 75
  %1407 = add i64 %1406, 40
  %1408 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1409 = fmul double %1408, 2.000000e+00
  %1410 = sext i32 %790 to i64
  %1411 = mul i64 %1410, 25
  %1412 = add i64 %1411, 15
  %1413 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1412
  %1414 = load double* %1413, align 8
  %1415 = fmul double %1409, %1414
  %1416 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1407
  store double %1415, double* %1416, align 8
  %1417 = sext i32 %790 to i64
  %1418 = mul i64 %1417, 75
  %1419 = add i64 %1418, 45
  %1420 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1421 = fmul double %1420, 2.000000e+00
  %1422 = sext i32 %790 to i64
  %1423 = mul i64 %1422, 25
  %1424 = add i64 %1423, 20
  %1425 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1424
  %1426 = load double* %1425, align 8
  %1427 = fmul double %1421, %1426
  %1428 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1419
  store double %1427, double* %1428, align 8
  %1429 = sext i32 %790 to i64
  %1430 = mul i64 %1429, 75
  %1431 = add i64 %1430, 26
  %1432 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1433 = fmul double %1432, 2.000000e+00
  %1434 = sext i32 %790 to i64
  %1435 = mul i64 %1434, 25
  %1436 = add i64 %1435, 1
  %1437 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1436
  %1438 = load double* %1437, align 8
  %1439 = fmul double %1433, %1438
  %1440 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1431
  store double %1439, double* %1440, align 8
  %1441 = sext i32 %790 to i64
  %1442 = mul i64 %1441, 75
  %1443 = add i64 %1442, 31
  %1444 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1445 = fmul double %1444, 2.000000e+00
  %1446 = sext i32 %790 to i64
  %1447 = mul i64 %1446, 25
  %1448 = add i64 %1447, 6
  %1449 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1448
  %1450 = load double* %1449, align 8
  %1451 = fmul double %1445, %1450
  %1452 = fadd double %1451, 1.000000e+00
  %1453 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1454 = fmul double %1453, 2.000000e+00
  %1455 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 10), align 16
  %1456 = fmul double %1454, %1455
  %1457 = fadd double %1452, %1456
  %1458 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1443
  store double %1457, double* %1458, align 8
  %1459 = sext i32 %790 to i64
  %1460 = mul i64 %1459, 75
  %1461 = add i64 %1460, 36
  %1462 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1463 = fmul double %1462, 2.000000e+00
  %1464 = sext i32 %790 to i64
  %1465 = mul i64 %1464, 25
  %1466 = add i64 %1465, 11
  %1467 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1466
  %1468 = load double* %1467, align 8
  %1469 = fmul double %1463, %1468
  %1470 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1461
  store double %1469, double* %1470, align 8
  %1471 = sext i32 %790 to i64
  %1472 = mul i64 %1471, 75
  %1473 = add i64 %1472, 41
  %1474 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1475 = fmul double %1474, 2.000000e+00
  %1476 = sext i32 %790 to i64
  %1477 = mul i64 %1476, 25
  %1478 = add i64 %1477, 16
  %1479 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1478
  %1480 = load double* %1479, align 8
  %1481 = fmul double %1475, %1480
  %1482 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1473
  store double %1481, double* %1482, align 8
  %1483 = sext i32 %790 to i64
  %1484 = mul i64 %1483, 75
  %1485 = add i64 %1484, 46
  %1486 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1487 = fmul double %1486, 2.000000e+00
  %1488 = sext i32 %790 to i64
  %1489 = mul i64 %1488, 25
  %1490 = add i64 %1489, 21
  %1491 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1490
  %1492 = load double* %1491, align 8
  %1493 = fmul double %1487, %1492
  %1494 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1485
  store double %1493, double* %1494, align 8
  %1495 = sext i32 %790 to i64
  %1496 = mul i64 %1495, 75
  %1497 = add i64 %1496, 27
  %1498 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1499 = fmul double %1498, 2.000000e+00
  %1500 = sext i32 %790 to i64
  %1501 = mul i64 %1500, 25
  %1502 = add i64 %1501, 2
  %1503 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1502
  %1504 = load double* %1503, align 8
  %1505 = fmul double %1499, %1504
  %1506 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1497
  store double %1505, double* %1506, align 8
  %1507 = sext i32 %790 to i64
  %1508 = mul i64 %1507, 75
  %1509 = add i64 %1508, 32
  %1510 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1511 = fmul double %1510, 2.000000e+00
  %1512 = sext i32 %790 to i64
  %1513 = mul i64 %1512, 25
  %1514 = add i64 %1513, 7
  %1515 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1514
  %1516 = load double* %1515, align 8
  %1517 = fmul double %1511, %1516
  %1518 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1509
  store double %1517, double* %1518, align 8
  %1519 = sext i32 %790 to i64
  %1520 = mul i64 %1519, 75
  %1521 = add i64 %1520, 37
  %1522 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1523 = fmul double %1522, 2.000000e+00
  %1524 = sext i32 %790 to i64
  %1525 = mul i64 %1524, 25
  %1526 = add i64 %1525, 12
  %1527 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1526
  %1528 = load double* %1527, align 8
  %1529 = fmul double %1523, %1528
  %1530 = fadd double %1529, 1.000000e+00
  %1531 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1532 = fmul double %1531, 2.000000e+00
  %1533 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 11), align 8
  %1534 = fmul double %1532, %1533
  %1535 = fadd double %1530, %1534
  %1536 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1521
  store double %1535, double* %1536, align 8
  %1537 = sext i32 %790 to i64
  %1538 = mul i64 %1537, 75
  %1539 = add i64 %1538, 42
  %1540 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1541 = fmul double %1540, 2.000000e+00
  %1542 = sext i32 %790 to i64
  %1543 = mul i64 %1542, 25
  %1544 = add i64 %1543, 17
  %1545 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1544
  %1546 = load double* %1545, align 8
  %1547 = fmul double %1541, %1546
  %1548 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1539
  store double %1547, double* %1548, align 8
  %1549 = sext i32 %790 to i64
  %1550 = mul i64 %1549, 75
  %1551 = add i64 %1550, 47
  %1552 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1553 = fmul double %1552, 2.000000e+00
  %1554 = sext i32 %790 to i64
  %1555 = mul i64 %1554, 25
  %1556 = add i64 %1555, 22
  %1557 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1556
  %1558 = load double* %1557, align 8
  %1559 = fmul double %1553, %1558
  %1560 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1551
  store double %1559, double* %1560, align 8
  %1561 = sext i32 %790 to i64
  %1562 = mul i64 %1561, 75
  %1563 = add i64 %1562, 28
  %1564 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1565 = fmul double %1564, 2.000000e+00
  %1566 = sext i32 %790 to i64
  %1567 = mul i64 %1566, 25
  %1568 = add i64 %1567, 3
  %1569 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1568
  %1570 = load double* %1569, align 8
  %1571 = fmul double %1565, %1570
  %1572 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1563
  store double %1571, double* %1572, align 8
  %1573 = sext i32 %790 to i64
  %1574 = mul i64 %1573, 75
  %1575 = add i64 %1574, 33
  %1576 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1577 = fmul double %1576, 2.000000e+00
  %1578 = sext i32 %790 to i64
  %1579 = mul i64 %1578, 25
  %1580 = add i64 %1579, 8
  %1581 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1580
  %1582 = load double* %1581, align 8
  %1583 = fmul double %1577, %1582
  %1584 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1575
  store double %1583, double* %1584, align 8
  %1585 = sext i32 %790 to i64
  %1586 = mul i64 %1585, 75
  %1587 = add i64 %1586, 38
  %1588 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1589 = fmul double %1588, 2.000000e+00
  %1590 = sext i32 %790 to i64
  %1591 = mul i64 %1590, 25
  %1592 = add i64 %1591, 13
  %1593 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1592
  %1594 = load double* %1593, align 8
  %1595 = fmul double %1589, %1594
  %1596 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1587
  store double %1595, double* %1596, align 8
  %1597 = sext i32 %790 to i64
  %1598 = mul i64 %1597, 75
  %1599 = add i64 %1598, 43
  %1600 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1601 = fmul double %1600, 2.000000e+00
  %1602 = sext i32 %790 to i64
  %1603 = mul i64 %1602, 25
  %1604 = add i64 %1603, 18
  %1605 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1604
  %1606 = load double* %1605, align 8
  %1607 = fmul double %1601, %1606
  %1608 = fadd double %1607, 1.000000e+00
  %1609 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1610 = fmul double %1609, 2.000000e+00
  %1611 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 12), align 16
  %1612 = fmul double %1610, %1611
  %1613 = fadd double %1608, %1612
  %1614 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1599
  store double %1613, double* %1614, align 8
  %1615 = sext i32 %790 to i64
  %1616 = mul i64 %1615, 75
  %1617 = add i64 %1616, 48
  %1618 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1619 = fmul double %1618, 2.000000e+00
  %1620 = sext i32 %790 to i64
  %1621 = mul i64 %1620, 25
  %1622 = add i64 %1621, 23
  %1623 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1622
  %1624 = load double* %1623, align 8
  %1625 = fmul double %1619, %1624
  %1626 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1617
  store double %1625, double* %1626, align 8
  %1627 = sext i32 %790 to i64
  %1628 = mul i64 %1627, 75
  %1629 = add i64 %1628, 29
  %1630 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1631 = fmul double %1630, 2.000000e+00
  %1632 = sext i32 %790 to i64
  %1633 = mul i64 %1632, 25
  %1634 = add i64 %1633, 4
  %1635 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1634
  %1636 = load double* %1635, align 8
  %1637 = fmul double %1631, %1636
  %1638 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1629
  store double %1637, double* %1638, align 8
  %1639 = sext i32 %790 to i64
  %1640 = mul i64 %1639, 75
  %1641 = add i64 %1640, 34
  %1642 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1643 = fmul double %1642, 2.000000e+00
  %1644 = sext i32 %790 to i64
  %1645 = mul i64 %1644, 25
  %1646 = add i64 %1645, 9
  %1647 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1646
  %1648 = load double* %1647, align 8
  %1649 = fmul double %1643, %1648
  %1650 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1641
  store double %1649, double* %1650, align 8
  %1651 = sext i32 %790 to i64
  %1652 = mul i64 %1651, 75
  %1653 = add i64 %1652, 39
  %1654 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1655 = fmul double %1654, 2.000000e+00
  %1656 = sext i32 %790 to i64
  %1657 = mul i64 %1656, 25
  %1658 = add i64 %1657, 14
  %1659 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1658
  %1660 = load double* %1659, align 8
  %1661 = fmul double %1655, %1660
  %1662 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1653
  store double %1661, double* %1662, align 8
  %1663 = sext i32 %790 to i64
  %1664 = mul i64 %1663, 75
  %1665 = add i64 %1664, 44
  %1666 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1667 = fmul double %1666, 2.000000e+00
  %1668 = sext i32 %790 to i64
  %1669 = mul i64 %1668, 25
  %1670 = add i64 %1669, 19
  %1671 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1670
  %1672 = load double* %1671, align 8
  %1673 = fmul double %1667, %1672
  %1674 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1665
  store double %1673, double* %1674, align 8
  %1675 = sext i32 %790 to i64
  %1676 = mul i64 %1675, 75
  %1677 = add i64 %1676, 49
  %1678 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1679 = fmul double %1678, 2.000000e+00
  %1680 = sext i32 %790 to i64
  %1681 = mul i64 %1680, 25
  %1682 = add i64 %1681, 24
  %1683 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1682
  %1684 = load double* %1683, align 8
  %1685 = fmul double %1679, %1684
  %1686 = fadd double %1685, 1.000000e+00
  %1687 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1688 = fmul double %1687, 2.000000e+00
  %1689 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 13), align 8
  %1690 = fmul double %1688, %1689
  %1691 = fadd double %1686, %1690
  %1692 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1677
  store double %1691, double* %1692, align 8
  %1693 = sext i32 %790 to i64
  %1694 = mul i64 %1693, 75
  %1695 = add i64 %1694, 50
  %1696 = add i32 %790, 1
  %1697 = sext i32 %1696 to i64
  %1698 = mul i64 %1697, 25
  %1699 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1698
  %1700 = load double* %1699, align 8
  %1701 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1702 = fmul double %1700, %1701
  %1703 = add i32 %790, 1
  %1704 = sext i32 %1703 to i64
  %1705 = mul i64 %1704, 25
  %1706 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1705
  %1707 = load double* %1706, align 8
  %1708 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1709 = fmul double %1707, %1708
  %1710 = fsub double %1702, %1709
  %1711 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1712 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 9), align 8
  %1713 = fmul double %1711, %1712
  %1714 = fsub double %1710, %1713
  %1715 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1695
  store double %1714, double* %1715, align 8
  %1716 = sext i32 %790 to i64
  %1717 = mul i64 %1716, 75
  %1718 = add i64 %1717, 55
  %1719 = add i32 %790, 1
  %1720 = sext i32 %1719 to i64
  %1721 = mul i64 %1720, 25
  %1722 = add i64 %1721, 5
  %1723 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1722
  %1724 = load double* %1723, align 8
  %1725 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1726 = fmul double %1724, %1725
  %1727 = add i32 %790, 1
  %1728 = sext i32 %1727 to i64
  %1729 = mul i64 %1728, 25
  %1730 = add i64 %1729, 5
  %1731 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1730
  %1732 = load double* %1731, align 8
  %1733 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1734 = fmul double %1732, %1733
  %1735 = fsub double %1726, %1734
  %1736 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1718
  store double %1735, double* %1736, align 8
  %1737 = sext i32 %790 to i64
  %1738 = mul i64 %1737, 75
  %1739 = add i64 %1738, 60
  %1740 = add i32 %790, 1
  %1741 = sext i32 %1740 to i64
  %1742 = mul i64 %1741, 25
  %1743 = add i64 %1742, 10
  %1744 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1743
  %1745 = load double* %1744, align 8
  %1746 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1747 = fmul double %1745, %1746
  %1748 = add i32 %790, 1
  %1749 = sext i32 %1748 to i64
  %1750 = mul i64 %1749, 25
  %1751 = add i64 %1750, 10
  %1752 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1751
  %1753 = load double* %1752, align 8
  %1754 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1755 = fmul double %1753, %1754
  %1756 = fsub double %1747, %1755
  %1757 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1739
  store double %1756, double* %1757, align 8
  %1758 = sext i32 %790 to i64
  %1759 = mul i64 %1758, 75
  %1760 = add i64 %1759, 65
  %1761 = add i32 %790, 1
  %1762 = sext i32 %1761 to i64
  %1763 = mul i64 %1762, 25
  %1764 = add i64 %1763, 15
  %1765 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1764
  %1766 = load double* %1765, align 8
  %1767 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1768 = fmul double %1766, %1767
  %1769 = add i32 %790, 1
  %1770 = sext i32 %1769 to i64
  %1771 = mul i64 %1770, 25
  %1772 = add i64 %1771, 15
  %1773 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1772
  %1774 = load double* %1773, align 8
  %1775 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1776 = fmul double %1774, %1775
  %1777 = fsub double %1768, %1776
  %1778 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1760
  store double %1777, double* %1778, align 8
  %1779 = sext i32 %790 to i64
  %1780 = mul i64 %1779, 75
  %1781 = add i64 %1780, 70
  %1782 = add i32 %790, 1
  %1783 = sext i32 %1782 to i64
  %1784 = mul i64 %1783, 25
  %1785 = add i64 %1784, 20
  %1786 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1785
  %1787 = load double* %1786, align 8
  %1788 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1789 = fmul double %1787, %1788
  %1790 = add i32 %790, 1
  %1791 = sext i32 %1790 to i64
  %1792 = mul i64 %1791, 25
  %1793 = add i64 %1792, 20
  %1794 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1793
  %1795 = load double* %1794, align 8
  %1796 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1797 = fmul double %1795, %1796
  %1798 = fsub double %1789, %1797
  %1799 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1781
  store double %1798, double* %1799, align 8
  %1800 = sext i32 %790 to i64
  %1801 = mul i64 %1800, 75
  %1802 = add i64 %1801, 51
  %1803 = add i32 %790, 1
  %1804 = sext i32 %1803 to i64
  %1805 = mul i64 %1804, 25
  %1806 = add i64 %1805, 1
  %1807 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1806
  %1808 = load double* %1807, align 8
  %1809 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1810 = fmul double %1808, %1809
  %1811 = add i32 %790, 1
  %1812 = sext i32 %1811 to i64
  %1813 = mul i64 %1812, 25
  %1814 = add i64 %1813, 1
  %1815 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1814
  %1816 = load double* %1815, align 8
  %1817 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1818 = fmul double %1816, %1817
  %1819 = fsub double %1810, %1818
  %1820 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1802
  store double %1819, double* %1820, align 8
  %1821 = sext i32 %790 to i64
  %1822 = mul i64 %1821, 75
  %1823 = add i64 %1822, 56
  %1824 = add i32 %790, 1
  %1825 = sext i32 %1824 to i64
  %1826 = mul i64 %1825, 25
  %1827 = add i64 %1826, 6
  %1828 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1827
  %1829 = load double* %1828, align 8
  %1830 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1831 = fmul double %1829, %1830
  %1832 = add i32 %790, 1
  %1833 = sext i32 %1832 to i64
  %1834 = mul i64 %1833, 25
  %1835 = add i64 %1834, 6
  %1836 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1835
  %1837 = load double* %1836, align 8
  %1838 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1839 = fmul double %1837, %1838
  %1840 = fsub double %1831, %1839
  %1841 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1842 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 10), align 16
  %1843 = fmul double %1841, %1842
  %1844 = fsub double %1840, %1843
  %1845 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1823
  store double %1844, double* %1845, align 8
  %1846 = sext i32 %790 to i64
  %1847 = mul i64 %1846, 75
  %1848 = add i64 %1847, 61
  %1849 = add i32 %790, 1
  %1850 = sext i32 %1849 to i64
  %1851 = mul i64 %1850, 25
  %1852 = add i64 %1851, 11
  %1853 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1852
  %1854 = load double* %1853, align 8
  %1855 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1856 = fmul double %1854, %1855
  %1857 = add i32 %790, 1
  %1858 = sext i32 %1857 to i64
  %1859 = mul i64 %1858, 25
  %1860 = add i64 %1859, 11
  %1861 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1860
  %1862 = load double* %1861, align 8
  %1863 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1864 = fmul double %1862, %1863
  %1865 = fsub double %1856, %1864
  %1866 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1848
  store double %1865, double* %1866, align 8
  %1867 = sext i32 %790 to i64
  %1868 = mul i64 %1867, 75
  %1869 = add i64 %1868, 66
  %1870 = add i32 %790, 1
  %1871 = sext i32 %1870 to i64
  %1872 = mul i64 %1871, 25
  %1873 = add i64 %1872, 16
  %1874 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1873
  %1875 = load double* %1874, align 8
  %1876 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1877 = fmul double %1875, %1876
  %1878 = add i32 %790, 1
  %1879 = sext i32 %1878 to i64
  %1880 = mul i64 %1879, 25
  %1881 = add i64 %1880, 16
  %1882 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1881
  %1883 = load double* %1882, align 8
  %1884 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1885 = fmul double %1883, %1884
  %1886 = fsub double %1877, %1885
  %1887 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1869
  store double %1886, double* %1887, align 8
  %1888 = sext i32 %790 to i64
  %1889 = mul i64 %1888, 75
  %1890 = add i64 %1889, 71
  %1891 = add i32 %790, 1
  %1892 = sext i32 %1891 to i64
  %1893 = mul i64 %1892, 25
  %1894 = add i64 %1893, 21
  %1895 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1894
  %1896 = load double* %1895, align 8
  %1897 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1898 = fmul double %1896, %1897
  %1899 = add i32 %790, 1
  %1900 = sext i32 %1899 to i64
  %1901 = mul i64 %1900, 25
  %1902 = add i64 %1901, 21
  %1903 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1902
  %1904 = load double* %1903, align 8
  %1905 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1906 = fmul double %1904, %1905
  %1907 = fsub double %1898, %1906
  %1908 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1890
  store double %1907, double* %1908, align 8
  %1909 = sext i32 %790 to i64
  %1910 = mul i64 %1909, 75
  %1911 = add i64 %1910, 52
  %1912 = add i32 %790, 1
  %1913 = sext i32 %1912 to i64
  %1914 = mul i64 %1913, 25
  %1915 = add i64 %1914, 2
  %1916 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1915
  %1917 = load double* %1916, align 8
  %1918 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1919 = fmul double %1917, %1918
  %1920 = add i32 %790, 1
  %1921 = sext i32 %1920 to i64
  %1922 = mul i64 %1921, 25
  %1923 = add i64 %1922, 2
  %1924 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1923
  %1925 = load double* %1924, align 8
  %1926 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1927 = fmul double %1925, %1926
  %1928 = fsub double %1919, %1927
  %1929 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1911
  store double %1928, double* %1929, align 8
  %1930 = sext i32 %790 to i64
  %1931 = mul i64 %1930, 75
  %1932 = add i64 %1931, 57
  %1933 = add i32 %790, 1
  %1934 = sext i32 %1933 to i64
  %1935 = mul i64 %1934, 25
  %1936 = add i64 %1935, 7
  %1937 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1936
  %1938 = load double* %1937, align 8
  %1939 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1940 = fmul double %1938, %1939
  %1941 = add i32 %790, 1
  %1942 = sext i32 %1941 to i64
  %1943 = mul i64 %1942, 25
  %1944 = add i64 %1943, 7
  %1945 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1944
  %1946 = load double* %1945, align 8
  %1947 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1948 = fmul double %1946, %1947
  %1949 = fsub double %1940, %1948
  %1950 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1932
  store double %1949, double* %1950, align 8
  %1951 = sext i32 %790 to i64
  %1952 = mul i64 %1951, 75
  %1953 = add i64 %1952, 62
  %1954 = add i32 %790, 1
  %1955 = sext i32 %1954 to i64
  %1956 = mul i64 %1955, 25
  %1957 = add i64 %1956, 12
  %1958 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1957
  %1959 = load double* %1958, align 8
  %1960 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1961 = fmul double %1959, %1960
  %1962 = add i32 %790, 1
  %1963 = sext i32 %1962 to i64
  %1964 = mul i64 %1963, 25
  %1965 = add i64 %1964, 12
  %1966 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1965
  %1967 = load double* %1966, align 8
  %1968 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1969 = fmul double %1967, %1968
  %1970 = fsub double %1961, %1969
  %1971 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1972 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 11), align 8
  %1973 = fmul double %1971, %1972
  %1974 = fsub double %1970, %1973
  %1975 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1953
  store double %1974, double* %1975, align 8
  %1976 = sext i32 %790 to i64
  %1977 = mul i64 %1976, 75
  %1978 = add i64 %1977, 67
  %1979 = add i32 %790, 1
  %1980 = sext i32 %1979 to i64
  %1981 = mul i64 %1980, 25
  %1982 = add i64 %1981, 17
  %1983 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %1982
  %1984 = load double* %1983, align 8
  %1985 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %1986 = fmul double %1984, %1985
  %1987 = add i32 %790, 1
  %1988 = sext i32 %1987 to i64
  %1989 = mul i64 %1988, 25
  %1990 = add i64 %1989, 17
  %1991 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %1990
  %1992 = load double* %1991, align 8
  %1993 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %1994 = fmul double %1992, %1993
  %1995 = fsub double %1986, %1994
  %1996 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1978
  store double %1995, double* %1996, align 8
  %1997 = sext i32 %790 to i64
  %1998 = mul i64 %1997, 75
  %1999 = add i64 %1998, 72
  %2000 = add i32 %790, 1
  %2001 = sext i32 %2000 to i64
  %2002 = mul i64 %2001, 25
  %2003 = add i64 %2002, 22
  %2004 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2003
  %2005 = load double* %2004, align 8
  %2006 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2007 = fmul double %2005, %2006
  %2008 = add i32 %790, 1
  %2009 = sext i32 %2008 to i64
  %2010 = mul i64 %2009, 25
  %2011 = add i64 %2010, 22
  %2012 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2011
  %2013 = load double* %2012, align 8
  %2014 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2015 = fmul double %2013, %2014
  %2016 = fsub double %2007, %2015
  %2017 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %1999
  store double %2016, double* %2017, align 8
  %2018 = sext i32 %790 to i64
  %2019 = mul i64 %2018, 75
  %2020 = add i64 %2019, 53
  %2021 = add i32 %790, 1
  %2022 = sext i32 %2021 to i64
  %2023 = mul i64 %2022, 25
  %2024 = add i64 %2023, 3
  %2025 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2024
  %2026 = load double* %2025, align 8
  %2027 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2028 = fmul double %2026, %2027
  %2029 = add i32 %790, 1
  %2030 = sext i32 %2029 to i64
  %2031 = mul i64 %2030, 25
  %2032 = add i64 %2031, 3
  %2033 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2032
  %2034 = load double* %2033, align 8
  %2035 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2036 = fmul double %2034, %2035
  %2037 = fsub double %2028, %2036
  %2038 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2020
  store double %2037, double* %2038, align 8
  %2039 = sext i32 %790 to i64
  %2040 = mul i64 %2039, 75
  %2041 = add i64 %2040, 58
  %2042 = add i32 %790, 1
  %2043 = sext i32 %2042 to i64
  %2044 = mul i64 %2043, 25
  %2045 = add i64 %2044, 8
  %2046 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2045
  %2047 = load double* %2046, align 8
  %2048 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2049 = fmul double %2047, %2048
  %2050 = add i32 %790, 1
  %2051 = sext i32 %2050 to i64
  %2052 = mul i64 %2051, 25
  %2053 = add i64 %2052, 8
  %2054 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2053
  %2055 = load double* %2054, align 8
  %2056 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2057 = fmul double %2055, %2056
  %2058 = fsub double %2049, %2057
  %2059 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2041
  store double %2058, double* %2059, align 8
  %2060 = sext i32 %790 to i64
  %2061 = mul i64 %2060, 75
  %2062 = add i64 %2061, 63
  %2063 = add i32 %790, 1
  %2064 = sext i32 %2063 to i64
  %2065 = mul i64 %2064, 25
  %2066 = add i64 %2065, 13
  %2067 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2066
  %2068 = load double* %2067, align 8
  %2069 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2070 = fmul double %2068, %2069
  %2071 = add i32 %790, 1
  %2072 = sext i32 %2071 to i64
  %2073 = mul i64 %2072, 25
  %2074 = add i64 %2073, 13
  %2075 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2074
  %2076 = load double* %2075, align 8
  %2077 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2078 = fmul double %2076, %2077
  %2079 = fsub double %2070, %2078
  %2080 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2062
  store double %2079, double* %2080, align 8
  %2081 = sext i32 %790 to i64
  %2082 = mul i64 %2081, 75
  %2083 = add i64 %2082, 68
  %2084 = add i32 %790, 1
  %2085 = sext i32 %2084 to i64
  %2086 = mul i64 %2085, 25
  %2087 = add i64 %2086, 18
  %2088 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2087
  %2089 = load double* %2088, align 8
  %2090 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2091 = fmul double %2089, %2090
  %2092 = add i32 %790, 1
  %2093 = sext i32 %2092 to i64
  %2094 = mul i64 %2093, 25
  %2095 = add i64 %2094, 18
  %2096 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2095
  %2097 = load double* %2096, align 8
  %2098 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2099 = fmul double %2097, %2098
  %2100 = fsub double %2091, %2099
  %2101 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2102 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 12), align 16
  %2103 = fmul double %2101, %2102
  %2104 = fsub double %2100, %2103
  %2105 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2083
  store double %2104, double* %2105, align 8
  %2106 = sext i32 %790 to i64
  %2107 = mul i64 %2106, 75
  %2108 = add i64 %2107, 73
  %2109 = add i32 %790, 1
  %2110 = sext i32 %2109 to i64
  %2111 = mul i64 %2110, 25
  %2112 = add i64 %2111, 23
  %2113 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2112
  %2114 = load double* %2113, align 8
  %2115 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2116 = fmul double %2114, %2115
  %2117 = add i32 %790, 1
  %2118 = sext i32 %2117 to i64
  %2119 = mul i64 %2118, 25
  %2120 = add i64 %2119, 23
  %2121 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2120
  %2122 = load double* %2121, align 8
  %2123 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2124 = fmul double %2122, %2123
  %2125 = fsub double %2116, %2124
  %2126 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2108
  store double %2125, double* %2126, align 8
  %2127 = sext i32 %790 to i64
  %2128 = mul i64 %2127, 75
  %2129 = add i64 %2128, 54
  %2130 = add i32 %790, 1
  %2131 = sext i32 %2130 to i64
  %2132 = mul i64 %2131, 25
  %2133 = add i64 %2132, 4
  %2134 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2133
  %2135 = load double* %2134, align 8
  %2136 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2137 = fmul double %2135, %2136
  %2138 = add i32 %790, 1
  %2139 = sext i32 %2138 to i64
  %2140 = mul i64 %2139, 25
  %2141 = add i64 %2140, 4
  %2142 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2141
  %2143 = load double* %2142, align 8
  %2144 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2145 = fmul double %2143, %2144
  %2146 = fsub double %2137, %2145
  %2147 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2129
  store double %2146, double* %2147, align 8
  %2148 = sext i32 %790 to i64
  %2149 = mul i64 %2148, 75
  %2150 = add i64 %2149, 59
  %2151 = add i32 %790, 1
  %2152 = sext i32 %2151 to i64
  %2153 = mul i64 %2152, 25
  %2154 = add i64 %2153, 9
  %2155 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2154
  %2156 = load double* %2155, align 8
  %2157 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2158 = fmul double %2156, %2157
  %2159 = add i32 %790, 1
  %2160 = sext i32 %2159 to i64
  %2161 = mul i64 %2160, 25
  %2162 = add i64 %2161, 9
  %2163 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2162
  %2164 = load double* %2163, align 8
  %2165 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2166 = fmul double %2164, %2165
  %2167 = fsub double %2158, %2166
  %2168 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2150
  store double %2167, double* %2168, align 8
  %2169 = sext i32 %790 to i64
  %2170 = mul i64 %2169, 75
  %2171 = add i64 %2170, 64
  %2172 = add i32 %790, 1
  %2173 = sext i32 %2172 to i64
  %2174 = mul i64 %2173, 25
  %2175 = add i64 %2174, 14
  %2176 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2175
  %2177 = load double* %2176, align 8
  %2178 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2179 = fmul double %2177, %2178
  %2180 = add i32 %790, 1
  %2181 = sext i32 %2180 to i64
  %2182 = mul i64 %2181, 25
  %2183 = add i64 %2182, 14
  %2184 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2183
  %2185 = load double* %2184, align 8
  %2186 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2187 = fmul double %2185, %2186
  %2188 = fsub double %2179, %2187
  %2189 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2171
  store double %2188, double* %2189, align 8
  %2190 = sext i32 %790 to i64
  %2191 = mul i64 %2190, 75
  %2192 = add i64 %2191, 69
  %2193 = add i32 %790, 1
  %2194 = sext i32 %2193 to i64
  %2195 = mul i64 %2194, 25
  %2196 = add i64 %2195, 19
  %2197 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2196
  %2198 = load double* %2197, align 8
  %2199 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2200 = fmul double %2198, %2199
  %2201 = add i32 %790, 1
  %2202 = sext i32 %2201 to i64
  %2203 = mul i64 %2202, 25
  %2204 = add i64 %2203, 19
  %2205 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2204
  %2206 = load double* %2205, align 8
  %2207 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2208 = fmul double %2206, %2207
  %2209 = fsub double %2200, %2208
  %2210 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2192
  store double %2209, double* %2210, align 8
  %2211 = sext i32 %790 to i64
  %2212 = mul i64 %2211, 75
  %2213 = add i64 %2212, 74
  %2214 = add i32 %790, 1
  %2215 = sext i32 %2214 to i64
  %2216 = mul i64 %2215, 25
  %2217 = add i64 %2216, 24
  %2218 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 0, i64 0), i64 %2217
  %2219 = load double* %2218, align 8
  %2220 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 4), align 16
  %2221 = fmul double %2219, %2220
  %2222 = add i32 %790, 1
  %2223 = sext i32 %2222 to i64
  %2224 = mul i64 %2223, 25
  %2225 = add i64 %2224, 24
  %2226 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 1, i64 0), i64 %2225
  %2227 = load double* %2226, align 8
  %2228 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2229 = fmul double %2227, %2228
  %2230 = fsub double %2221, %2229
  %2231 = load double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 3), align 8
  %2232 = load double* getelementptr inbounds (%0* @constants_, i64 0, i32 13), align 8
  %2233 = fmul double %2231, %2232
  %2234 = fsub double %2230, %2233
  %2235 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2213
  store double %2234, double* %2235, align 8
  %2236 = icmp eq i32 %790, %788
  %2237 = add i32 %790, 1
  %2238 = icmp ne i1 %2236, false
  br i1 %2238, label %"12", label %"11"

"11":                                             ; preds = %"10"
  br label %"10"

"12":                                             ; preds = %"10", %"9"
  %2239 = sext i32 %8 to i64
  %2240 = mul i64 %2239, 21125
  %2241 = sext i32 %12 to i64
  %2242 = mul i64 %2241, 325
  %2243 = add i64 %2240, %2242
  %2244 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2243
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 25), double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 50), double* %2244) #1
  %2245 = load i32* %isize, align 4
  %2246 = add i32 %2245, -1
  %2247 = icmp sle i32 1, %2246
  br i1 %2247, label %"13", label %"15"

"13":                                             ; preds = %"14", %"12"
  %2248 = phi i32 [ %2301, %"14" ], [ 1, %"12" ]
  %2249 = sext i32 %8 to i64
  %2250 = mul i64 %2249, 21125
  %2251 = sext i32 %12 to i64
  %2252 = mul i64 %2251, 325
  %2253 = add i64 %2250, %2252
  %2254 = sext i32 %2248 to i64
  %2255 = mul i64 %2254, 5
  %2256 = add i64 %2253, %2255
  %2257 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2256
  %2258 = sext i32 %8 to i64
  %2259 = mul i64 %2258, 21125
  %2260 = sext i32 %12 to i64
  %2261 = mul i64 %2260, 325
  %2262 = add i64 %2259, %2261
  %2263 = add i32 %2248, -1
  %2264 = sext i32 %2263 to i64
  %2265 = mul i64 %2264, 5
  %2266 = add i64 %2262, %2265
  %2267 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2266
  %2268 = sext i32 %2248 to i64
  %2269 = mul i64 %2268, 75
  %2270 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2269
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2270, double* %2267, double* %2257) #1
  %2271 = sext i32 %2248 to i64
  %2272 = mul i64 %2271, 75
  %2273 = add i64 %2272, 25
  %2274 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2273
  %2275 = add i32 %2248, -1
  %2276 = sext i32 %2275 to i64
  %2277 = mul i64 %2276, 75
  %2278 = add i64 %2277, 50
  %2279 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2278
  %2280 = sext i32 %2248 to i64
  %2281 = mul i64 %2280, 75
  %2282 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2281
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2282, double* %2279, double* %2274) #1
  %2283 = sext i32 %8 to i64
  %2284 = mul i64 %2283, 21125
  %2285 = sext i32 %12 to i64
  %2286 = mul i64 %2285, 325
  %2287 = add i64 %2284, %2286
  %2288 = sext i32 %2248 to i64
  %2289 = mul i64 %2288, 5
  %2290 = add i64 %2287, %2289
  %2291 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2290
  %2292 = sext i32 %2248 to i64
  %2293 = mul i64 %2292, 75
  %2294 = add i64 %2293, 50
  %2295 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2294
  %2296 = sext i32 %2248 to i64
  %2297 = mul i64 %2296, 75
  %2298 = add i64 %2297, 25
  %2299 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2298
  call void bitcast (void (...)* @binvcrhs_ to void (double*, double*, double*)*)(double* %2299, double* %2295, double* %2291) #1
  %2300 = icmp eq i32 %2248, %2246
  %2301 = add i32 %2248, 1
  %2302 = icmp ne i1 %2300, false
  br i1 %2302, label %"15", label %"14"

"14":                                             ; preds = %"13"
  br label %"13"

"15":                                             ; preds = %"13", %"12"
  %2303 = sext i32 %8 to i64
  %2304 = mul i64 %2303, 21125
  %2305 = sext i32 %12 to i64
  %2306 = mul i64 %2305, 325
  %2307 = add i64 %2304, %2306
  %2308 = load i32* %isize, align 4
  %2309 = sext i32 %2308 to i64
  %2310 = mul i64 %2309, 5
  %2311 = add i64 %2307, %2310
  %2312 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2311
  %2313 = sext i32 %8 to i64
  %2314 = mul i64 %2313, 21125
  %2315 = sext i32 %12 to i64
  %2316 = mul i64 %2315, 325
  %2317 = add i64 %2314, %2316
  %2318 = load i32* %isize, align 4
  %2319 = add i32 %2318, -1
  %2320 = sext i32 %2319 to i64
  %2321 = mul i64 %2320, 5
  %2322 = add i64 %2317, %2321
  %2323 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2322
  %2324 = load i32* %isize, align 4
  %2325 = sext i32 %2324 to i64
  %2326 = mul i64 %2325, 75
  %2327 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2326
  call void bitcast (void (...)* @matvec_sub_ to void (double*, double*, double*)*)(double* %2327, double* %2323, double* %2312) #1
  %2328 = load i32* %isize, align 4
  %2329 = sext i32 %2328 to i64
  %2330 = mul i64 %2329, 75
  %2331 = add i64 %2330, 25
  %2332 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2331
  %2333 = load i32* %isize, align 4
  %2334 = add i32 %2333, -1
  %2335 = sext i32 %2334 to i64
  %2336 = mul i64 %2335, 75
  %2337 = add i64 %2336, 50
  %2338 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2337
  %2339 = load i32* %isize, align 4
  %2340 = sext i32 %2339 to i64
  %2341 = mul i64 %2340, 75
  %2342 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2341
  call void bitcast (void (...)* @matmul_sub_ to void (double*, double*, double*)*)(double* %2342, double* %2338, double* %2332) #1
  %2343 = sext i32 %8 to i64
  %2344 = mul i64 %2343, 21125
  %2345 = sext i32 %12 to i64
  %2346 = mul i64 %2345, 325
  %2347 = add i64 %2344, %2346
  %2348 = load i32* %isize, align 4
  %2349 = sext i32 %2348 to i64
  %2350 = mul i64 %2349, 5
  %2351 = add i64 %2347, %2350
  %2352 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2351
  %2353 = load i32* %isize, align 4
  %2354 = sext i32 %2353 to i64
  %2355 = mul i64 %2354, 75
  %2356 = add i64 %2355, 25
  %2357 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2356
  call void bitcast (void (...)* @binvrhs_ to void (double*, double*)*)(double* %2357, double* %2352) #1
  %2358 = load i32* %isize, align 4
  %2359 = add i32 %2358, -1
  %2360 = icmp sge i32 %2359, 0
  br i1 %2360, label %"16", label %"24"

"16":                                             ; preds = %"23", %"15"
  %2361 = phi i32 [ %2422, %"23" ], [ %2359, %"15" ]
  br i1 true, label %"17", label %"22"

"17":                                             ; preds = %"21", %"16"
  %2362 = phi i32 [ %2419, %"21" ], [ 1, %"16" ]
  br i1 true, label %"18", label %"20"

"18":                                             ; preds = %"19", %"17"
  %2363 = phi i32 [ %2416, %"19" ], [ 1, %"17" ]
  %2364 = sext i32 %8 to i64
  %2365 = mul i64 %2364, 21125
  %2366 = sext i32 %12 to i64
  %2367 = mul i64 %2366, 325
  %2368 = add i64 %2365, %2367
  %2369 = sext i32 %2361 to i64
  %2370 = mul i64 %2369, 5
  %2371 = add i64 %2368, %2370
  %2372 = sext i32 %2362 to i64
  %2373 = add i64 %2371, %2372
  %2374 = add i64 %2373, -1
  %2375 = sext i32 %8 to i64
  %2376 = mul i64 %2375, 21125
  %2377 = sext i32 %12 to i64
  %2378 = mul i64 %2377, 325
  %2379 = add i64 %2376, %2378
  %2380 = sext i32 %2361 to i64
  %2381 = mul i64 %2380, 5
  %2382 = add i64 %2379, %2381
  %2383 = sext i32 %2362 to i64
  %2384 = add i64 %2382, %2383
  %2385 = add i64 %2384, -1
  %2386 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2385
  %2387 = load double* %2386, align 8
  %2388 = sext i32 %2361 to i64
  %2389 = mul i64 %2388, 75
  %2390 = sext i32 %2363 to i64
  %2391 = mul i64 %2390, 5
  %2392 = add i64 %2389, %2391
  %2393 = sext i32 %2362 to i64
  %2394 = add i64 %2392, %2393
  %2395 = add i64 %2394, 44
  %2396 = getelementptr double* getelementptr inbounds (%4* @work_lhs_, i64 0, i32 2, i64 0), i64 %2395
  %2397 = load double* %2396, align 8
  %2398 = sext i32 %8 to i64
  %2399 = mul i64 %2398, 21125
  %2400 = sext i32 %12 to i64
  %2401 = mul i64 %2400, 325
  %2402 = add i64 %2399, %2401
  %2403 = add i32 %2361, 1
  %2404 = sext i32 %2403 to i64
  %2405 = mul i64 %2404, 5
  %2406 = add i64 %2402, %2405
  %2407 = sext i32 %2363 to i64
  %2408 = add i64 %2406, %2407
  %2409 = add i64 %2408, -1
  %2410 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2409
  %2411 = load double* %2410, align 8
  %2412 = fmul double %2397, %2411
  %2413 = fsub double %2387, %2412
  %2414 = getelementptr double* getelementptr inbounds (%1* @fields_, i64 0, i32 7, i64 0), i64 %2374
  store double %2413, double* %2414, align 8
  %2415 = icmp eq i32 %2363, 5
  %2416 = add i32 %2363, 1
  %2417 = icmp ne i1 %2415, false
  br i1 %2417, label %"20", label %"19"

"19":                                             ; preds = %"18"
  br label %"18"

"20":                                             ; preds = %"18", %"17"
  %2418 = icmp eq i32 %2362, 5
  %2419 = add i32 %2362, 1
  %2420 = icmp ne i1 %2418, false
  br i1 %2420, label %"22", label %"21"

"21":                                             ; preds = %"20"
  br label %"17"

"22":                                             ; preds = %"20", %"16"
  %2421 = icmp eq i32 %2361, 0
  %2422 = add i32 %2361, -1
  %2423 = icmp ne i1 %2421, false
  br i1 %2423, label %"24", label %"23"

"23":                                             ; preds = %"22"
  br label %"16"

"24":                                             ; preds = %"22", %"15"
  %2424 = icmp eq i32 %12, %10
  %2425 = add i32 %12, 1
  %2426 = icmp ne i1 %2424, false
  br i1 %2426, label %"26", label %"25"

"25":                                             ; preds = %"24"
  br label %"6"

"26":                                             ; preds = %"24", %"5"
  %2427 = icmp eq i32 %8, %6
  %2428 = add i32 %8, 1
  %2429 = icmp ne i1 %2427, false
  br i1 %2429, label %"28", label %"27"

"27":                                             ; preds = %"26"
  br label %"5"

"28":                                             ; preds = %"26", %"4"
  %2430 = load i32* getelementptr inbounds (%2* @global_, i64 0, i32 2), align 4, !range !0
  %2431 = trunc i32 %2430 to i1
  %2432 = icmp ne i1 %2431, false
  br i1 %2432, label %"29", label %"30"

"29":                                             ; preds = %"28"
  call void bitcast (void (...)* @timer_stop_ to void (i32*)*)(i32* @1) #1
  br label %"30"

"30":                                             ; preds = %"29", %"28"
  br label %"31"

"31":                                             ; preds = %"30"
  %2433 = bitcast i32* %isize to i8*
  call void @llvm.lifetime.end(i64 4, i8* %2433)
  br label %"32"

"32":                                             ; preds = %"31"
  br label %return

return:                                           ; preds = %"32"
  ret void
}

declare void @timer_start_(...)

declare void @lhsinit_(...)

declare void @binvcrhs_(...)

declare void @matvec_sub_(...)

declare void @matmul_sub_(...)

declare void @binvrhs_(...)

declare void @timer_stop_(...)

; Function Attrs: nounwind
declare void @llvm.lifetime.end(i64, i8* nocapture) #1

attributes #0 = { nounwind uwtable "no-frame-pointer-elim-non-leaf"="true" }
attributes #1 = { nounwind }

!0 = metadata !{i32 0, i32 2}
